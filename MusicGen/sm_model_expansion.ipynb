{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding my own layers on top of the small-stereo-model\n",
    "# Uses the codes from the musicgen model\n",
    "# Onehot encodes the angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "from audiocraft.models import MusicGen\n",
    "from transformers import get_scheduler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "import random\n",
    "import wandb\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from audiocraft.modules.conditioners import ClassifierFreeGuidanceDropout\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpandedMusicGen(nn.Module):\n",
    "    def __init__(self, pretrained_model, num_angles):\n",
    "        super(ExpandedMusicGen, self).__init__()\n",
    "        self.pretrained_model = pretrained_model\n",
    "        self.angle_layer = nn.Linear(3, num_angles)  # New layer to process the angle\n",
    "        self.audio_layer = ...  # New layer to process the audio\n",
    "        self.final_layer = nn.Linear(..., ...)  # New layer to combine the outputs\n",
    "        self,num_angles = num_angles\n",
    "\n",
    "    def forward(self, audio, angle):\n",
    "        # audio will be in the [1,2,x] format\n",
    "\n",
    "        # Convert angle to the one-hot encoding\n",
    "        angle = nn.functional.one_hot(angle, num_classes=self.num_angles)\n",
    "        # get the musicgen codes\n",
    "        audio = self.pretrained_model(audio)\n",
    "        # Add the layer for processing the angle\n",
    "        angle = self.angle_layer(angle)\n",
    "        # Use the codes\n",
    "        audio = self.audio_layer(audio)\n",
    "        x = torch.cat((audio, angle), dim=1)  # Concatenate the outputs\n",
    "        x = self.final_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
