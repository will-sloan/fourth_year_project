{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiocraft.models import MusicGen\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "from tqdm import trange\n",
    "import os\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MusicGen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m my_trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mMusicGen\u001b[49m\u001b[38;5;241m.\u001b[39mget_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfacebook/musicgen-stereo-small\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MusicGen' is not defined"
     ]
    }
   ],
   "source": [
    "my_trained_model = MusicGen.get_pretrained('facebook/musicgen-stereo-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_trained_model.lm.load_state_dict(torch.load('my_save_path/lm_final.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_trained_model.lm = my_trained_model.lm.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_trained_model.lm.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should be the file used as input into the model\n",
    "# The model takes mono and outputs stereo\n",
    "# so this should be the mono \"original\" audio\n",
    "myfile = 'model1_data_modified/30S_FreqSweep_Original_0.wav'\n",
    "s0, bd = wavfile.read(myfile)\n",
    "# The waveform has to have two channels \n",
    "# so we duplicate the 1 channel into both\n",
    "bd = np.column_stack((bd, bd))\n",
    "# Convert bd to a tensor\n",
    "bd_tensor = torch.from_numpy(bd)\n",
    "bd_tensor = bd_tensor.to(torch.float32)\n",
    "bd = np.transpose(bd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd_tensor = bd_tensor.unsqueeze(0) # add batch dimension\n",
    "# 90 in this case is the angle\n",
    "attributes, prompt_tokens = my_trained_model._prepare_tokens_and_attributes(descriptions=['90'], prompt=bd_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much to generate, but it accounts for the length of the prompt\n",
    "max_gen_len = int(30 * my_trained_model.frame_rate) + prompt_tokens.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_trained_model.generation_params = {\n",
    "    'max_gen_len': max_gen_len,\n",
    "    'use_sampling': True,\n",
    "    'temp': 1.0,\n",
    "    'top_k': 250,\n",
    "    'top_p': 0.0,\n",
    "    'cfg_coef': 3.0,\n",
    "    'two_step_cfg': 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = []\n",
    "sample_loops = 4\n",
    "for _ in trange(sample_loops):\n",
    "    with my_trained_model.autocast:\n",
    "        gen_tokens = my_trained_model.lm.generate(prompt_tokens, attributes, callback=None, **my_trained_model.generation_params)\n",
    "        total.append(gen_tokens[..., prompt_tokens.shape[-1] if prompt_tokens is not None else 0:])\n",
    "        prompt_tokens = gen_tokens[..., -gen_tokens.shape[-1] // 2:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expands total into what is generated, not really sure how\n",
    "gen_tokens = torch.cat(total, -1)\n",
    "\n",
    "# Decode the tokens into audio\n",
    "with torch.no_grad():\n",
    "    gen_audio = my_trained_model.compression_model.decode(gen_tokens, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves it\n",
    "gen_audio = gen_audio.cpu()\n",
    "save_path = 'my_save_path/'\n",
    "save_file_name = 'output_audio.wav'\n",
    "save_path_temp = os.path.join(save_path, save_file_name)\n",
    "#print(save_path_temp)\n",
    "torchaudio.save(save_path_temp, gen_audio[0], my_trained_model.sample_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
