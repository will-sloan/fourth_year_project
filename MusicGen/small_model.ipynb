{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchaudio'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# This file is the current 'working' file for the musicgen-stereo-small\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# I don't really know if the training is work just yet. \u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchaudio\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maudiocraft\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MusicGen\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchaudio'"
     ]
    }
   ],
   "source": [
    "# This file is the current 'working' file for the musicgen-stereo-small\n",
    "# I don't really know if the training is work just yet. \n",
    "\n",
    "import torchaudio\n",
    "from audiocraft.models import MusicGen\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "import random\n",
    "import wandb\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from audiocraft.modules.conditioners import ClassifierFreeGuidanceDropout\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Code from: https://github.com/chavinlo/musicgen_trainer/blob/main/train.py\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Create a class to hold your data set\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Modified since we are not loading the same way as the original code. \u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mAudioDataset\u001b[39;00m(\u001b[43mDataset\u001b[49m):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data_dir, baseline):\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;66;03m# Uses the absolute path to the directory where the data is stored\u001b[39;00m\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir \u001b[38;5;241m=\u001b[39m data_dir\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Code from: https://github.com/chavinlo/musicgen_trainer/blob/main/train.py\n",
    "# Create a class to hold your data set\n",
    "# Modified since we are not loading the same way as the original code. \n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, data_dir, baseline):\n",
    "        # Uses the absolute path to the directory where the data is stored\n",
    "        self.data_dir = data_dir\n",
    "        self.data_map = []\n",
    "        #self.baseline_file_name = baseline_file_name\n",
    "        dir_map = os.listdir(data_dir)\n",
    "        for d in dir_map:\n",
    "            name, ext = os.path.splitext(d)\n",
    "            # Only have 1 data point for each recording\n",
    "            # that takes the left/right and the baseline\n",
    "            if 'recording' in name or 'EARS_1' in name:\n",
    "                continue\n",
    "            if ext == \".wav\":\n",
    "                # We will have labels for everything\n",
    "                #if os.path.exists(os.path.join(data_dir, name + \".txt\")):\n",
    "                label = name.split('Deg')[0]\n",
    "                if label is not None:\n",
    "                    temp = name.split('_')\n",
    "                    index_of_target = name.split('_')[-1]\n",
    "                    left_ear = temp[0] + '_' + temp[1] + '_' + '1' + '_' + index_of_target + '.wav'\n",
    "                    right_ear = temp[0] + '_' + temp[1] + '_' + '2' + '_' + index_of_target + '.wav'\n",
    "                    orig = baseline + index_of_target + \".wav\"\n",
    "                    self.data_map.append(\n",
    "                        {\n",
    "                            \"left_target\": os.path.join(data_dir, left_ear),\n",
    "                            \"right_target\": os.path.join(data_dir, right_ear),\n",
    "                            \"label\": label,\n",
    "                            \"original\": os.path.join(data_dir, orig),\n",
    "                        }\n",
    "                    )\n",
    "                else:\n",
    "                    raise ValueError(f\"No label file for {name}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_map)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data_map[idx]\n",
    "        left_audio = data[\"left_target\"]\n",
    "        right_audio = data[\"right_target\"]\n",
    "        label = data.get(\"label\", \"\")\n",
    "        original = data.get(\"original\", \"\")\n",
    "\n",
    "        return left_audio, right_audio, label, original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts number of nans \n",
    "def count_nans(tensor):\n",
    "    # Creating a boolean mask where True represents NaN values in the tensor\n",
    "    nan_mask = torch.isnan(tensor)\n",
    "\n",
    "    # Calculating the total number of NaN values by summing up the True values in the mask\n",
    "    num_nans = torch.sum(nan_mask).item()\n",
    "\n",
    "    # Returning the count of NaN values as an integer\n",
    "    return num_nans\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts mono audio into codes that can be used by the model\n",
    "def preprocess_audio(audio_path, model, duration: int = 30):\n",
    "    # Keep the audio in stereo\n",
    "    wav, sr = torchaudio.load(audio_path)\n",
    "\n",
    "    # check if frequencies match\n",
    "    assert sr == model.sample_rate, f\"Sample rate is {sr} not {model.sample_rate}\"\n",
    "\n",
    "    # Check if audio length is long enough\n",
    "    #if wav.shape[1] < duration * model.sample_rate:\n",
    "    #    return None\n",
    "    \n",
    "    # Audio should be 30 seconds long exactly\n",
    "    assert wav.shape[1] == duration * model.sample_rate, f\"Audio is {wav.shape[1] / model.sample_rate} seconds long, not {duration} seconds long\"\n",
    "\n",
    "    # Move audio tensor to GPU\n",
    "    wav = wav.cuda()\n",
    "    #print(f\"Audio shape: {wav.shape}\")\n",
    "    # add a batch dimension\n",
    "    # Copy the audio to the other channel\n",
    "    wav = torch.cat((wav, wav), dim=0)\n",
    "    # Add a batch dimension\n",
    "    wav = wav.unsqueeze(0)\n",
    "    #print(f\"Audio shape: {wav.shape}\")\n",
    "    \n",
    "\n",
    "    # Encode using models compression method\n",
    "    with torch.no_grad():\n",
    "        gen_audio = model.compression_model.encode(wav)\n",
    "    \n",
    "    codes, scale = gen_audio\n",
    "    #print(codes)\n",
    "    #print(scale)\n",
    "\n",
    "    assert scale is None\n",
    "\n",
    "    return codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts stereo audio into codes that can be used by the model\n",
    "def preprocess_stereo_audio(audio_path1, audio_path2, model, duration: int = 30):\n",
    "    # Keep the audio in stereo\n",
    "    wav1, sr1 = torchaudio.load(audio_path1)\n",
    "    wav2, sr2 = torchaudio.load(audio_path2)\n",
    "\n",
    "    assert sr1 == sr2, f\"Sample rates are {sr1} and {sr2} not the same\"\n",
    "    assert wav1.shape == wav2.shape, f\"Audio shapes are {wav1.shape} and {wav2.shape} not the same\"\n",
    "\n",
    "    wav, sr = torch.cat((wav1, wav2), dim=0), sr1\n",
    "\n",
    "    # check if frequencies match\n",
    "    assert sr == model.sample_rate, f\"Sample rate is {sr} not {model.sample_rate}\"\n",
    "\n",
    "    # Check if audio length is long enough\n",
    "    #if wav.shape[1] < duration * model.sample_rate:\n",
    "    #    return None\n",
    "    \n",
    "    # Audio should be 30 seconds long exactly\n",
    "    assert wav.shape[1] == duration * model.sample_rate, f\"Audio is {wav.shape[1] / model.sample_rate} seconds long, not {duration} seconds long\"\n",
    "\n",
    "    # Move audio tensor to GPU\n",
    "    wav = wav.cuda()\n",
    "    #print(f\"Audio shape: {wav.shape}\")\n",
    "    # add a batch dimension\n",
    "    # Copy the audio to the other channel\n",
    "    #wav = torch.cat((wav, wav), dim=0)\n",
    "    # Add a batch dimension\n",
    "    wav = wav.unsqueeze(0)\n",
    "    #print(f\"Audio shape: {wav.shape}\")\n",
    "    \n",
    "\n",
    "    # Encode using models compression method\n",
    "    with torch.no_grad():\n",
    "        gen_audio = model.compression_model.encode(wav)\n",
    "    \n",
    "    codes, scale = gen_audio\n",
    "    #print(codes)\n",
    "    #print(scale)\n",
    "\n",
    "    assert scale is None\n",
    "\n",
    "    return codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfixnan\u001b[39m(tensor: \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m      2\u001b[0m     nan_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39misnan(tensor)\n\u001b[0;32m      3\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(nan_mask, torch\u001b[38;5;241m.\u001b[39mzeros_like(tensor), tensor)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "def fixnan(tensor: torch.Tensor):\n",
    "    nan_mask = torch.isnan(tensor)\n",
    "    result = torch.where(nan_mask, torch.zeros_like(tensor), tensor)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def one_hot_encode(tensor, num_classes=2048):\n",
    "    shape = tensor.shape\n",
    "    one_hot = torch.zeros((shape[0], shape[1], num_classes))\n",
    "\n",
    "    for i in range(shape[0]):\n",
    "        for j in range(shape[1]):\n",
    "            index = tensor[i, j].item()\n",
    "            one_hot[i, j, index] = 1\n",
    "\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MusicGen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# The train loop of the model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Takes the model and dataset to fine tune\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\n\u001b[0;32m      5\u001b[0m     dataset_path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m----> 6\u001b[0m     model: \u001b[43mMusicGen\u001b[49m,\n\u001b[0;32m      7\u001b[0m     lr: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[0;32m      8\u001b[0m     epochs: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m      9\u001b[0m     use_wandb: \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m     10\u001b[0m     no_label: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     11\u001b[0m     tune_text: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     12\u001b[0m     save_step: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     13\u001b[0m     grad_acc: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m,\n\u001b[0;32m     14\u001b[0m     use_scaler: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     15\u001b[0m     weight_decay: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-5\u001b[39m,\n\u001b[0;32m     16\u001b[0m     warmup_steps: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m     17\u001b[0m     batch_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m     18\u001b[0m     use_cfg: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     19\u001b[0m     save_path: \u001b[38;5;28mstr\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     20\u001b[0m     baseline: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecording_01_\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     21\u001b[0m ):\n\u001b[0;32m     22\u001b[0m \n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# Load the pretrained model\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m#model = MusicGen.get_pretrained(model_id)\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     model\u001b[38;5;241m.\u001b[39mlm \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlm\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)  \u001b[38;5;66;03m# Convert model to float32\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m# Create dataset and dataloader\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MusicGen' is not defined"
     ]
    }
   ],
   "source": [
    "# The train loop of the model\n",
    "# Takes the model and dataset to fine tune\n",
    "\n",
    "def train(\n",
    "    dataset_path: str,\n",
    "    model: MusicGen,\n",
    "    lr: float,\n",
    "    epochs: int,\n",
    "    use_wandb: bool,\n",
    "    no_label: bool = False,\n",
    "    tune_text: bool = False,\n",
    "    save_step: int = None,\n",
    "    grad_acc: int = 8,\n",
    "    use_scaler: bool = False,\n",
    "    weight_decay: float = 1e-5,\n",
    "    warmup_steps: int = 10,\n",
    "    batch_size: int = 10,\n",
    "    use_cfg: bool = False,\n",
    "    save_path: str='models/',\n",
    "    baseline: str = 'recording_01_'\n",
    "):\n",
    "\n",
    "    # Load the pretrained model\n",
    "    #model = MusicGen.get_pretrained(model_id)\n",
    "    model.lm = model.lm.to(torch.float32)  # Convert model to float32\n",
    "\n",
    "    # Create dataset and dataloader\n",
    "    dataset = AudioDataset(dataset_path, baseline)\n",
    "    train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    learning_rate = lr\n",
    "    model.lm.train()\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    # what is inside train_dataloader\n",
    "    #print(train_dataloader)\n",
    "    #print(train_dataloader.dataset.data_map)\n",
    "    #return 0\n",
    "    if tune_text:\n",
    "        print(\"Tuning text\")\n",
    "    else:\n",
    "        print(\"Tuning everything\")\n",
    "\n",
    "    # from paper\n",
    "    optimizer = AdamW(\n",
    "        model.lm.condition_provider.parameters()\n",
    "        if tune_text\n",
    "        else model.lm.parameters(),\n",
    "        lr=learning_rate,\n",
    "        betas=(0.9, 0.95),\n",
    "        weight_decay=weight_decay,\n",
    "    )\n",
    "    scheduler = get_scheduler(\n",
    "        \"cosine\",\n",
    "        optimizer,\n",
    "        warmup_steps,\n",
    "        int(epochs * len(train_dataloader) / grad_acc),\n",
    "    )\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    num_epochs = epochs\n",
    "\n",
    "    save_step = save_step\n",
    "    save_models = False if save_step is None else True\n",
    "\n",
    "    save_path = save_path\n",
    "\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    current_step = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_idx, (left_targets, right_targets, labels, originals) in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            all_codes = []\n",
    "            all_target_codes = []\n",
    "            texts = []\n",
    "            # What are audio and label\n",
    "            #print(target)\n",
    "            #print(label)\n",
    "            #print(original)\n",
    "            \n",
    "\n",
    "            # Iterate through audio paths and corresponding labels\n",
    "            for left_target, right_target, label, original in zip(left_targets, right_targets, labels, originals):\n",
    "                #print(inner_audio)\n",
    "                #print(l)\n",
    "                #print(inner_orig.shape)\n",
    "                inner_audio = preprocess_audio(original, model)  # Preprocess audio to tensor\n",
    "                target_audio = preprocess_stereo_audio(left_target, right_target, model) # Prepocess target audio to tensor\n",
    "\n",
    "                # Need both the predicted audio and a target\n",
    "                if inner_audio is None or target_audio is None:\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                if use_cfg:\n",
    "                    codes = torch.cat([inner_audio, inner_audio], dim=0)\n",
    "                    target_codes = torch.cat([target_audio, target_audio], dim=0)\n",
    "                else:\n",
    "                    codes = inner_audio\n",
    "                    target_codes = target_audio\n",
    "\n",
    "                all_codes.append(codes)\n",
    "                all_target_codes.append(target_codes)\n",
    "                #texts.append(open(l, \"r\").read().strip())\n",
    "                texts.append(label)\n",
    "            \n",
    "            attributes, _ = model._prepare_tokens_and_attributes(texts, None)\n",
    "            conditions = attributes\n",
    "            if use_cfg:\n",
    "                null_conditions = ClassifierFreeGuidanceDropout(p=1.0)(conditions)\n",
    "                conditions = conditions + null_conditions\n",
    "            tokenized = model.lm.condition_provider.tokenize(conditions)\n",
    "            cfg_conditions = model.lm.condition_provider(tokenized)\n",
    "            condition_tensors = cfg_conditions\n",
    "\n",
    "            # If we have no codes then no training :(\n",
    "            if len(all_codes) == 0 or len(all_target_codes) == 0:\n",
    "                continue\n",
    "            \n",
    "\n",
    "            codes = torch.cat(all_codes, dim=0)\n",
    "            target_codes = torch.cat(all_target_codes, dim=0)\n",
    "            \n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                # Compute predictions using the model's LM\n",
    "                # 'codes' are the input data, 'conditions' are the conditioning features\n",
    "                # 'condition_tensors' are the tokenized conditions\n",
    "                lm_output = model.lm.compute_predictions(\n",
    "                    codes=codes, conditions=[], condition_tensors=condition_tensors\n",
    "                )\n",
    "\n",
    "                # The lines below copy the values \n",
    "                # 'codes' are the input data\n",
    "                target_codes = target_codes[0]\n",
    "                #codes = codes[0]\n",
    "                # 'logits' are the raw, unnormalized predictions generated by the model\n",
    "                logits = lm_output.logits[0]\n",
    "                # 'mask' is a binary mask that indicates where the model should pay attention\n",
    "                mask = lm_output.mask[0]\n",
    "\n",
    "                # One-hot encode the 'codes' to match the dimensionality of the 'logits'\n",
    "                #codes = one_hot_encode(codes, num_classes=2048)\n",
    "                target_codes = one_hot_encode(target_codes, num_classes=2048)\n",
    "                target_codes = target_codes.cuda()\n",
    "                # Move 'codes', 'logits', and 'mask' to the GPU\n",
    "                #codes = codes.cuda()\n",
    "                logits = logits.cuda()\n",
    "                mask = mask.cuda()\n",
    "\n",
    "                # Reshape 'mask' to be a 1D tensor\n",
    "                mask = mask.view(-1)\n",
    "                # Apply the 'mask' to the 'logits' and 'codes'\n",
    "                # This step is necessary because the loss function only considers the masked positions\n",
    "                masked_logits = logits.view(-1, 2048)[mask]\n",
    "                #masked_codes = codes.view(-1, 2048)[mask]\n",
    "                masked_target_codes = target_codes.view(-1, 2048)[mask]\n",
    "\n",
    "                # Compute the cross-entropy loss between the masked logits and codes\n",
    "                # The loss quantifies how well the model's predictions match the actual data\n",
    "                #loss = criterion(masked_logits, masked_codes)\n",
    "                loss = criterion(masked_logits, masked_target_codes)\n",
    "            current_step += 1 / grad_acc\n",
    "\n",
    "            # assert count_nans(masked_logits) == 0\n",
    "\n",
    "            (scaler.scale(loss) if use_scaler else loss).backward()\n",
    "\n",
    "            total_norm = 0\n",
    "            for p in model.lm.condition_provider.parameters():\n",
    "                try:\n",
    "                    param_norm = p.grad.data.norm(2)\n",
    "                    total_norm += param_norm.item() ** 2\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "            total_norm = total_norm ** (1.0 / 2)\n",
    "\n",
    "            if use_wandb:\n",
    "                run.log(\n",
    "                    {\n",
    "                        \"loss\": loss.item(),\n",
    "                        \"total_norm\": total_norm,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            print(\n",
    "                f\"Epoch: {epoch}/{num_epochs}, Batch: {batch_idx}/{len(train_dataloader)}, Loss: {loss.item()}\"\n",
    "            )\n",
    "\n",
    "            if batch_idx % grad_acc != grad_acc - 1:\n",
    "                continue\n",
    "\n",
    "            if use_scaler:\n",
    "                scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.lm.parameters(), 0.5)\n",
    "\n",
    "            if use_scaler:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "            scheduler.step()\n",
    "            print(f\"Current step: {current_step}\")\n",
    "            if save_models:\n",
    "                if (\n",
    "                    current_step == int(current_step)\n",
    "                    and int(current_step) % save_step == 0\n",
    "                ):\n",
    "                    torch.save(\n",
    "                        model.lm.state_dict(), f\"{save_path}/lm_{current_step}.pt\"\n",
    "                    )\n",
    "\n",
    "    torch.save(model.lm.state_dict(), f\"{save_path}/lm_final.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/workspace/small_model_data2' # Where the AudioBreakdown outputed\n",
    "model_id = 'facebook/musicgen-stereo-small' # What pretrained model you are used\n",
    "lr = 1e-5\n",
    "epochs = 100\n",
    "use_wandb = False\n",
    "save_step = 32\n",
    "grad_acc = 2\n",
    "no_label = False\n",
    "tune_text = False\n",
    "weight_decay = 1e-5\n",
    "warmup_steps = 16\n",
    "batch_size = 4\n",
    "use_cfg = False\n",
    "save_path = 'small_model_save_path/' # Check points of the models training are saved so you can go back if we find a specific model did really well. \n",
    "baseline = 'recording_01_' # This is actually what we feed the model then the targets are used as comparison to see how we did. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MusicGen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mMusicGen\u001b[49m\u001b[38;5;241m.\u001b[39mget_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfacebook/musicgen-stereo-small\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MusicGen' is not defined"
     ]
    }
   ],
   "source": [
    "model = MusicGen.get_pretrained('facebook/musicgen-stereo-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m(\n\u001b[0;32m      2\u001b[0m     dataset_path\u001b[38;5;241m=\u001b[39mdataset_path,\n\u001b[0;32m      3\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m      4\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[0;32m      5\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[0;32m      6\u001b[0m     use_wandb\u001b[38;5;241m=\u001b[39muse_wandb,\n\u001b[0;32m      7\u001b[0m     save_step\u001b[38;5;241m=\u001b[39msave_step,\n\u001b[0;32m      8\u001b[0m     no_label\u001b[38;5;241m=\u001b[39mno_label,\n\u001b[0;32m      9\u001b[0m     tune_text\u001b[38;5;241m=\u001b[39mtune_text,\n\u001b[0;32m     10\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[0;32m     11\u001b[0m     grad_acc\u001b[38;5;241m=\u001b[39mgrad_acc,\n\u001b[0;32m     12\u001b[0m     warmup_steps\u001b[38;5;241m=\u001b[39mwarmup_steps,\n\u001b[0;32m     13\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m     14\u001b[0m     use_cfg\u001b[38;5;241m=\u001b[39muse_cfg,\n\u001b[0;32m     15\u001b[0m     save_path\u001b[38;5;241m=\u001b[39msave_path,\n\u001b[0;32m     16\u001b[0m     use_scaler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     17\u001b[0m     baseline\u001b[38;5;241m=\u001b[39mbaseline,\n\u001b[0;32m     18\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "train(\n",
    "    dataset_path=dataset_path,\n",
    "    model=model,\n",
    "    lr=lr,\n",
    "    epochs=epochs,\n",
    "    use_wandb=use_wandb,\n",
    "    save_step=save_step,\n",
    "    no_label=no_label,\n",
    "    tune_text=tune_text,\n",
    "    weight_decay=weight_decay,\n",
    "    grad_acc=grad_acc,\n",
    "    warmup_steps=warmup_steps,\n",
    "    batch_size=batch_size,\n",
    "    use_cfg=use_cfg,\n",
    "    save_path=save_path,\n",
    "    use_scaler=True,\n",
    "    baseline=baseline,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
