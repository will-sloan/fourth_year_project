{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To hold the code of a blank transformer model\n",
    "import sys\n",
    "sys.path.append('/workspace/fourth_year_project/MusicGen')\n",
    "#print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MyAudioDataset import MyAudioDataset\n",
    "from AudioCodesDataset import AudioCodesDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/venv_work/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "from audiocraft.models import CompressionModel\n",
    "from audiocraft.models.encodec import InterleaveStereoCompressionModel\n",
    "model = CompressionModel.get_pretrained('facebook/encodec_32khz')\n",
    "#model = model.cuda()\n",
    "comp_model = InterleaveStereoCompressionModel(model).cuda()\n",
    "# move to GPU\n",
    "##comp_model = comp_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydataset = MyAudioDataset('/workspace/small_model_data3', 'recording_01_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_codes_dataset = AudioCodesDataset(comp_model=comp_model, dataset=mydataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_codes_dataset.run_compression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1500])\n",
      "torch.Size([8, 1500])\n"
     ]
    }
   ],
   "source": [
    "print(audio_codes_dataset.data_map[0]['original'].shape)\n",
    "print(audio_codes_dataset.data_map[0]['original_norm'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.8611, -1.8611, -1.8611,  ..., -1.0767, -1.0767, -1.0767],\n",
       "        [-1.8611, -1.8611, -1.8611,  ..., -1.0767, -1.0767, -1.0767],\n",
       "        [ 0.3033,  1.0248,  1.0248,  ...,  0.3646,  0.3646, -2.0135],\n",
       "        ...,\n",
       "        [-1.0437,  0.4118,  0.9556,  ...,  1.1112,  0.5155,  0.9321],\n",
       "        [-0.5454,  1.1678,  0.9446,  ...,  0.8676,  0.9446,  0.8676],\n",
       "        [-0.5454,  1.1678,  0.9446,  ...,  0.8676,  0.9446,  0.8676]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_codes_dataset.data_map[0]['original_norm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "570"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(audio_codes_dataset.data_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_codes_dataset.save_data('90_degree_compress_tensors.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "570"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(audio_codes_dataset.data_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_codes_dataset = AudioCodesDataset(comp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_codes_dataset.load_data('90_degree_compress_tensors.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1500])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_codes_dataset.data_map[0]['original_norm'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchaudio\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AudioTransformer(nn.Module):\n",
    "    def __init__(self, comp_model, d_model, nhead, num_layers, dim_feedforward, compute_seperate_loss=False):\n",
    "        super(AudioTransformer, self).__init__()\n",
    "        self.input_encoding = nn.Linear(1500, d_model)  # input audio\n",
    "        self.input_bn = nn.BatchNorm1d(d_model)\n",
    "\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_layers, dim_feedforward)\n",
    "        self.hidden_layer = nn.\n",
    "        self.output_bn = nn.BatchNorm1d(d_model)\n",
    "        self.output_decoding = nn.Linear(d_model, 1500)  # Decoding back to stereo audio\n",
    "        #self.angle_encoding = nn.Linear(6, d_model)  # add this once 90 works. \n",
    "        \n",
    "        if comp_model is not None:\n",
    "            self.comp_model = comp_model.cuda()\n",
    "        else:\n",
    "            self.comp_model = None\n",
    "        self.compute_seperate_loss = compute_seperate_loss\n",
    "\n",
    "    # Orig and target are the normalized values of the codes\n",
    "    def forward(self, orig, target, angle):\n",
    "        orig = orig.cuda()\n",
    "        target = target.cuda()\n",
    "        #print(audio.shape)\n",
    "        orig = self.input_encoding(orig)\n",
    "        target = self.input_encoding(target)\n",
    "        # Relu to rid negatives\n",
    "        #orig = F.relu(orig)\n",
    "        #target = F.relu(target)\n",
    "\n",
    "        #angle = self.angle_encoding(angle)  # Process one-hot encoded angle\n",
    "        #angle = angle.unsqueeze(1).repeat(1, audio.size(2), 1)  # Repeat angle for each time step\n",
    "        #x = audio + angle  # Combine audio and angle\n",
    "\n",
    "        x = self.transformer(src=orig, tgt=target)\n",
    "        #x = F.relu(x)\n",
    "\n",
    "        x = self.output_decoding(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Scale back to integers\n",
    "        #x = x * 1000\n",
    "        #x = torch.round(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def compress(self, stereo):\n",
    "            if self.comp_model is None:\n",
    "                raise Exception(\"No compression model found\")\n",
    "            stereo = stereo.cuda()\n",
    "            with torch.no_grad():\n",
    "                stereo, scale = self.comp_model.encode(stereo)\n",
    "            return stereo\n",
    "\n",
    "\n",
    "    def decompress(self, stereo):\n",
    "            if self.comp_model is None:\n",
    "                raise Exception(\"No compression model found\")\n",
    "            stereo = stereo.cuda()\n",
    "            with torch.no_grad():\n",
    "                stereo = self.comp_model.decode(stereo)\n",
    "            return stereo\n",
    "    \n",
    "    def compute_mean_std(self):\n",
    "        all_data = torch.cat([item['target'] for item in self.data_map] + [item['original'] for item in self.data_map])\n",
    "        mean = torch.mean(all_data)\n",
    "        std = torch.std(all_data)\n",
    "        return mean, std\n",
    "\n",
    "    \n",
    "    \n",
    "    def train_loop(self, dataset, batch_size=1, epochs=1, lr=0.001, cosine_loss=False):\n",
    "        if not cosine_loss:\n",
    "            loss_fn = nn.MSELoss()\n",
    "        else:\n",
    "            loss_fn = nn.CosineEmbeddingLoss()\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.1)\n",
    "        train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        \n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            #for i, (target, target_norm, orig, orig_norm, angle, sr) in enumerate(train_loader):\n",
    "            for i, (_, target, _, orig, angle, sr) in enumerate(train_loader):\n",
    "                optimizer.zero_grad()\n",
    "                target = target.cuda()\n",
    "                orig = orig.cuda()\n",
    "                # Convert wav to codes\n",
    "                target_codes = target\n",
    "                orig_codes = orig\n",
    "                #print(target_codes.shape, orig_codes.shape)\n",
    "                #target_codes = self.compress(target)\n",
    "                #orig_codes = self.compress(orig)\n",
    "\n",
    "                #print(type(target_codes[0][0]))\n",
    "                #print(type(orig_codes[0][0]))\n",
    "                # Pass codes to model\n",
    "                output = self(orig=orig_codes.float(), target=target_codes.float(), angle=angle)\n",
    "                #output = output.squeeze(0)\n",
    "                #target = target_codes.squeeze(0)\n",
    "                if cosine_loss:\n",
    "                    if self.compute_seperate_loss:\n",
    "                        total_loss = 0\n",
    "                        for j in range(target_codes.shape[1]):\n",
    "                            output_j = output[:, j, :]\n",
    "                            target_codes_j = target_codes[:, j, :]\n",
    "                            # Should be a 1D tensor of length equal to the batch size\n",
    "                            # So if 4 then should be 4 ones\n",
    "                            y = torch.ones(target_codes_j.shape[0]).cuda()\n",
    "                            #print(output_j.shape, target_codes_j.shape, y.shape)\n",
    "                            loss = loss_fn(output_j, target_codes_j, y)\n",
    "                            total_loss += loss\n",
    "                        total_loss /=  target_codes.shape[1]  # average the loss over the batch\n",
    "                        total_loss.backward()  # backpropagate the average loss\n",
    "                        optimizer.step()\n",
    "                        scheduler.step()\n",
    "                        print(f\"Epoch {epoch}, batch {i}, loss {total_loss}\")\n",
    "                    else:\n",
    "                        output = output.view(-1, 1500)\n",
    "                        target_codes = target_codes.view(-1, 1500)\n",
    "                        y = torch.ones((output.size(0),)).cuda()\n",
    "                        loss = loss_fn(output, target_codes, y)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        scheduler.step()\n",
    "                        print(f\"Epoch {epoch}, batch {i}, loss {loss.item()}\")\n",
    "                else:\n",
    "                    # Using MSE loss\n",
    "                    output = output.float()\n",
    "                    target_codes = target_codes.float()\n",
    "                    loss = loss_fn(output, target_codes)\n",
    "\n",
    "                    #print(output, target_codes)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "                    if i % 10 == 0:\n",
    "                        print(f\"Epoch {epoch}, batch {i}, loss {loss.item()}\")\n",
    "                    \n",
    "\n",
    "\n",
    "                #y = torch.ones_like(target_codes).cuda()\n",
    "                #y = torch.ones(target_codes.size(0)).cuda()\n",
    "                \n",
    "                \n",
    "                #print(output.shape, target_codes.shape, y.shape)\n",
    "                #loss = loss_fn(output, target_codes, y)\n",
    "                #loss.backward()\n",
    "                #optimizer.step()\n",
    "                #print(f\"Epoch {epoch}, batch {i}, loss {loss.item()}\")\n",
    "\n",
    "            if epoch % 5 == 0 and epoch != 0:\n",
    "                torch.save(self.state_dict(), f\"model_{epoch}.pth\")\n",
    "                print(f\"Saved model_{epoch}.pth\")\n",
    "\n",
    "        print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/venv_work/lib/python3.8/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AudioTransformer(\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-2): 3 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-255): 256 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (input_encoding): Linear(in_features=1500, out_features=512, bias=True)\n",
       "  (output_decoding): Linear(in_features=512, out_features=1500, bias=True)\n",
       "  (comp_model): InterleaveStereoCompressionModel(\n",
       "    (model): HFEncodecCompressionModel(\n",
       "      (model): EncodecModel(\n",
       "        (encoder): EncodecEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): EncodecConv1d(\n",
       "              (conv): Conv1d(1, 64, kernel_size=(7,), stride=(1,))\n",
       "            )\n",
       "            (1): EncodecResnetBlock(\n",
       "              (block): ModuleList(\n",
       "                (0): ELU(alpha=1.0)\n",
       "                (1): EncodecConv1d(\n",
       "                  (conv): Conv1d(64, 32, kernel_size=(3,), stride=(1,))\n",
       "                )\n",
       "                (2): ELU(alpha=1.0)\n",
       "                (3): EncodecConv1d(\n",
       "                  (conv): Conv1d(32, 64, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (shortcut): Identity()\n",
       "            )\n",
       "            (2): ELU(alpha=1.0)\n",
       "            (3): EncodecConv1d(\n",
       "              (conv): Conv1d(64, 128, kernel_size=(8,), stride=(4,))\n",
       "            )\n",
       "            (4): EncodecResnetBlock(\n",
       "              (block): ModuleList(\n",
       "                (0): ELU(alpha=1.0)\n",
       "                (1): EncodecConv1d(\n",
       "                  (conv): Conv1d(128, 64, kernel_size=(3,), stride=(1,))\n",
       "                )\n",
       "                (2): ELU(alpha=1.0)\n",
       "                (3): EncodecConv1d(\n",
       "                  (conv): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (shortcut): Identity()\n",
       "            )\n",
       "            (5): ELU(alpha=1.0)\n",
       "            (6): EncodecConv1d(\n",
       "              (conv): Conv1d(128, 256, kernel_size=(8,), stride=(4,))\n",
       "            )\n",
       "            (7): EncodecResnetBlock(\n",
       "              (block): ModuleList(\n",
       "                (0): ELU(alpha=1.0)\n",
       "                (1): EncodecConv1d(\n",
       "                  (conv): Conv1d(256, 128, kernel_size=(3,), stride=(1,))\n",
       "                )\n",
       "                (2): ELU(alpha=1.0)\n",
       "                (3): EncodecConv1d(\n",
       "                  (conv): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (shortcut): Identity()\n",
       "            )\n",
       "            (8): ELU(alpha=1.0)\n",
       "            (9): EncodecConv1d(\n",
       "              (conv): Conv1d(256, 512, kernel_size=(10,), stride=(5,))\n",
       "            )\n",
       "            (10): EncodecResnetBlock(\n",
       "              (block): ModuleList(\n",
       "                (0): ELU(alpha=1.0)\n",
       "                (1): EncodecConv1d(\n",
       "                  (conv): Conv1d(512, 256, kernel_size=(3,), stride=(1,))\n",
       "                )\n",
       "                (2): ELU(alpha=1.0)\n",
       "                (3): EncodecConv1d(\n",
       "                  (conv): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (shortcut): Identity()\n",
       "            )\n",
       "            (11): ELU(alpha=1.0)\n",
       "            (12): EncodecConv1d(\n",
       "              (conv): Conv1d(512, 1024, kernel_size=(16,), stride=(8,))\n",
       "            )\n",
       "            (13): EncodecLSTM(\n",
       "              (lstm): LSTM(1024, 1024, num_layers=2)\n",
       "            )\n",
       "            (14): ELU(alpha=1.0)\n",
       "            (15): EncodecConv1d(\n",
       "              (conv): Conv1d(1024, 128, kernel_size=(7,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (decoder): EncodecDecoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): EncodecConv1d(\n",
       "              (conv): Conv1d(128, 1024, kernel_size=(7,), stride=(1,))\n",
       "            )\n",
       "            (1): EncodecLSTM(\n",
       "              (lstm): LSTM(1024, 1024, num_layers=2)\n",
       "            )\n",
       "            (2): ELU(alpha=1.0)\n",
       "            (3): EncodecConvTranspose1d(\n",
       "              (conv): ConvTranspose1d(1024, 512, kernel_size=(16,), stride=(8,))\n",
       "            )\n",
       "            (4): EncodecResnetBlock(\n",
       "              (block): ModuleList(\n",
       "                (0): ELU(alpha=1.0)\n",
       "                (1): EncodecConv1d(\n",
       "                  (conv): Conv1d(512, 256, kernel_size=(3,), stride=(1,))\n",
       "                )\n",
       "                (2): ELU(alpha=1.0)\n",
       "                (3): EncodecConv1d(\n",
       "                  (conv): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (shortcut): Identity()\n",
       "            )\n",
       "            (5): ELU(alpha=1.0)\n",
       "            (6): EncodecConvTranspose1d(\n",
       "              (conv): ConvTranspose1d(512, 256, kernel_size=(10,), stride=(5,))\n",
       "            )\n",
       "            (7): EncodecResnetBlock(\n",
       "              (block): ModuleList(\n",
       "                (0): ELU(alpha=1.0)\n",
       "                (1): EncodecConv1d(\n",
       "                  (conv): Conv1d(256, 128, kernel_size=(3,), stride=(1,))\n",
       "                )\n",
       "                (2): ELU(alpha=1.0)\n",
       "                (3): EncodecConv1d(\n",
       "                  (conv): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (shortcut): Identity()\n",
       "            )\n",
       "            (8): ELU(alpha=1.0)\n",
       "            (9): EncodecConvTranspose1d(\n",
       "              (conv): ConvTranspose1d(256, 128, kernel_size=(8,), stride=(4,))\n",
       "            )\n",
       "            (10): EncodecResnetBlock(\n",
       "              (block): ModuleList(\n",
       "                (0): ELU(alpha=1.0)\n",
       "                (1): EncodecConv1d(\n",
       "                  (conv): Conv1d(128, 64, kernel_size=(3,), stride=(1,))\n",
       "                )\n",
       "                (2): ELU(alpha=1.0)\n",
       "                (3): EncodecConv1d(\n",
       "                  (conv): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (shortcut): Identity()\n",
       "            )\n",
       "            (11): ELU(alpha=1.0)\n",
       "            (12): EncodecConvTranspose1d(\n",
       "              (conv): ConvTranspose1d(128, 64, kernel_size=(8,), stride=(4,))\n",
       "            )\n",
       "            (13): EncodecResnetBlock(\n",
       "              (block): ModuleList(\n",
       "                (0): ELU(alpha=1.0)\n",
       "                (1): EncodecConv1d(\n",
       "                  (conv): Conv1d(64, 32, kernel_size=(3,), stride=(1,))\n",
       "                )\n",
       "                (2): ELU(alpha=1.0)\n",
       "                (3): EncodecConv1d(\n",
       "                  (conv): Conv1d(32, 64, kernel_size=(1,), stride=(1,))\n",
       "                )\n",
       "              )\n",
       "              (shortcut): Identity()\n",
       "            )\n",
       "            (14): ELU(alpha=1.0)\n",
       "            (15): EncodecConv1d(\n",
       "              (conv): Conv1d(64, 1, kernel_size=(7,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (quantizer): EncodecResidualVectorQuantizer(\n",
       "          (layers): ModuleList(\n",
       "            (0-3): 4 x EncodecVectorQuantization(\n",
       "              (codebook): EncodecEuclideanCodebook()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myTransformer = AudioTransformer(comp_model=comp_model, d_model=512, nhead=4, num_layers=3, dim_feedforward=256).cuda()\n",
    "myTransformer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, batch 0, loss 1.1546118259429932\n",
      "Epoch 0, batch 10, loss 0.9905588030815125\n",
      "Epoch 0, batch 20, loss 0.9900474548339844\n",
      "Epoch 0, batch 30, loss 0.9915304183959961\n",
      "Epoch 0, batch 40, loss 0.9881868958473206\n",
      "Epoch 0, batch 50, loss 0.9917421936988831\n",
      "Epoch 0, batch 60, loss 0.9907203912734985\n",
      "Epoch 0, batch 70, loss 0.9899943470954895\n",
      "Epoch 1, batch 0, loss 0.9878147840499878\n",
      "Epoch 1, batch 10, loss 0.9897182583808899\n",
      "Epoch 1, batch 20, loss 0.9901285171508789\n",
      "Epoch 1, batch 30, loss 0.9897047281265259\n",
      "Epoch 1, batch 40, loss 0.9892682433128357\n",
      "Epoch 1, batch 50, loss 0.9896485805511475\n",
      "Epoch 1, batch 60, loss 0.9895039200782776\n",
      "Epoch 1, batch 70, loss 0.9866693615913391\n",
      "Epoch 2, batch 0, loss 0.9900076389312744\n",
      "Epoch 2, batch 10, loss 0.9885878562927246\n",
      "Epoch 2, batch 20, loss 0.989489734172821\n",
      "Epoch 2, batch 30, loss 0.9887732863426208\n",
      "Epoch 2, batch 40, loss 0.9908292293548584\n",
      "Epoch 2, batch 50, loss 0.9901090264320374\n",
      "Epoch 2, batch 60, loss 0.9891948103904724\n",
      "Epoch 2, batch 70, loss 0.989554762840271\n",
      "Epoch 3, batch 0, loss 0.988450825214386\n",
      "Epoch 3, batch 10, loss 0.9892112612724304\n",
      "Epoch 3, batch 20, loss 0.9896604418754578\n",
      "Epoch 3, batch 30, loss 0.989863932132721\n",
      "Epoch 3, batch 40, loss 0.988703191280365\n",
      "Epoch 3, batch 50, loss 0.9903225898742676\n",
      "Epoch 3, batch 60, loss 0.9911960363388062\n",
      "Epoch 3, batch 70, loss 0.9904941320419312\n",
      "Epoch 4, batch 0, loss 0.9908946752548218\n",
      "Epoch 4, batch 10, loss 0.989536464214325\n",
      "Epoch 4, batch 20, loss 0.9901646971702576\n",
      "Epoch 4, batch 30, loss 0.9906104803085327\n",
      "Epoch 4, batch 40, loss 0.9915045499801636\n",
      "Epoch 4, batch 50, loss 0.98931485414505\n",
      "Epoch 4, batch 60, loss 0.9894174933433533\n",
      "Epoch 4, batch 70, loss 0.9884941577911377\n",
      "Epoch 5, batch 0, loss 0.9889085292816162\n",
      "Epoch 5, batch 10, loss 0.9889969229698181\n",
      "Epoch 5, batch 20, loss 0.9910672307014465\n",
      "Epoch 5, batch 30, loss 0.9887793064117432\n",
      "Epoch 5, batch 40, loss 0.9901688694953918\n",
      "Epoch 5, batch 50, loss 0.9870215654373169\n",
      "Epoch 5, batch 60, loss 0.9886056780815125\n",
      "Epoch 5, batch 70, loss 0.9900403618812561\n",
      "Saved model_5.pth\n",
      "Epoch 6, batch 0, loss 0.9898708462715149\n",
      "Epoch 6, batch 10, loss 0.9905501008033752\n",
      "Epoch 6, batch 20, loss 0.988632082939148\n",
      "Epoch 6, batch 30, loss 0.989947497844696\n",
      "Epoch 6, batch 40, loss 0.9905810356140137\n",
      "Epoch 6, batch 50, loss 0.99052894115448\n",
      "Epoch 6, batch 60, loss 0.991512656211853\n",
      "Epoch 6, batch 70, loss 0.9898812770843506\n",
      "Epoch 7, batch 0, loss 0.9892297983169556\n",
      "Epoch 7, batch 10, loss 0.9905431270599365\n",
      "Epoch 7, batch 20, loss 0.9913788437843323\n",
      "Epoch 7, batch 30, loss 0.9897781610488892\n",
      "Epoch 7, batch 40, loss 0.9886917471885681\n",
      "Epoch 7, batch 50, loss 0.990122377872467\n",
      "Epoch 7, batch 60, loss 0.9914275407791138\n",
      "Epoch 7, batch 70, loss 0.9913845658302307\n",
      "Epoch 8, batch 0, loss 0.9882584810256958\n",
      "Epoch 8, batch 10, loss 0.98995041847229\n",
      "Epoch 8, batch 20, loss 0.9908974766731262\n",
      "Epoch 8, batch 30, loss 0.9890384078025818\n",
      "Epoch 8, batch 40, loss 0.9889301657676697\n",
      "Epoch 8, batch 50, loss 0.9898170232772827\n",
      "Epoch 8, batch 60, loss 0.9869550466537476\n",
      "Epoch 8, batch 70, loss 0.9899137020111084\n",
      "Epoch 9, batch 0, loss 0.9896864891052246\n",
      "Epoch 9, batch 10, loss 0.9895427227020264\n",
      "Epoch 9, batch 20, loss 0.989230215549469\n",
      "Epoch 9, batch 30, loss 0.9905688166618347\n",
      "Epoch 9, batch 40, loss 0.9914259910583496\n",
      "Epoch 9, batch 50, loss 0.9892933368682861\n",
      "Epoch 9, batch 60, loss 0.9916320443153381\n",
      "Epoch 9, batch 70, loss 0.9899378418922424\n",
      "Epoch 10, batch 0, loss 0.9895810484886169\n",
      "Epoch 10, batch 10, loss 0.9919382929801941\n",
      "Epoch 10, batch 20, loss 0.9902060627937317\n",
      "Epoch 10, batch 30, loss 0.9908068180084229\n",
      "Epoch 10, batch 40, loss 0.9902241826057434\n",
      "Epoch 10, batch 50, loss 0.9890792369842529\n",
      "Epoch 10, batch 60, loss 0.9904124140739441\n",
      "Epoch 10, batch 70, loss 0.9893462061882019\n",
      "Saved model_10.pth\n",
      "Epoch 11, batch 0, loss 0.9897124171257019\n",
      "Epoch 11, batch 10, loss 0.9903857111930847\n",
      "Epoch 11, batch 20, loss 0.9876678586006165\n",
      "Epoch 11, batch 30, loss 0.9888694286346436\n",
      "Epoch 11, batch 40, loss 0.9909118413925171\n",
      "Epoch 11, batch 50, loss 0.9897944331169128\n",
      "Epoch 11, batch 60, loss 0.9903100728988647\n",
      "Epoch 11, batch 70, loss 0.9905239343643188\n",
      "Epoch 12, batch 0, loss 0.9893245100975037\n",
      "Epoch 12, batch 10, loss 0.9907705187797546\n",
      "Epoch 12, batch 20, loss 0.9893788695335388\n",
      "Epoch 12, batch 30, loss 0.9913083910942078\n",
      "Epoch 12, batch 40, loss 0.9894142150878906\n",
      "Epoch 12, batch 50, loss 0.9892229437828064\n",
      "Epoch 12, batch 60, loss 0.9900802373886108\n",
      "Epoch 12, batch 70, loss 0.9905750155448914\n",
      "Epoch 13, batch 0, loss 0.991548478603363\n",
      "Epoch 13, batch 10, loss 0.9904968738555908\n",
      "Epoch 13, batch 20, loss 0.9889014959335327\n",
      "Epoch 13, batch 30, loss 0.9922354817390442\n",
      "Epoch 13, batch 40, loss 0.9900466203689575\n",
      "Epoch 13, batch 50, loss 0.9896879196166992\n",
      "Epoch 13, batch 60, loss 0.988916277885437\n",
      "Epoch 13, batch 70, loss 0.9865440726280212\n",
      "Epoch 14, batch 0, loss 0.9890167713165283\n",
      "Epoch 14, batch 10, loss 0.9912195801734924\n",
      "Epoch 14, batch 20, loss 0.9878076314926147\n",
      "Epoch 14, batch 30, loss 0.9892538785934448\n",
      "Epoch 14, batch 40, loss 0.9900771379470825\n",
      "Epoch 14, batch 50, loss 0.9890729188919067\n",
      "Epoch 14, batch 60, loss 0.9883492588996887\n",
      "Epoch 14, batch 70, loss 0.9907644987106323\n",
      "Epoch 15, batch 0, loss 0.9894323348999023\n",
      "Epoch 15, batch 10, loss 0.9893507361412048\n",
      "Epoch 15, batch 20, loss 0.9882723093032837\n",
      "Epoch 15, batch 30, loss 0.992110013961792\n",
      "Epoch 15, batch 40, loss 0.990403950214386\n",
      "Epoch 15, batch 50, loss 0.9895037412643433\n",
      "Epoch 15, batch 60, loss 0.9909846782684326\n",
      "Epoch 15, batch 70, loss 0.9880041480064392\n",
      "Saved model_15.pth\n",
      "Epoch 16, batch 0, loss 0.9894845485687256\n",
      "Epoch 16, batch 10, loss 0.9894793033599854\n",
      "Epoch 16, batch 20, loss 0.9910831451416016\n",
      "Epoch 16, batch 30, loss 0.9911274909973145\n",
      "Epoch 16, batch 40, loss 0.9895937442779541\n",
      "Epoch 16, batch 50, loss 0.9906013607978821\n",
      "Epoch 16, batch 60, loss 0.9898887276649475\n",
      "Epoch 16, batch 70, loss 0.9899163246154785\n",
      "Epoch 17, batch 0, loss 0.990207850933075\n",
      "Epoch 17, batch 10, loss 0.9892984628677368\n",
      "Epoch 17, batch 20, loss 0.9882643818855286\n",
      "Epoch 17, batch 30, loss 0.9902228116989136\n",
      "Epoch 17, batch 40, loss 0.9879847168922424\n",
      "Epoch 17, batch 50, loss 0.9903846979141235\n",
      "Epoch 17, batch 60, loss 0.9899514317512512\n",
      "Epoch 17, batch 70, loss 0.9894348978996277\n",
      "Epoch 18, batch 0, loss 0.9912346005439758\n",
      "Epoch 18, batch 10, loss 0.9895561337471008\n",
      "Epoch 18, batch 20, loss 0.9918518662452698\n",
      "Epoch 18, batch 30, loss 0.9894912242889404\n",
      "Epoch 18, batch 40, loss 0.9880183935165405\n",
      "Epoch 18, batch 50, loss 0.9892261624336243\n",
      "Epoch 18, batch 60, loss 0.9903919100761414\n",
      "Epoch 18, batch 70, loss 0.9893388748168945\n",
      "Epoch 19, batch 0, loss 0.9891132712364197\n",
      "Epoch 19, batch 10, loss 0.9892091155052185\n",
      "Epoch 19, batch 20, loss 0.9899488687515259\n",
      "Epoch 19, batch 30, loss 0.9898989200592041\n",
      "Epoch 19, batch 40, loss 0.988969087600708\n",
      "Epoch 19, batch 50, loss 0.9909917712211609\n",
      "Epoch 19, batch 60, loss 0.9908322095870972\n",
      "Epoch 19, batch 70, loss 0.9907504916191101\n",
      "Epoch 20, batch 0, loss 0.9887094497680664\n",
      "Epoch 20, batch 10, loss 0.9905017614364624\n",
      "Epoch 20, batch 20, loss 0.9919482469558716\n",
      "Epoch 20, batch 30, loss 0.9893063902854919\n",
      "Epoch 20, batch 40, loss 0.9897515177726746\n",
      "Epoch 20, batch 50, loss 0.9890257120132446\n",
      "Epoch 20, batch 60, loss 0.9894874691963196\n",
      "Epoch 20, batch 70, loss 0.9900385737419128\n",
      "Saved model_20.pth\n",
      "Epoch 21, batch 0, loss 0.988740086555481\n",
      "Epoch 21, batch 10, loss 0.988473117351532\n",
      "Epoch 21, batch 20, loss 0.989293098449707\n",
      "Epoch 21, batch 30, loss 0.9899330735206604\n",
      "Epoch 21, batch 40, loss 0.9901794195175171\n",
      "Epoch 21, batch 50, loss 0.9900961518287659\n",
      "Epoch 21, batch 60, loss 0.9893476366996765\n",
      "Epoch 21, batch 70, loss 0.9903439879417419\n",
      "Epoch 22, batch 0, loss 0.9900432825088501\n",
      "Epoch 22, batch 10, loss 0.9883798956871033\n",
      "Epoch 22, batch 20, loss 0.9900216460227966\n",
      "Epoch 22, batch 30, loss 0.9919377565383911\n",
      "Epoch 22, batch 40, loss 0.9891765713691711\n",
      "Epoch 22, batch 50, loss 0.9894334077835083\n",
      "Epoch 22, batch 60, loss 0.9921513795852661\n",
      "Epoch 22, batch 70, loss 0.9900645017623901\n",
      "Epoch 23, batch 0, loss 0.9903128743171692\n",
      "Epoch 23, batch 10, loss 0.9907869100570679\n",
      "Epoch 23, batch 20, loss 0.9896832704544067\n",
      "Epoch 23, batch 30, loss 0.9892796874046326\n",
      "Epoch 23, batch 40, loss 0.9892006516456604\n",
      "Epoch 23, batch 50, loss 0.9901412725448608\n",
      "Epoch 23, batch 60, loss 0.9899112582206726\n",
      "Epoch 23, batch 70, loss 0.9903069734573364\n",
      "Epoch 24, batch 0, loss 0.9906423091888428\n",
      "Epoch 24, batch 10, loss 0.9895783066749573\n",
      "Epoch 24, batch 20, loss 0.9912503957748413\n",
      "Epoch 24, batch 30, loss 0.9888201355934143\n",
      "Epoch 24, batch 40, loss 0.9901138544082642\n",
      "Epoch 24, batch 50, loss 0.9908219575881958\n",
      "Epoch 24, batch 60, loss 0.9906365275382996\n",
      "Epoch 24, batch 70, loss 0.9895937442779541\n",
      "Epoch 25, batch 0, loss 0.9892379641532898\n",
      "Epoch 25, batch 10, loss 0.9904361963272095\n",
      "Epoch 25, batch 20, loss 0.9884270429611206\n",
      "Epoch 25, batch 30, loss 0.9889881610870361\n",
      "Epoch 25, batch 40, loss 0.9887136220932007\n",
      "Epoch 25, batch 50, loss 0.9895501136779785\n",
      "Epoch 25, batch 60, loss 0.9900111556053162\n",
      "Epoch 25, batch 70, loss 0.9903061389923096\n",
      "Saved model_25.pth\n",
      "Epoch 26, batch 0, loss 0.9898953437805176\n",
      "Epoch 26, batch 10, loss 0.9913070201873779\n",
      "Epoch 26, batch 20, loss 0.987795889377594\n",
      "Epoch 26, batch 30, loss 0.9893671870231628\n",
      "Epoch 26, batch 40, loss 0.9883121848106384\n",
      "Epoch 26, batch 50, loss 0.9883795380592346\n",
      "Epoch 26, batch 60, loss 0.9881993532180786\n",
      "Epoch 26, batch 70, loss 0.9908143877983093\n",
      "Epoch 27, batch 0, loss 0.9915992021560669\n",
      "Epoch 27, batch 10, loss 0.9897142648696899\n",
      "Epoch 27, batch 20, loss 0.9893212914466858\n",
      "Epoch 27, batch 30, loss 0.9869598746299744\n",
      "Epoch 27, batch 40, loss 0.9900266528129578\n",
      "Epoch 27, batch 50, loss 0.9901677966117859\n",
      "Epoch 27, batch 60, loss 0.9909207224845886\n",
      "Epoch 27, batch 70, loss 0.988776683807373\n",
      "Epoch 28, batch 0, loss 0.9902693629264832\n",
      "Epoch 28, batch 10, loss 0.9889358878135681\n",
      "Epoch 28, batch 20, loss 0.9921106696128845\n",
      "Epoch 28, batch 30, loss 0.9874932169914246\n",
      "Epoch 28, batch 40, loss 0.9894980192184448\n",
      "Epoch 28, batch 50, loss 0.989311695098877\n",
      "Epoch 28, batch 60, loss 0.9908738732337952\n",
      "Epoch 28, batch 70, loss 0.9895123839378357\n",
      "Epoch 29, batch 0, loss 0.9933766722679138\n",
      "Epoch 29, batch 10, loss 0.9888690710067749\n",
      "Epoch 29, batch 20, loss 0.9911459684371948\n",
      "Epoch 29, batch 30, loss 0.9898517727851868\n",
      "Epoch 29, batch 40, loss 0.9891821146011353\n",
      "Epoch 29, batch 50, loss 0.9894177913665771\n",
      "Epoch 29, batch 60, loss 0.9899196028709412\n",
      "Epoch 29, batch 70, loss 0.989334762096405\n",
      "Epoch 30, batch 0, loss 0.9892783164978027\n",
      "Epoch 30, batch 10, loss 0.9880768060684204\n",
      "Epoch 30, batch 20, loss 0.9886860251426697\n",
      "Epoch 30, batch 30, loss 0.9922404885292053\n",
      "Epoch 30, batch 40, loss 0.9886365532875061\n",
      "Epoch 30, batch 50, loss 0.9914105534553528\n",
      "Epoch 30, batch 60, loss 0.9908191561698914\n",
      "Epoch 30, batch 70, loss 0.9885911345481873\n",
      "Saved model_30.pth\n",
      "Epoch 31, batch 0, loss 0.9894405007362366\n",
      "Epoch 31, batch 10, loss 0.9880480170249939\n",
      "Epoch 31, batch 20, loss 0.9899735450744629\n",
      "Epoch 31, batch 30, loss 0.9901785254478455\n",
      "Epoch 31, batch 40, loss 0.991226077079773\n",
      "Epoch 31, batch 50, loss 0.990731418132782\n",
      "Epoch 31, batch 60, loss 0.9912495017051697\n",
      "Epoch 31, batch 70, loss 0.9893427491188049\n",
      "Epoch 32, batch 0, loss 0.9901012182235718\n",
      "Epoch 32, batch 10, loss 0.9881964325904846\n",
      "Epoch 32, batch 20, loss 0.990215539932251\n",
      "Epoch 32, batch 30, loss 0.991050660610199\n",
      "Epoch 32, batch 40, loss 0.9889768958091736\n",
      "Epoch 32, batch 50, loss 0.989212691783905\n",
      "Epoch 32, batch 60, loss 0.9903363585472107\n",
      "Epoch 32, batch 70, loss 0.9909589886665344\n",
      "Epoch 33, batch 0, loss 0.9899213910102844\n",
      "Epoch 33, batch 10, loss 0.990909993648529\n",
      "Epoch 33, batch 20, loss 0.9899822473526001\n",
      "Epoch 33, batch 30, loss 0.9889441728591919\n",
      "Epoch 33, batch 40, loss 0.9886580109596252\n",
      "Epoch 33, batch 50, loss 0.9897747039794922\n",
      "Epoch 33, batch 60, loss 0.991495668888092\n",
      "Epoch 33, batch 70, loss 0.9899539351463318\n",
      "Epoch 34, batch 0, loss 0.9890673160552979\n",
      "Epoch 34, batch 10, loss 0.9897722005844116\n",
      "Epoch 34, batch 20, loss 0.9899535775184631\n",
      "Epoch 34, batch 30, loss 0.9890707731246948\n",
      "Epoch 34, batch 40, loss 0.9919735193252563\n",
      "Epoch 34, batch 50, loss 0.9893335103988647\n",
      "Epoch 34, batch 60, loss 0.989456057548523\n",
      "Epoch 34, batch 70, loss 0.9904159307479858\n",
      "Epoch 35, batch 0, loss 0.990837037563324\n",
      "Epoch 35, batch 10, loss 0.9887457489967346\n",
      "Epoch 35, batch 20, loss 0.9907644987106323\n",
      "Epoch 35, batch 30, loss 0.9896518588066101\n",
      "Epoch 35, batch 40, loss 0.9899216890335083\n",
      "Epoch 35, batch 50, loss 0.9910506010055542\n",
      "Epoch 35, batch 60, loss 0.9902825355529785\n",
      "Epoch 35, batch 70, loss 0.9894044399261475\n",
      "Saved model_35.pth\n",
      "Epoch 36, batch 0, loss 0.9905271530151367\n",
      "Epoch 36, batch 10, loss 0.9893922209739685\n",
      "Epoch 36, batch 20, loss 0.9884024262428284\n",
      "Epoch 36, batch 30, loss 0.9912367463111877\n",
      "Epoch 36, batch 40, loss 0.9921568632125854\n",
      "Epoch 36, batch 50, loss 0.9901751279830933\n",
      "Epoch 36, batch 60, loss 0.9892470836639404\n",
      "Epoch 36, batch 70, loss 0.9882398843765259\n",
      "Epoch 37, batch 0, loss 0.9903866052627563\n",
      "Epoch 37, batch 10, loss 0.9894532561302185\n",
      "Epoch 37, batch 20, loss 0.9901770949363708\n",
      "Epoch 37, batch 30, loss 0.9886035323143005\n",
      "Epoch 37, batch 40, loss 0.9890912771224976\n",
      "Epoch 37, batch 50, loss 0.990710437297821\n",
      "Epoch 37, batch 60, loss 0.9897421002388\n",
      "Epoch 37, batch 70, loss 0.9886642098426819\n",
      "Epoch 38, batch 0, loss 0.9904950857162476\n",
      "Epoch 38, batch 10, loss 0.9885346293449402\n",
      "Epoch 38, batch 20, loss 0.9893474578857422\n",
      "Epoch 38, batch 30, loss 0.9900503754615784\n",
      "Epoch 38, batch 40, loss 0.9884855151176453\n",
      "Epoch 38, batch 50, loss 0.9918462634086609\n",
      "Epoch 38, batch 60, loss 0.9919741153717041\n",
      "Epoch 38, batch 70, loss 0.9902000427246094\n",
      "Epoch 39, batch 0, loss 0.9898341298103333\n",
      "Epoch 39, batch 10, loss 0.9902337789535522\n",
      "Epoch 39, batch 20, loss 0.9895977973937988\n",
      "Epoch 39, batch 30, loss 0.9885517358779907\n",
      "Epoch 39, batch 40, loss 0.9901548624038696\n",
      "Epoch 39, batch 50, loss 0.9908542633056641\n",
      "Epoch 39, batch 60, loss 0.9907281994819641\n",
      "Epoch 39, batch 70, loss 0.9896605610847473\n",
      "Epoch 40, batch 0, loss 0.9907307624816895\n",
      "Epoch 40, batch 10, loss 0.9909788370132446\n",
      "Epoch 40, batch 20, loss 0.9895187020301819\n",
      "Epoch 40, batch 30, loss 0.9883114099502563\n",
      "Epoch 40, batch 40, loss 0.9897149205207825\n",
      "Epoch 40, batch 50, loss 0.9895997643470764\n",
      "Epoch 40, batch 60, loss 0.9885515570640564\n",
      "Epoch 40, batch 70, loss 0.9887076616287231\n",
      "Saved model_40.pth\n",
      "Epoch 41, batch 0, loss 0.9887637495994568\n",
      "Epoch 41, batch 10, loss 0.9876866936683655\n",
      "Epoch 41, batch 20, loss 0.9900209903717041\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmyTransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudio_codes_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 129\u001b[0m, in \u001b[0;36mAudioTransformer.train_loop\u001b[0;34m(self, dataset, batch_size, epochs, lr, cosine_loss)\u001b[0m\n\u001b[1;32m    126\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output, target_codes)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m#print(output, target_codes)\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    131\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/workspace/venv_work/lib/python3.8/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/venv_work/lib/python3.8/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "myTransformer.train_loop(dataset=audio_codes_dataset, batch_size=8, epochs=100, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_work",
   "language": "python",
   "name": "venv_work"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
