{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same model but trains without using the mask values\n",
    "# The mask is used to select what to train on\n",
    "# Instead, we want to train on everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "from audiocraft.models import MusicGen\n",
    "from transformers import get_scheduler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "import random\n",
    "import wandb\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from audiocraft.modules.conditioners import ClassifierFreeGuidanceDropout\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Each element has\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Code from: https://github.com/chavinlo/musicgen_trainer/blob/main/train.py\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Create a class to hold your data set\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Modified since we are not loading the same way as the original code. \u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mAudioDataset\u001b[39;00m(\u001b[43mDataset\u001b[49m):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data_dir, baseline):\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;66;03m# Uses the absolute path to the directory where the data is stored\u001b[39;00m\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir \u001b[38;5;241m=\u001b[39m data_dir\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Each element has\n",
    "#    Left ear path\n",
    "#    Right ear path\n",
    "#    Label\n",
    "#    Original path\n",
    "\n",
    "# Code from: https://github.com/chavinlo/musicgen_trainer/blob/main/train.py\n",
    "# Create a class to hold your data set\n",
    "# Modified since we are not loading the same way as the original code. \n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, data_dir, baseline):\n",
    "        # Uses the absolute path to the directory where the data is stored\n",
    "        self.data_dir = data_dir\n",
    "        self.data_map = []\n",
    "        #self.baseline_file_name = baseline_file_name\n",
    "        dir_map = os.listdir(data_dir)\n",
    "        for d in dir_map:\n",
    "            name, ext = os.path.splitext(d)\n",
    "            # Only have 1 data point for each recording\n",
    "            # that takes the left/right and the baseline\n",
    "            if 'recording' in name or 'EARS_1' in name:\n",
    "                continue\n",
    "            if ext == \".wav\":\n",
    "                # We will have labels for everything\n",
    "                #if os.path.exists(os.path.join(data_dir, name + \".txt\")):\n",
    "                label = name.split('Deg')[0]\n",
    "                if label is not None:\n",
    "                    temp = name.split('_')\n",
    "                    index_of_target = name.split('_')[-1]\n",
    "                    left_ear = temp[0] + '_' + temp[1] + '_' + '1' + '_' + index_of_target + '.wav'\n",
    "                    right_ear = temp[0] + '_' + temp[1] + '_' + '2' + '_' + index_of_target + '.wav'\n",
    "                    orig = baseline + index_of_target + \".wav\"\n",
    "                    self.data_map.append(\n",
    "                        {\n",
    "                            \"left_target\": os.path.join(data_dir, left_ear),\n",
    "                            \"right_target\": os.path.join(data_dir, right_ear),\n",
    "                            \"label\": label,\n",
    "                            \"original\": os.path.join(data_dir, orig),\n",
    "                        }\n",
    "                    )\n",
    "                else:\n",
    "                    raise ValueError(f\"No label file for {name}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_map)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data_map[idx]\n",
    "        left_audio = data[\"left_target\"]\n",
    "        right_audio = data[\"right_target\"]\n",
    "        label = data.get(\"label\", \"\")\n",
    "        original = data.get(\"original\", \"\")\n",
    "\n",
    "        return left_audio, right_audio, label, original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixnan(tensor: torch.Tensor):\n",
    "    nan_mask = torch.isnan(tensor)\n",
    "    result = torch.where(nan_mask, torch.zeros_like(tensor), tensor)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def one_hot_encode(tensor, num_classes=2048):\n",
    "    shape = tensor.shape\n",
    "    one_hot = torch.zeros((shape[0], shape[1], num_classes))\n",
    "\n",
    "    for i in range(shape[0]):\n",
    "        for j in range(shape[1]):\n",
    "            index = tensor[i, j].item()\n",
    "            one_hot[i, j, index] = 1\n",
    "\n",
    "    return one_hot\n",
    "\n",
    "def preprocess_stereo_audio(audio_path1, audio_path2, model, duration: int = 30):\n",
    "    # Keep the audio in stereo\n",
    "    wav1, sr1 = torchaudio.load(audio_path1)\n",
    "    wav2, sr2 = torchaudio.load(audio_path2)\n",
    "\n",
    "    assert sr1 == sr2, f\"Sample rates are {sr1} and {sr2} not the same\"\n",
    "    assert wav1.shape == wav2.shape, f\"Audio shapes are {wav1.shape} and {wav2.shape} not the same\"\n",
    "\n",
    "    wav, sr = torch.cat((wav1, wav2), dim=0), sr1\n",
    "\n",
    "    # check if frequencies match\n",
    "    assert sr == model.sample_rate, f\"Sample rate is {sr} not {model.sample_rate}\"\n",
    "\n",
    "    # Check if audio length is long enough\n",
    "    #if wav.shape[1] < duration * model.sample_rate:\n",
    "    #    return None\n",
    "    \n",
    "    # Audio should be 30 seconds long exactly\n",
    "    assert wav.shape[1] == duration * model.sample_rate, f\"Audio is {wav.shape[1] / model.sample_rate} seconds long, not {duration} seconds long\"\n",
    "\n",
    "    # Move audio tensor to GPU\n",
    "    wav = wav.cuda()\n",
    "    #print(f\"Audio shape: {wav.shape}\")\n",
    "    # add a batch dimension\n",
    "    # Copy the audio to the other channel\n",
    "    #wav = torch.cat((wav, wav), dim=0)\n",
    "    # Add a batch dimension\n",
    "    wav = wav.unsqueeze(0)\n",
    "    #print(f\"Audio shape: {wav.shape}\")\n",
    "    \n",
    "\n",
    "    # Encode using models compression method\n",
    "    with torch.no_grad():\n",
    "        gen_audio = model.compression_model.encode(wav)\n",
    "    \n",
    "    codes, scale = gen_audio\n",
    "    #print(codes)\n",
    "    #print(scale)\n",
    "\n",
    "    assert scale is None\n",
    "\n",
    "    return codes\n",
    "\n",
    "def preprocess_audio(audio_path, model, duration: int = 30):\n",
    "    # Keep the audio in stereo\n",
    "    wav, sr = torchaudio.load(audio_path)\n",
    "\n",
    "    # check if frequencies match\n",
    "    assert sr == model.sample_rate, f\"Sample rate is {sr} not {model.sample_rate}\"\n",
    "\n",
    "    # Check if audio length is long enough\n",
    "    #if wav.shape[1] < duration * model.sample_rate:\n",
    "    #    return None\n",
    "    \n",
    "    # Audio should be 30 seconds long exactly\n",
    "    assert wav.shape[1] == duration * model.sample_rate, f\"Audio is {wav.shape[1] / model.sample_rate} seconds long, not {duration} seconds long\"\n",
    "\n",
    "    # Move audio tensor to GPU\n",
    "    wav = wav.cuda()\n",
    "    #print(f\"Audio shape: {wav.shape}\")\n",
    "    # add a batch dimension\n",
    "    # Copy the audio to the other channel\n",
    "    wav = torch.cat((wav, wav), dim=0)\n",
    "    # Add a batch dimension\n",
    "    wav = wav.unsqueeze(0)\n",
    "    #print(f\"Audio shape: {wav.shape}\")\n",
    "    \n",
    "\n",
    "    # Encode using models compression method\n",
    "    with torch.no_grad():\n",
    "        gen_audio = model.compression_model.encode(wav)\n",
    "    \n",
    "    codes, scale = gen_audio\n",
    "    #print(codes)\n",
    "    #print(scale)\n",
    "\n",
    "    assert scale is None\n",
    "\n",
    "    return codes\n",
    "\n",
    "\n",
    "# Counts number of nans \n",
    "def count_nans(tensor):\n",
    "    # Creating a boolean mask where True represents NaN values in the tensor\n",
    "    nan_mask = torch.isnan(tensor)\n",
    "\n",
    "    # Calculating the total number of NaN values by summing up the True values in the mask\n",
    "    num_nans = torch.sum(nan_mask).item()\n",
    "\n",
    "    # Returning the count of NaN values as an integer\n",
    "    return num_nans\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import wandb  # Assuming wandb is used, it's being imported\n",
    "\n",
    "def train(\n",
    "    dataset_path: str,\n",
    "    model: MusicGen,\n",
    "    lr: float,\n",
    "    epochs: int,\n",
    "    use_wandb: bool,\n",
    "    no_label: bool = False,\n",
    "    tune_text: bool = False,\n",
    "    save_step: int = None,\n",
    "    grad_acc: int = 8,\n",
    "    use_scaler: bool = False,\n",
    "    weight_decay: float = 1e-5,\n",
    "    warmup_steps: int = 10,\n",
    "    batch_size: int = 10,\n",
    "    use_cfg: bool = False,\n",
    "    save_path: str='models/',\n",
    "    baseline: str = 'recording_01_'\n",
    "):\n",
    "\n",
    "    # Load the pretrained model\n",
    "    #model = MusicGen.get_pretrained(model_id)\n",
    "    model.lm = model.lm.to(torch.float32)  # Convert model to float32\n",
    "\n",
    "    # Create dataset and dataloader\n",
    "    dataset = AudioDataset(dataset_path, baseline)\n",
    "    train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    learning_rate = lr\n",
    "    model.lm.train()\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    # what is inside train_dataloader\n",
    "    #print(train_dataloader)\n",
    "    #print(train_dataloader.dataset.data_map)\n",
    "    #return 0\n",
    "    if tune_text:\n",
    "        print(\"Tuning text\")\n",
    "    else:\n",
    "        print(\"Tuning everything\")\n",
    "\n",
    "    # from paper\n",
    "    optimizer = AdamW(\n",
    "        model.lm.condition_provider.parameters()\n",
    "        if tune_text\n",
    "        else model.lm.parameters(),\n",
    "        lr=learning_rate,\n",
    "        betas=(0.9, 0.95),\n",
    "        weight_decay=weight_decay,\n",
    "    )\n",
    "    scheduler = get_scheduler(\n",
    "        \"cosine\",\n",
    "        optimizer,\n",
    "        warmup_steps,\n",
    "        int(epochs * len(train_dataloader) / grad_acc),\n",
    "    )\n",
    "\n",
    "    #criterion = nn.CrossEntropyLoss()\n",
    "    criterion = nn.MSELoss() #thnks\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    num_epochs = epochs\n",
    "\n",
    "    save_step = save_step\n",
    "    save_models = False if save_step is None else True\n",
    "\n",
    "    save_path = save_path\n",
    "\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    current_step = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_idx, (left_targets, right_targets, labels, originals) in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            all_codes = []\n",
    "            all_target_codes = []\n",
    "            texts = []\n",
    "            # What are audio and label\n",
    "            #print(target)\n",
    "            #print(label)\n",
    "            #print(original)\n",
    "            \n",
    "\n",
    "            # Iterate through audio paths and corresponding labels\n",
    "            for left_target, right_target, label, original in zip(left_targets, right_targets, labels, originals):\n",
    "                #print(inner_audio)\n",
    "                #print(l)\n",
    "                #print(inner_orig.shape)\n",
    "                inner_audio = preprocess_audio(original, model)  # Preprocess audio to tensor\n",
    "                target_audio = preprocess_stereo_audio(left_target, right_target, model) # Prepocess target audio to tensor\n",
    "\n",
    "                # Need both the predicted audio and a target\n",
    "                if inner_audio is None or target_audio is None:\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                if use_cfg:\n",
    "                    codes = torch.cat([inner_audio, inner_audio], dim=0)\n",
    "                    target_codes = torch.cat([target_audio, target_audio], dim=0)\n",
    "                else:\n",
    "                    codes = inner_audio\n",
    "                    target_codes = target_audio\n",
    "\n",
    "                all_codes.append(codes)\n",
    "                all_target_codes.append(target_codes)\n",
    "                #texts.append(open(l, \"r\").read().strip())\n",
    "                texts.append(label)\n",
    "            \n",
    "            attributes, _ = model._prepare_tokens_and_attributes(texts, None)\n",
    "            conditions = attributes\n",
    "            if use_cfg:\n",
    "                null_conditions = ClassifierFreeGuidanceDropout(p=1.0)(conditions)\n",
    "                conditions = conditions + null_conditions\n",
    "            tokenized = model.lm.condition_provider.tokenize(conditions)\n",
    "            cfg_conditions = model.lm.condition_provider(tokenized)\n",
    "            condition_tensors = cfg_conditions\n",
    "\n",
    "            # If we have no codes then no training :(\n",
    "            if len(all_codes) == 0 or len(all_target_codes) == 0:\n",
    "                continue\n",
    "            \n",
    "\n",
    "            codes = torch.cat(all_codes, dim=0)\n",
    "            target_codes = torch.cat(all_target_codes, dim=0)\n",
    "            \n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                # Compute predictions using the model's LM\n",
    "                # 'codes' are the input data, 'conditions' are the conditioning features\n",
    "                # 'condition_tensors' are the tokenized conditions\n",
    "                lm_output = model.lm.compute_predictions(\n",
    "                    codes=codes, conditions=[], condition_tensors=condition_tensors\n",
    "                )\n",
    "\n",
    "                # The lines below copy the values \n",
    "                # 'codes' are the input data\n",
    "                target_codes = target_codes[0]\n",
    "                # 'logits' are the raw, unnormalized predictions generated by the model\n",
    "                logits = lm_output.logits[0]\n",
    "\n",
    "                # One-hot encode the 'codes' to match the dimensionality of the 'logits'\n",
    "                target_codes = one_hot_encode(target_codes, num_classes=2048)\n",
    "                target_codes = target_codes.cuda()\n",
    "                logits = logits.cuda()\n",
    "\n",
    "                \n",
    "\n",
    "                logits = logits.view(-1, 2048)\n",
    "                target_codes = target_codes.view(-1, 2048)\n",
    "                loss = criterion(logits, target_codes)\n",
    "            current_step += 1 / grad_acc\n",
    "\n",
    "            # assert count_nans(masked_logits) == 0\n",
    "\n",
    "            (scaler.scale(loss) if use_scaler else loss).backward()\n",
    "\n",
    "            total_norm = 0\n",
    "            for p in model.lm.condition_provider.parameters():\n",
    "                try:\n",
    "                    param_norm = p.grad.data.norm(2)\n",
    "                    total_norm += param_norm.item() ** 2\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "            total_norm = total_norm ** (1.0 / 2)\n",
    "\n",
    "            if use_wandb:\n",
    "                run.log(\n",
    "                    {\n",
    "                        \"loss\": loss.item(),\n",
    "                        \"total_norm\": total_norm,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            print(\n",
    "                f\"Epoch: {epoch}/{num_epochs}, Batch: {batch_idx}/{len(train_dataloader)}, Loss: {loss.item()}\"\n",
    "            )\n",
    "\n",
    "            if batch_idx % grad_acc != grad_acc - 1:\n",
    "                continue\n",
    "\n",
    "            if use_scaler:\n",
    "                scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.lm.parameters(), 0.5)\n",
    "\n",
    "            if use_scaler:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "            scheduler.step()\n",
    "            print(f\"Current step: {current_step}\")\n",
    "            # Saves every save_step during batch training\n",
    "            # if save_models:\n",
    "            #     if (\n",
    "            #         current_step == int(current_step)\n",
    "            #         and int(current_step) % save_step == 0\n",
    "            #     ):\n",
    "            #         torch.save(\n",
    "            #             model.lm.state_dict(), f\"{save_path}/lm_{current_step}.pt\"\n",
    "            #         )\n",
    "        # Save once an epoch\n",
    "        if epoch % save_step == 0 and save_models:\n",
    "            torch.save(model.lm.state_dict(), f\"{save_path}/lm_epoch_{epoch}.pt\")\n",
    "\n",
    "    torch.save(model.lm.state_dict(), f\"{save_path}/lm_final.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/workspace/small_model_data2'\n",
    "model_id = 'facebook/musicgen-stereo-small'\n",
    "lr = 1e-5\n",
    "epochs = 100\n",
    "use_wandb = False\n",
    "save_step = 32\n",
    "grad_acc = 2\n",
    "no_label = False\n",
    "tune_text = False\n",
    "weight_decay = 1e-5\n",
    "warmup_steps = 16\n",
    "batch_size = 4\n",
    "use_cfg = False\n",
    "save_path = 'small_model_save_path/'\n",
    "baseline = 'recording_01_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MusicGen.get_pretrained('facebook/musicgen-stereo-small')\n",
    "# Continue from previous training\n",
    "#model.lm.load_state_dict(torch.load('small_model_save_path/lm_896.0.pt'))\n",
    "model.lm = model.lm.to(torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train(\n",
    "    dataset_path=dataset_path,\n",
    "    model=model,\n",
    "    lr=lr,\n",
    "    epochs=epochs,\n",
    "    use_wandb=use_wandb,\n",
    "    save_step=save_step,\n",
    "    no_label=no_label,\n",
    "    tune_text=tune_text,\n",
    "    weight_decay=weight_decay,\n",
    "    grad_acc=grad_acc,\n",
    "    warmup_steps=warmup_steps,\n",
    "    batch_size=batch_size,\n",
    "    use_cfg=use_cfg,\n",
    "    save_path=save_path,\n",
    "    use_scaler=True,\n",
    "    baseline=baseline,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
