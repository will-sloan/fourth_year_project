{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/workspace/fourth_year_project/HRTF Models/')\n",
    "\n",
    "from BasicDataset import BasicDataset\n",
    "# from BasicTransformer import BasicTransformer\n",
    "\n",
    "sofa_file = '/workspace/fourth_year_project/HRTF Models/sofa_hrtfs/RIEC_hrir_subject_001.sofa'\n",
    "# Basic Dataset only loads the HRIRs at 0 degrees and 90 degrees for baseline and 45 degree for testing\n",
    "hrir_dataset = BasicDataset()\n",
    "for i in range(1,100):\n",
    "    hrir_dataset.load(sofa_file.replace('001', str(i).zfill(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hrir_dataset[0]\n",
    "\n",
    "len(hrir_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PositionalEncoding import PositionalEncoding\n",
    "import torch.nn.functional as F\n",
    "# load first iteam from the dataset\n",
    "# hrirs0, angle0, hrirs90, angle90, hrirs45, angle45 = hrir_dataset[0]\n",
    "\n",
    "# hrirs0, angle0, hrirs90, angle90, hrirs45, angle45\n",
    "train_size = int(0.7 * len(hrir_dataset))\n",
    "val_size = int(0.2 * len(hrir_dataset))\n",
    "test_size = len(hrir_dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(hrir_dataset, [train_size, val_size, test_size])\n",
    "from torch.utils.data import DataLoader\n",
    "batch_size = 4\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=6)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=6)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PositionalEncoding import PositionalEncoding\n",
    "import torch.nn.functional as F\n",
    "class BasicTransformer(nn.Module):\n",
    "    # d_model=192, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048, angle_dim=64\n",
    "    def __init__(self, d_model=512, nhead=8, num_encoder_layers=3, num_decoder_layers=3, dim_feedforward=1024, dropout=0.0, max_len=512):\n",
    "        super(BasicTransformer, self).__init__()\n",
    "        \n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_encoder_layers)\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=num_decoder_layers)\n",
    "        self.angle_encoder = nn.Linear(1, d_model)  # Encodes angle to a high-dimensional space\n",
    "        self.encoder = nn.Linear(d_model, d_model)  # Adjusted for concatenated angles\n",
    "        self.decoder = nn.Linear(d_model, d_model)  # Output HRIR prediction\n",
    "        self.d_model = d_model\n",
    "        self.angle_dim = d_model\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.angle_encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    def forward(self, hrir0, angle0, hrir90, angle90, hrir45, angle45):\n",
    "        # batch, seq_len, d_model = 32, n, 512\n",
    "        # Encode the angles\n",
    "        angle0 = self.angle_encoder(angle0.unsqueeze(-1))\n",
    "        angle90 = self.angle_encoder(angle90.unsqueeze(-1))\n",
    "        angle45 = self.angle_encoder(angle45.unsqueeze(-1))\n",
    "        \n",
    "        # batch, 512\n",
    "        # convert to batch, 1, 512\n",
    "        angle0 = angle0.unsqueeze(1)\n",
    "        angle90 = angle90.unsqueeze(1)\n",
    "        angle45 = angle45.unsqueeze(1)\n",
    "\n",
    "        # Concatenate the encoded angles with the HRIR data\n",
    "        # angles = [32, 1, 512]\n",
    "        # hrir0 = [32, 2, 512]\n",
    "        # concated = [32, 3, 512]\n",
    "        concated0 = torch.cat([hrir0, angle0], dim=1)\n",
    "        concated90 = torch.cat([hrir90, angle90], dim=1)\n",
    "        # print(concated0.shape, concated90.shape, angle45.shape)\n",
    "        # concated45 = torch.cat([hrir45, angle45], dim=1)\n",
    "\n",
    "        # Concate the 3 angles\n",
    "        # concated = [32, 7, 512]\n",
    "        concated = torch.cat([concated0, concated90, angle45], dim=1)\n",
    "        # print(concated.shape)\n",
    "\n",
    "        # Encode the concatenated data\n",
    "        src = self.encoder(concated) * math.sqrt(self.d_model)\n",
    "\n",
    "        # Add positional encoding\n",
    "        src = self.pos_encoder(src)\n",
    "\n",
    "        # Encode the data\n",
    "        memory = self.transformer_encoder(src)\n",
    "\n",
    "        # Decode the data\n",
    "        # Encode the target data\n",
    "        tgt_encoded = self.encoder(hrir45) * math.sqrt(self.d_model)  # Apply encoding and scale\n",
    "        tgt_pos_encoded = self.pos_encoder(tgt_encoded)  # Apply positional encoding\n",
    "        # print(tgt_pos_encoded.shape, memory.shape)\n",
    "\n",
    "        # Decode the data\n",
    "        output = self.transformer_decoder(tgt_pos_encoded, memory)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "# Test the forward method\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load one batch of data\n",
    "# hrir0, angle0, hrir90, angle90, hrir45, angle45 = next(iter(train_loader))\n",
    "\n",
    "# # print(hrir0.shape)\n",
    "# # print(angle0.shape)\n",
    "# # print(hrir90.shape)\n",
    "# # print(angle90.shape)\n",
    "# # print(hrir45.shape)\n",
    "# # print(angle45.shape)\n",
    "# output = model(hrir0, angle0, hrir90, angle90, hrir45, angle45)\n",
    "# print(output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_epoch(model, data_loader, loss_function, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for hrir0, angle0, hrir90, angle90, hrir45, angle45 in data_loader:\n",
    "        # move all to device\n",
    "        hrir0, angle0, hrir90, angle90, hrir45, angle45 = hrir0.to(device), angle0.to(device), hrir90.to(device), angle90.to(device), hrir45.to(device), angle45.to(device)\n",
    "\n",
    "        # normalize between -1 and 1\n",
    "        hrir0 = (hrir0 - hrir0.mean()) / hrir0.std()\n",
    "        hrir90 = (hrir90 - hrir90.mean()) / hrir90.std()\n",
    "        hrir45 = (hrir45 - hrir45.mean()) / hrir45.std()\n",
    "\n",
    "        output = model(hrir0, angle0, hrir90, angle90, hrir45, angle45)\n",
    "\n",
    "\n",
    "        loss = loss_function(output, hrir45)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Update the learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def validate_epoch(model, data_loader, loss_function, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for hrir0, angle0, hrir90, angle90, hrir45, angle45 in data_loader:\n",
    "            # move all to device\n",
    "            hrir0, angle0, hrir90, angle90, hrir45, angle45 = hrir0.to(device), angle0.to(device), hrir90.to(device), angle90.to(device), hrir45.to(device), angle45.to(device)\n",
    "        \n",
    "            output = model(hrir0, angle0, hrir90, angle90, hrir45, angle45)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = loss_function(output, hrir45)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 2.63470819261339, Val Loss: 1.7198660850524903\n",
      "Epoch 1, Train Loss: 1.509469164742364, Val Loss: 0.09442025274038315\n",
      "Epoch 2, Train Loss: 1.048094650109609, Val Loss: 0.04537348672747612\n",
      "Epoch 3, Train Loss: 1.0296925571229723, Val Loss: 0.03418379575014115\n",
      "Epoch 4, Train Loss: 1.0272749463717143, Val Loss: 0.034359057992696763\n",
      "Epoch 5, Train Loss: 1.0272861023743947, Val Loss: 0.05225671678781509\n",
      "Epoch 6, Train Loss: 1.035758376121521, Val Loss: 0.05775032341480255\n",
      "Epoch 7, Train Loss: 1.0490141345394983, Val Loss: 0.05428018271923065\n",
      "Epoch 8, Train Loss: 1.0384901795122359, Val Loss: 0.04969988390803337\n",
      "Epoch 9, Train Loss: 1.0414156549506717, Val Loss: 0.03991222903132439\n",
      "Epoch 10, Train Loss: 1.0152022772365146, Val Loss: 0.05980220064520836\n",
      "Epoch 11, Train Loss: 0.9840053882863786, Val Loss: 0.0890192598104477\n",
      "Epoch 12, Train Loss: 0.9537650280528598, Val Loss: 0.09843572080135346\n",
      "Epoch 13, Train Loss: 0.9468417829937406, Val Loss: 0.15745626389980316\n",
      "Epoch 14, Train Loss: 0.9793609778086344, Val Loss: 0.08945726007223129\n",
      "Epoch 15, Train Loss: 0.9984128806326125, Val Loss: 0.06913783103227615\n",
      "Epoch 16, Train Loss: 0.9691601859198676, Val Loss: 0.07859684824943543\n",
      "Epoch 17, Train Loss: 0.9648952086766561, Val Loss: 0.08436853289604188\n",
      "Epoch 18, Train Loss: 1.003817425833808, Val Loss: 0.05101342722773552\n",
      "Epoch 19, Train Loss: 1.0199503733052149, Val Loss: 0.02852323167026043\n",
      "Epoch 20, Train Loss: 1.0192432469791837, Val Loss: 0.02340371198952198\n",
      "Epoch 21, Train Loss: 1.0125015477339427, Val Loss: 0.02129109725356102\n",
      "Epoch 22, Train Loss: 1.0144956873522863, Val Loss: 0.019649621471762656\n",
      "Epoch 23, Train Loss: 1.0111730496088664, Val Loss: 0.023331813141703607\n",
      "Epoch 24, Train Loss: 1.0117872489823236, Val Loss: 0.021839777752757073\n",
      "Epoch 25, Train Loss: 1.0097268753581576, Val Loss: 0.018712610006332397\n",
      "Epoch 26, Train Loss: 1.010691871245702, Val Loss: 0.01855963096022606\n",
      "Epoch 27, Train Loss: 1.006856809059779, Val Loss: 0.020326435193419458\n",
      "Epoch 28, Train Loss: 1.0090742972162035, Val Loss: 0.01874423399567604\n",
      "Epoch 29, Train Loss: 1.0072417093647852, Val Loss: 0.02042901925742626\n",
      "Epoch 30, Train Loss: 1.002249879969491, Val Loss: 0.017997298017144205\n",
      "Epoch 31, Train Loss: 1.0049084822336833, Val Loss: 0.01974724642932415\n",
      "Epoch 32, Train Loss: 1.0067220893171098, Val Loss: 0.016240695677697658\n",
      "Epoch 33, Train Loss: 0.9982017444239722, Val Loss: 0.01994691975414753\n",
      "Epoch 34, Train Loss: 1.000005453824997, Val Loss: 0.01605649422854185\n",
      "Epoch 35, Train Loss: 0.9980987674660153, Val Loss: 0.016331267915666102\n",
      "Epoch 36, Train Loss: 0.9986141688293881, Val Loss: 0.018569433689117433\n",
      "Epoch 37, Train Loss: 0.9972937338882022, Val Loss: 0.01683955118060112\n",
      "Epoch 38, Train Loss: 1.0012780692842271, Val Loss: 0.018131829984486102\n",
      "Epoch 39, Train Loss: 0.9983767304155562, Val Loss: 0.014863030053675175\n",
      "Epoch 40, Train Loss: 0.9959853788216909, Val Loss: 0.015424658171832561\n",
      "Epoch 41, Train Loss: 0.9951123429669274, Val Loss: 0.016995546594262122\n",
      "Epoch 42, Train Loss: 0.9983734687169393, Val Loss: 0.01670591253787279\n",
      "Epoch 43, Train Loss: 0.9948924779891968, Val Loss: 0.014236043393611907\n",
      "Epoch 44, Train Loss: 0.9949677056736417, Val Loss: 0.016771497204899788\n",
      "Epoch 45, Train Loss: 0.9936688211229112, Val Loss: 0.016841889917850496\n",
      "Epoch 46, Train Loss: 0.9939035044776069, Val Loss: 0.015853369794785976\n",
      "Epoch 47, Train Loss: 0.9942771957980262, Val Loss: 0.0165385402739048\n",
      "Epoch 48, Train Loss: 0.9950646062692007, Val Loss: 0.017138240858912468\n",
      "Epoch 49, Train Loss: 0.9945946103996701, Val Loss: 0.015172868967056274\n",
      "Epoch 50, Train Loss: 0.9941219786802927, Val Loss: 0.015635414607822896\n",
      "Epoch 51, Train Loss: 0.9939292106363509, Val Loss: 0.014835385978221894\n",
      "Epoch 52, Train Loss: 0.9925055272049375, Val Loss: 0.01600794158875942\n",
      "Epoch 53, Train Loss: 0.9941990574200948, Val Loss: 0.017095675319433214\n",
      "Epoch 54, Train Loss: 0.9916067686345842, Val Loss: 0.016016940027475356\n",
      "Epoch 55, Train Loss: 0.9929531580872006, Val Loss: 0.015559418126940727\n",
      "Epoch 56, Train Loss: 0.9922443926334381, Val Loss: 0.015412030182778836\n",
      "Epoch 57, Train Loss: 0.9920206334855821, Val Loss: 0.0154540978372097\n",
      "Epoch 58, Train Loss: 0.9937874476114908, Val Loss: 0.015393438003957271\n",
      "Epoch 59, Train Loss: 0.9926990502410464, Val Loss: 0.015859018079936504\n",
      "Epoch 60, Train Loss: 0.9933788544601865, Val Loss: 0.015124647878110409\n",
      "Epoch 61, Train Loss: 0.989973677529229, Val Loss: 0.01490614227950573\n",
      "Epoch 62, Train Loss: 0.9922215044498444, Val Loss: 0.01474114302545786\n",
      "Epoch 63, Train Loss: 0.9918051924970415, Val Loss: 0.014586995542049407\n",
      "Epoch 64, Train Loss: 0.992395775185691, Val Loss: 0.01538461185991764\n",
      "Epoch 65, Train Loss: 0.9910527202818129, Val Loss: 0.01474564876407385\n",
      "Epoch 66, Train Loss: 0.992394377787908, Val Loss: 0.015835816971957684\n",
      "Epoch 67, Train Loss: 0.9902432196670108, Val Loss: 0.015119099617004394\n",
      "Epoch 68, Train Loss: 0.9901005493270026, Val Loss: 0.015074800886213779\n",
      "Epoch 69, Train Loss: 0.99202761054039, Val Loss: 0.015581117197871207\n",
      "Epoch 70, Train Loss: 0.9898960789044698, Val Loss: 0.015053600445389748\n",
      "Epoch 71, Train Loss: 0.9905645151933035, Val Loss: 0.015178348496556282\n",
      "Epoch 72, Train Loss: 0.9903058972623613, Val Loss: 0.01545829176902771\n",
      "Epoch 73, Train Loss: 0.9911831451786889, Val Loss: 0.015137379243969917\n",
      "Epoch 74, Train Loss: 0.9913458426793417, Val Loss: 0.014833268150687218\n",
      "Epoch 75, Train Loss: 0.9905561870998807, Val Loss: 0.01449933610856533\n",
      "Epoch 76, Train Loss: 0.9908499850167168, Val Loss: 0.014407884143292905\n",
      "Epoch 77, Train Loss: 0.990208324458864, Val Loss: 0.01448073573410511\n",
      "Epoch 78, Train Loss: 0.9910248286194272, Val Loss: 0.014804025925695897\n",
      "Epoch 79, Train Loss: 0.9921760724650489, Val Loss: 0.014949837699532509\n",
      "Epoch 80, Train Loss: 0.9903218878640069, Val Loss: 0.014316922798752785\n",
      "Epoch 81, Train Loss: 0.9900436798731486, Val Loss: 0.01449706107378006\n",
      "Epoch 82, Train Loss: 0.9898056652810838, Val Loss: 0.014679478108882904\n",
      "Epoch 83, Train Loss: 0.9905582633283403, Val Loss: 0.014713800139725208\n",
      "Epoch 84, Train Loss: 0.9899644917911954, Val Loss: 0.014763600379228591\n",
      "Epoch 85, Train Loss: 0.9916127555900149, Val Loss: 0.014711790159344674\n",
      "Epoch 86, Train Loss: 0.9913606180085076, Val Loss: 0.014640703611075879\n",
      "Epoch 87, Train Loss: 0.9897269507249197, Val Loss: 0.01426379643380642\n",
      "Epoch 88, Train Loss: 0.9894017610285017, Val Loss: 0.01466581504791975\n",
      "Epoch 89, Train Loss: 0.9904542830255296, Val Loss: 0.014699559658765793\n",
      "Epoch 90, Train Loss: 0.9893085459868113, Val Loss: 0.014623933471739292\n",
      "Epoch 91, Train Loss: 0.9899585081471337, Val Loss: 0.014782691188156605\n",
      "Epoch 92, Train Loss: 0.9901392923461066, Val Loss: 0.0145869804546237\n",
      "Epoch 93, Train Loss: 0.9883505933814578, Val Loss: 0.014634416438639164\n",
      "Epoch 94, Train Loss: 0.989077048169242, Val Loss: 0.01484739277511835\n",
      "Epoch 95, Train Loss: 0.9902840455373129, Val Loss: 0.015098530612885953\n",
      "Epoch 96, Train Loss: 0.9891426033443875, Val Loss: 0.01506248340010643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _releaseLock at 0x7fd27d21bc10>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 227, in _releaseLock\n",
      "    def _releaseLock():\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 11513, 11529, 11530) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[0;32m/workspace/venv_work/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/multiprocessing/queues.py:108\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout):\n\u001b[0;32m--> 108\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "\u001b[0;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 71\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# Assume train_epoch and validate_epoch functions return the loss for the current epoch\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m train_epoch(model, train_loader, loss_function, optimizer, scheduler, device)\n\u001b[0;32m---> 71\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 36\u001b[0m, in \u001b[0;36mvalidate_epoch\u001b[0;34m(model, data_loader, loss_function, device)\u001b[0m\n\u001b[1;32m     33\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hrir0, angle0, hrir90, angle90, hrir45, angle45 \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;66;03m# move all to device\u001b[39;00m\n\u001b[1;32m     38\u001b[0m         hrir0, angle0, hrir90, angle90, hrir45, angle45 \u001b[38;5;241m=\u001b[39m hrir0\u001b[38;5;241m.\u001b[39mto(device), angle0\u001b[38;5;241m.\u001b[39mto(device), hrir90\u001b[38;5;241m.\u001b[39mto(device), angle90\u001b[38;5;241m.\u001b[39mto(device), hrir45\u001b[38;5;241m.\u001b[39mto(device), angle45\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     40\u001b[0m         output \u001b[38;5;241m=\u001b[39m model(hrir0, angle0, hrir90, angle90, hrir45, angle45)\n",
      "File \u001b[0;32m/workspace/venv_work/lib/python3.8/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/workspace/venv_work/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1328\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1331\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/venv_work/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1294\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1290\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1294\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1295\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1296\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/workspace/venv_work/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1145\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1144\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[0;32m-> 1145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[1;32m   1147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 11513, 11529, 11530) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/workspace/fourth_year_project/HRTF Models/')\n",
    "\n",
    "from BasicDataset import BasicDataset\n",
    "# from BasicTransformer import BasicTransformer\n",
    "\n",
    "sofa_file = '/workspace/fourth_year_project/HRTF Models/sofa_hrtfs/RIEC_hrir_subject_001.sofa'\n",
    "# Basic Dataset only loads the HRIRs at 0 degrees and 90 degrees for baseline and 45 degree for testing\n",
    "hrir_dataset = BasicDataset()\n",
    "for i in range(1,100):\n",
    "    hrir_dataset.load(sofa_file.replace('001', str(i).zfill(3)))\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "num_epochs = 200 # Number of epochs to train for\n",
    "\n",
    "# total_epochs = 100\n",
    "warmup_epochs = 10\n",
    "base_lr = 1e-3\n",
    "\n",
    "# Define lambda function for the learning rate schedule\n",
    "def lr_lambda(epoch):\n",
    "    if epoch < warmup_epochs:\n",
    "        # Linear warm-up\n",
    "        return float(epoch) / float(max(1, warmup_epochs))\n",
    "    else:\n",
    "        # Exponential decay\n",
    "        decay_rate = 0.95  # Decay rate\n",
    "        decay_epochs = epoch - warmup_epochs  # Subtract warmup epochs\n",
    "        return pow(decay_rate, decay_epochs)\n",
    "\n",
    "nhead = 8\n",
    "encoder_layer = 6\n",
    "decoder_layer = 6\n",
    "feedforward_dim = 2048\n",
    "\n",
    "\n",
    "train_size = int(0.7 * len(hrir_dataset))\n",
    "val_size = int(0.2 * len(hrir_dataset))\n",
    "test_size = len(hrir_dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(hrir_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=6)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=6)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=6)\n",
    "\n",
    "# Initialize model with the current set of hyperparameters\n",
    "model = BasicTransformer(nhead=nhead, \n",
    "                            num_encoder_layers=encoder_layer, \n",
    "                            num_decoder_layers=decoder_layer, \n",
    "                            dim_feedforward=feedforward_dim)\n",
    "model = model.to(device)  # Move model to the specified device\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Assume train_epoch and validate_epoch functions return the loss for the current epoch\n",
    "    train_loss = train_epoch(model, train_loader, loss_function, optimizer, scheduler, device)\n",
    "    val_loss = validate_epoch(model, val_loader, loss_function, device)\n",
    "    print(f'Epoch {epoch}, Train Loss: {train_loss}, Val Loss: {val_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "device = torch.device('cuda')\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "num_epochs = 200 # Number of epochs to train for\n",
    "\n",
    "# total_epochs = 100\n",
    "warmup_epochs = 10\n",
    "base_lr = 1e-3\n",
    "\n",
    "# Define lambda function for the learning rate schedule\n",
    "def lr_lambda(epoch):\n",
    "    if epoch < warmup_epochs:\n",
    "        # Linear warm-up\n",
    "        return float(epoch) / float(max(1, warmup_epochs))\n",
    "    else:\n",
    "        # Exponential decay\n",
    "        decay_rate = 0.95  # Decay rate\n",
    "        decay_epochs = epoch - warmup_epochs  # Subtract warmup epochs\n",
    "        return pow(decay_rate, decay_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model with parameters: nhead=8, num_encoder_layers=4, num_decoder_layers=4, dim_feedforward=1024\n",
      "Min Train Loss: 0.0670, Avg Train Loss: 0.1538, Median Train Loss: 0.0745\n",
      "Min Validation Loss: 0.0384, Avg Validation Loss: 0.5668, Median Validation Loss: 0.6198\n",
      "\n",
      "\n",
      "Model with parameters: nhead=8, num_encoder_layers=4, num_decoder_layers=4, dim_feedforward=2048\n",
      "Min Train Loss: 0.2655, Avg Train Loss: 0.4278, Median Train Loss: 0.3027\n",
      "Min Validation Loss: 0.0451, Avg Validation Loss: 0.4325, Median Validation Loss: 0.5184\n",
      "\n",
      "\n",
      "Model with parameters: nhead=8, num_encoder_layers=4, num_decoder_layers=4, dim_feedforward=4096\n",
      "Min Train Loss: 0.7858, Avg Train Loss: 0.9830, Median Train Loss: 0.9862\n",
      "Min Validation Loss: 0.0181, Avg Validation Loss: 0.0300, Median Validation Loss: 0.0194\n",
      "\n",
      "\n",
      "Model with parameters: nhead=8, num_encoder_layers=4, num_decoder_layers=4, dim_feedforward=8192\n",
      "Min Train Loss: 0.8682, Avg Train Loss: 0.9884, Median Train Loss: 0.9886\n",
      "Min Validation Loss: 0.0146, Avg Validation Loss: 0.0240, Median Validation Loss: 0.0153\n",
      "\n",
      "\n",
      "Model with parameters: nhead=8, num_encoder_layers=4, num_decoder_layers=6, dim_feedforward=1024\n",
      "Min Train Loss: 0.8897, Avg Train Loss: 0.9188, Median Train Loss: 0.9093\n",
      "Min Validation Loss: 0.0331, Avg Validation Loss: 0.0688, Median Validation Loss: 0.0705\n",
      "\n",
      "\n",
      "Model with parameters: nhead=8, num_encoder_layers=4, num_decoder_layers=6, dim_feedforward=2048\n",
      "Min Train Loss: 0.7614, Avg Train Loss: 0.8447, Median Train Loss: 0.8026\n",
      "Min Validation Loss: 0.0164, Avg Validation Loss: 0.1037, Median Validation Loss: 0.1191\n",
      "\n",
      "\n",
      "Model with parameters: nhead=8, num_encoder_layers=4, num_decoder_layers=6, dim_feedforward=4096\n",
      "Min Train Loss: 0.9063, Avg Train Loss: 0.9394, Median Train Loss: 0.9233\n",
      "Min Validation Loss: 0.0167, Avg Validation Loss: 0.0294, Median Validation Loss: 0.0300\n",
      "\n",
      "\n",
      "Model with parameters: nhead=8, num_encoder_layers=4, num_decoder_layers=6, dim_feedforward=8192\n",
      "Min Train Loss: 0.9858, Avg Train Loss: 0.9933, Median Train Loss: 0.9902\n",
      "Min Validation Loss: 0.0138, Avg Validation Loss: 0.0175, Median Validation Loss: 0.0149\n",
      "\n",
      "\n",
      "Model with parameters: nhead=8, num_encoder_layers=4, num_decoder_layers=8, dim_feedforward=1024\n",
      "Min Train Loss: 0.1483, Avg Train Loss: 0.4370, Median Train Loss: 0.1964\n",
      "Min Validation Loss: 0.0205, Avg Validation Loss: 0.3338, Median Validation Loss: 0.4728\n",
      "\n",
      "\n",
      "Model with parameters: nhead=8, num_encoder_layers=4, num_decoder_layers=8, dim_feedforward=2048\n",
      "Min Train Loss: 0.1264, Avg Train Loss: 0.3534, Median Train Loss: 0.1543\n",
      "Min Validation Loss: 0.0228, Avg Validation Loss: 0.3621, Median Validation Loss: 0.4629\n",
      "\n",
      "\n",
      "Model with parameters: nhead=8, num_encoder_layers=4, num_decoder_layers=8, dim_feedforward=4096\n",
      "Min Train Loss: 0.1046, Avg Train Loss: 0.3961, Median Train Loss: 0.1382\n",
      "Min Validation Loss: 0.0209, Avg Validation Loss: 0.3689, Median Validation Loss: 0.5479\n",
      "\n",
      "\n",
      "Model with parameters: nhead=8, num_encoder_layers=4, num_decoder_layers=8, dim_feedforward=8192\n",
      "Min Train Loss: 0.9850, Avg Train Loss: 0.9911, Median Train Loss: 0.9879\n",
      "Min Validation Loss: 0.0160, Avg Validation Loss: 0.0195, Median Validation Loss: 0.0171\n",
      "\n",
      "\n",
      "Model with parameters: nhead=8, num_encoder_layers=4, num_decoder_layers=10, dim_feedforward=1024\n",
      "Min Train Loss: 0.5116, Avg Train Loss: 0.6178, Median Train Loss: 0.5389\n",
      "Min Validation Loss: 0.0248, Avg Validation Loss: 0.2364, Median Validation Loss: 0.2963\n",
      "\n",
      "\n",
      "Model with parameters: nhead=8, num_encoder_layers=4, num_decoder_layers=10, dim_feedforward=2048\n",
      "Min Train Loss: 0.3084, Avg Train Loss: 0.6030, Median Train Loss: 0.5886\n",
      "Min Validation Loss: 0.0262, Avg Validation Loss: 0.2099, Median Validation Loss: 0.2657\n",
      "\n",
      "\n",
      "Model with parameters: nhead=8, num_encoder_layers=4, num_decoder_layers=10, dim_feedforward=4096\n",
      "Min Train Loss: 0.1665, Avg Train Loss: 0.4697, Median Train Loss: 0.2861\n",
      "Min Validation Loss: 0.0157, Avg Validation Loss: 0.2904, Median Validation Loss: 0.2717\n",
      "\n",
      "\n",
      "Model with parameters: nhead=8, num_encoder_layers=4, num_decoder_layers=10, dim_feedforward=8192\n",
      "Min Train Loss: 0.9820, Avg Train Loss: 0.9888, Median Train Loss: 0.9857\n",
      "Min Validation Loss: 0.0187, Avg Validation Loss: 0.0226, Median Validation Loss: 0.0202\n",
      "\n",
      "\n",
      "Model with parameters: nhead=8, num_encoder_layers=6, num_decoder_layers=4, dim_feedforward=1024\n",
      "Min Train Loss: 0.1148, Avg Train Loss: 0.2464, Median Train Loss: 0.1277\n",
      "Min Validation Loss: 0.0266, Avg Validation Loss: 0.5995, Median Validation Loss: 0.7170\n",
      "\n",
      "\n",
      "Model with parameters: nhead=8, num_encoder_layers=6, num_decoder_layers=4, dim_feedforward=2048\n",
      "Min Train Loss: 0.1228, Avg Train Loss: 0.2590, Median Train Loss: 0.1368\n",
      "Min Validation Loss: 0.0419, Avg Validation Loss: 0.5732, Median Validation Loss: 0.6770\n",
      "\n",
      "\n",
      "Model with parameters: nhead=8, num_encoder_layers=6, num_decoder_layers=4, dim_feedforward=4096\n",
      "Min Train Loss: 0.7802, Avg Train Loss: 0.8459, Median Train Loss: 0.8222\n",
      "Min Validation Loss: 0.0297, Avg Validation Loss: 0.1218, Median Validation Loss: 0.1258\n",
      "\n",
      "\n",
      "Model with parameters: nhead=8, num_encoder_layers=6, num_decoder_layers=4, dim_feedforward=8192\n",
      "Min Train Loss: 0.3846, Avg Train Loss: 0.5469, Median Train Loss: 0.4314\n",
      "Min Validation Loss: 0.0305, Avg Validation Loss: 0.3966, Median Validation Loss: 0.5081\n",
      "\n",
      "\n",
      "Model with parameters: nhead=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=1024\n",
      "Min Train Loss: 0.5006, Avg Train Loss: 0.6410, Median Train Loss: 0.5521\n",
      "Min Validation Loss: 0.0366, Avg Validation Loss: 0.2515, Median Validation Loss: 0.3088\n",
      "\n",
      "\n",
      "Model with parameters: nhead=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048\n",
      "Min Train Loss: 0.9725, Avg Train Loss: 0.9926, Median Train Loss: 0.9897\n",
      "Min Validation Loss: 0.0140, Avg Validation Loss: 0.0188, Median Validation Loss: 0.0146\n",
      "\n",
      "\n",
      "Model with parameters: nhead=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=4096\n",
      "Min Train Loss: 0.9836, Avg Train Loss: 0.9902, Median Train Loss: 0.9869\n",
      "Min Validation Loss: 0.0181, Avg Validation Loss: 0.0215, Median Validation Loss: 0.0189\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# Assume train_epoch and validate_epoch functions return the loss for the current epoch\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m     train_loss\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     39\u001b[0m     val_loss\u001b[38;5;241m.\u001b[39mappend(validate_epoch(model, val_loader, loss_function, device))\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# remove first 10 epochs from the list\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 21\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, data_loader, loss_function, optimizer, scheduler, device)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Backward pass and optimize\u001b[39;00m\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 21\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     24\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/workspace/venv_work/lib/python3.8/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/venv_work/lib/python3.8/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "nheads = [8, 16, 32]\n",
    "num_encoder_layers = [4, 6, 8, 10]\n",
    "num_decoder_layers = [4, 6, 8, 10]\n",
    "dim_feedforward = [1024, 2048, 4096, 8192]\n",
    "\n",
    "# Assume num_epochs, train_loader, val_loader, loss_function, optimizer, scheduler, device are defined\n",
    "hyper_dict = {}\n",
    "for nhead in nheads:\n",
    "    for encoder_layer in num_encoder_layers:\n",
    "        for decoder_layer in num_decoder_layers:\n",
    "            for feedforward_dim in dim_feedforward:\n",
    "                train_size = int(0.7 * len(hrir_dataset))\n",
    "                val_size = int(0.2 * len(hrir_dataset))\n",
    "                test_size = len(hrir_dataset) - train_size - val_size\n",
    "                train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(hrir_dataset, [train_size, val_size, test_size])\n",
    "                \n",
    "                batch_size = 4\n",
    "\n",
    "                train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=6)\n",
    "                val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=6)\n",
    "                test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=6)\n",
    "                # Initialize model with the current set of hyperparameters\n",
    "                model = BasicTransformer(nhead=nhead, \n",
    "                                         num_encoder_layers=encoder_layer, \n",
    "                                         num_decoder_layers=decoder_layer, \n",
    "                                         dim_feedforward=feedforward_dim)\n",
    "                model = model.to(device)  # Move model to the specified device\n",
    "                optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "                # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "                scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "                train_loss = []\n",
    "                val_loss = []\n",
    "                for epoch in range(num_epochs):\n",
    "                    # Assume train_epoch and validate_epoch functions return the loss for the current epoch\n",
    "                    train_loss.append(train_epoch(model, train_loader, loss_function, optimizer, scheduler, device))\n",
    "                    val_loss.append(validate_epoch(model, val_loader, loss_function, device))\n",
    "                # remove first 10 epochs from the list\n",
    "                train_loss = train_loss[10:]\n",
    "                val_loss = val_loss[3:]\n",
    "                \n",
    "                # Add all variables to the dictionary\n",
    "                hyper_dict[(nhead, encoder_layer, decoder_layer, feedforward_dim)] = (train_loss, val_loss, np.min(val_loss), np.mean(val_loss), np.median(val_loss))\n",
    "\n",
    "                # Print model parameters and loss statistics\n",
    "\n",
    "\n",
    "                print(f'\\nModel with parameters: nhead={nhead}, num_encoder_layers={encoder_layer}, num_decoder_layers={decoder_layer}, dim_feedforward={feedforward_dim}')\n",
    "                print(f'Min Train Loss: {np.min(train_loss):.4f}, Avg Train Loss: {np.mean(train_loss):.4f}, Median Train Loss: {np.median(train_loss):.4f}')\n",
    "                print(f'Min Validation Loss: {np.min(val_loss):.4f}, Avg Validation Loss: {np.mean(val_loss):.4f}, Median Validation Loss: {np.median(val_loss):.4f}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0131\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def test_and_plot(model, test_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    with torch.no_grad():  # No need to track gradients\n",
    "        # Get one batch from the test_loader\n",
    "        for hrir0, angle0, hrir90, angle90, hrir45, angle45 in test_loader:\n",
    "            # move all to device\n",
    "            hrir0, angle0, hrir90, angle90, hrir45, angle45 = hrir0.to(device), angle0.to(device), hrir90.to(device), angle90.to(device), hrir45.to(device), angle45.to(device)\n",
    "        \n",
    "            output = model(hrir0, angle0, hrir90, angle90, hrir45, angle45)\n",
    "            \n",
    "            # Plot the first sample in the batch for src, target, and output\n",
    "            plt.figure(figsize=(15, 5))\n",
    "            \n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.plot(hrir0[0, 0].cpu().numpy(), label='Source')\n",
    "            plt.title('Source')\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.plot(hrir45[0, 0].cpu().numpy(), label='Target')\n",
    "            plt.title('Target')\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.plot(output[0, 0].cpu().numpy(), label='Output')\n",
    "            plt.title('Output')\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.show()\n",
    "\n",
    "            # Print out the loss\n",
    "            loss = loss_function(output, hrir45)\n",
    "            print(f'Loss: {loss.item():.4f}')\n",
    "            \n",
    "            # Since we only want to plot for the first element, break after the first batch\n",
    "            break\n",
    "\n",
    "\n",
    "# Assuming you have a model, test_loader, and device defined\n",
    "test_and_plot(model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = '''\n",
    "Model with parameters: nhead=4, num_encoder_layers=2, num_decoder_layers=2, dim_feedforward=128\n",
    "Min Train Loss: 0.6338, Avg Train Loss: 0.7705, Median Train Loss: 0.7018\n",
    "Min Validation Loss: 0.0584, Avg Validation Loss: 0.2511, Median Validation Loss: 0.3002\n",
    "Model with parameters: nhead=4, num_encoder_layers=2, num_decoder_layers=2, dim_feedforward=256\n",
    "Min Train Loss: 0.6339, Avg Train Loss: 0.7610, Median Train Loss: 0.7054\n",
    "Min Validation Loss: 0.0708, Avg Validation Loss: 0.2011, Median Validation Loss: 0.2160\n",
    "Model with parameters: nhead=4, num_encoder_layers=2, num_decoder_layers=2, dim_feedforward=512\n",
    "Min Train Loss: 0.5203, Avg Train Loss: 0.6520, Median Train Loss: 0.5752\n",
    "Min Validation Loss: 0.0543, Avg Validation Loss: 0.2358, Median Validation Loss: 0.2624\n",
    "Model with parameters: nhead=4, num_encoder_layers=2, num_decoder_layers=2, dim_feedforward=1024\n",
    "Min Train Loss: 0.4543, Avg Train Loss: 0.6099, Median Train Loss: 0.5182\n",
    "Min Validation Loss: 0.0905, Avg Validation Loss: 0.4602, Median Validation Loss: 0.5220\n",
    "Model with parameters: nhead=4, num_encoder_layers=2, num_decoder_layers=2, dim_feedforward=2048\n",
    "Min Train Loss: nan, Avg Train Loss: nan, Median Train Loss: nan\n",
    "Min Validation Loss: nan, Avg Validation Loss: nan, Median Validation Loss: nan\n",
    "Model with parameters: nhead=4, num_encoder_layers=2, num_decoder_layers=2, dim_feedforward=4096\n",
    "Min Train Loss: 0.7329, Avg Train Loss: 0.8400, Median Train Loss: 0.7822\n",
    "Min Validation Loss: 0.0658, Avg Validation Loss: 0.2734, Median Validation Loss: 0.3296\n",
    "Model with parameters: nhead=4, num_encoder_layers=2, num_decoder_layers=2, dim_feedforward=8192\n",
    "Min Train Loss: nan, Avg Train Loss: nan, Median Train Loss: nan\n",
    "Min Validation Loss: nan, Avg Validation Loss: nan, Median Validation Loss: nan\n",
    "Model with parameters: nhead=4, num_encoder_layers=2, num_decoder_layers=3, dim_feedforward=128\n",
    "Min Train Loss: 0.9687, Avg Train Loss: 1.0126, Median Train Loss: 0.9907\n",
    "Min Validation Loss: 0.0187, Avg Validation Loss: 0.0298, Median Validation Loss: 0.0193\n",
    "Model with parameters: nhead=4, num_encoder_layers=2, num_decoder_layers=3, dim_feedforward=256\n",
    "Min Train Loss: 0.9557, Avg Train Loss: 1.0177, Median Train Loss: 0.9944\n",
    "Min Validation Loss: 0.0181, Avg Validation Loss: 0.0286, Median Validation Loss: 0.0195\n",
    "Model with parameters: nhead=4, num_encoder_layers=2, num_decoder_layers=3, dim_feedforward=512\n",
    "Min Train Loss: 0.9382, Avg Train Loss: 0.9887, Median Train Loss: 0.9610\n",
    "Min Validation Loss: 0.0176, Avg Validation Loss: 0.0439, Median Validation Loss: 0.0379\n",
    "Model with parameters: nhead=4, num_encoder_layers=2, num_decoder_layers=3, dim_feedforward=1024\n",
    "Min Train Loss: nan, Avg Train Loss: nan, Median Train Loss: nan\n",
    "Min Validation Loss: nan, Avg Validation Loss: nan, Median Validation Loss: nan\n",
    "Model with parameters: nhead=4, num_encoder_layers=2, num_decoder_layers=3, dim_feedforward=2048\n",
    "Min Train Loss: 0.9883, Avg Train Loss: 1.0205, Median Train Loss: 0.9964\n",
    "Min Validation Loss: 0.0181, Avg Validation Loss: 0.0268, Median Validation Loss: 0.0189\n",
    "Model with parameters: nhead=4, num_encoder_layers=2, num_decoder_layers=3, dim_feedforward=4096\n",
    "Min Train Loss: 0.9890, Avg Train Loss: 1.0231, Median Train Loss: 0.9957\n",
    "Min Validation Loss: 0.0195, Avg Validation Loss: 0.0268, Median Validation Loss: 0.0204\n",
    "Model with parameters: nhead=4, num_encoder_layers=2, num_decoder_layers=3, dim_feedforward=8192\n",
    "Min Train Loss: 0.9880, Avg Train Loss: 1.0238, Median Train Loss: 0.9961\n",
    "Min Validation Loss: 0.0191, Avg Validation Loss: 0.0256, Median Validation Loss: 0.0204\n",
    "Model with parameters: nhead=4, num_encoder_layers=2, num_decoder_layers=4, dim_feedforward=128\n",
    "Min Train Loss: 0.9815, Avg Train Loss: 1.0207, Median Train Loss: 0.9924\n",
    "Min Validation Loss: 0.0182, Avg Validation Loss: 0.0273, Median Validation Loss: 0.0207\n",
    "Model with parameters: nhead=4, num_encoder_layers=2, num_decoder_layers=4, dim_feedforward=256\n",
    "Min Train Loss: 0.9443, Avg Train Loss: 1.0037, Median Train Loss: 0.9748\n",
    "Min Validation Loss: 0.0217, Avg Validation Loss: 0.0305, Median Validation Loss: 0.0243\n",
    "Model with parameters: nhead=4, num_encoder_layers=2, num_decoder_layers=4, dim_feedforward=512\n",
    "Min Train Loss: 0.9638, Avg Train Loss: 1.0118, Median Train Loss: 0.9849\n",
    "Min Validation Loss: 0.0221, Avg Validation Loss: 0.0359, Median Validation Loss: 0.0347\n",
    "Model with parameters: nhead=4, num_encoder_layers=2, num_decoder_layers=4, dim_feedforward=1024\n",
    "Min Train Loss: 0.9581, Avg Train Loss: 1.0078, Median Train Loss: 0.9773\n",
    "Min Validation Loss: 0.0165, Avg Validation Loss: 0.0267, Median Validation Loss: 0.0249\n",
    "Model with parameters: nhead=4, num_encoder_layers=2, num_decoder_layers=4, dim_feedforward=2048\n",
    "Min Train Loss: 0.9827, Avg Train Loss: 1.0173, Median Train Loss: 0.9901\n",
    "Min Validation Loss: 0.0238, Avg Validation Loss: 0.0308, Median Validation Loss: 0.0265\n",
    "Model with parameters: nhead=4, num_encoder_layers=2, num_decoder_layers=4, dim_feedforward=4096\n",
    "Min Train Loss: 0.9787, Avg Train Loss: 1.0168, Median Train Loss: 0.9901\n",
    "Min Validation Loss: 0.0215, Avg Validation Loss: 0.0273, Median Validation Loss: 0.0229\n",
    "Model with parameters: nhead=4, num_encoder_layers=2, num_decoder_layers=4, dim_feedforward=8192\n",
    "Min Train Loss: 0.8862, Avg Train Loss: 0.9773, Median Train Loss: 0.9300\n",
    "Min Validation Loss: 0.0236, Avg Validation Loss: 0.0566, Median Validation Loss: 0.0640\n",
    "Model with parameters: nhead=4, num_encoder_layers=2, num_decoder_layers=5, dim_feedforward=128\n",
    "Min Train Loss: 0.9561, Avg Train Loss: 1.0090, Median Train Loss: 0.9804\n",
    "Min Validation Loss: 0.0178, Avg Validation Loss: 0.0296, Median Validation Loss: 0.0252\n",
    "Model with parameters: nhead=4, num_encoder_layers=2, num_decoder_layers=5, dim_feedforward=256\n",
    "Min Train Loss: 0.9780, Avg Train Loss: 1.0186, Median Train Loss: 0.9902\n",
    "Min Validation Loss: 0.0178, Avg Validation Loss: 0.0250, Median Validation Loss: 0.0191\n",
    "Model with parameters: nhead=4, num_encoder_layers=2, num_decoder_layers=5, dim_feedforward=512\n",
    "Min Train Loss: 0.9599, Avg Train Loss: 1.0082, Median Train Loss: 0.9810\n",
    "Min Validation Loss: 0.0195, Avg Validation Loss: 0.0282, Median Validation Loss: 0.0241\n",
    "Model with parameters: nhead=4, num_encoder_layers=2, num_decoder_layers=5, dim_feedforward=1024\n",
    "Min Train Loss: 0.9636, Avg Train Loss: 1.0128, Median Train Loss: 0.9853\n",
    "Min Validation Loss: 0.0186, Avg Validation Loss: 0.0306, Median Validation Loss: 0.0274\n",
    "Model with parameters: nhead=4, num_encoder_layers=2, num_decoder_layers=5, dim_feedforward=2048\n",
    "Min Train Loss: 0.9554, Avg Train Loss: 1.0090, Median Train Loss: 0.9810\n",
    "Min Validation Loss: 0.0188, Avg Validation Loss: 0.0257, Median Validation Loss: 0.0225\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nhead': 4, 'num_encoder_layers': 2, 'num_decoder_layers': 2, 'dim_feedforward': 128, 'min_train_loss': 0.6338, 'avg_train_loss': 0.7705, 'median_train_loss': 0.7018, 'min_val_loss': 0.0584, 'avg_val_loss': 0.2511, 'median_val_loss': 0.3002}\n",
      "{'nhead': 4, 'num_encoder_layers': 2, 'num_decoder_layers': 2, 'dim_feedforward': 256, 'min_train_loss': 0.6339, 'avg_train_loss': 0.761, 'median_train_loss': 0.7054, 'min_val_loss': 0.0708, 'avg_val_loss': 0.2011, 'median_val_loss': 0.216}\n",
      "{'nhead': 4, 'num_encoder_layers': 2, 'num_decoder_layers': 2, 'dim_feedforward': 512, 'min_train_loss': 0.5203, 'avg_train_loss': 0.652, 'median_train_loss': 0.5752, 'min_val_loss': 0.0543, 'avg_val_loss': 0.2358, 'median_val_loss': 0.2624}\n",
      "{'nhead': 4, 'num_encoder_layers': 2, 'num_decoder_layers': 2, 'dim_feedforward': 1024, 'min_train_loss': 0.4543, 'avg_train_loss': 0.6099, 'median_train_loss': 0.5182, 'min_val_loss': 0.0905, 'avg_val_loss': 0.4602, 'median_val_loss': 0.522}\n",
      "{'nhead': 4, 'num_encoder_layers': 2, 'num_decoder_layers': 2, 'dim_feedforward': 4096, 'min_train_loss': 0.7329, 'avg_train_loss': 0.84, 'median_train_loss': 0.7822, 'min_val_loss': 0.0658, 'avg_val_loss': 0.2734, 'median_val_loss': 0.3296}\n",
      "{'nhead': 4, 'num_encoder_layers': 2, 'num_decoder_layers': 3, 'dim_feedforward': 128, 'min_train_loss': 0.9687, 'avg_train_loss': 1.0126, 'median_train_loss': 0.9907, 'min_val_loss': 0.0187, 'avg_val_loss': 0.0298, 'median_val_loss': 0.0193}\n",
      "{'nhead': 4, 'num_encoder_layers': 2, 'num_decoder_layers': 3, 'dim_feedforward': 256, 'min_train_loss': 0.9557, 'avg_train_loss': 1.0177, 'median_train_loss': 0.9944, 'min_val_loss': 0.0181, 'avg_val_loss': 0.0286, 'median_val_loss': 0.0195}\n",
      "{'nhead': 4, 'num_encoder_layers': 2, 'num_decoder_layers': 3, 'dim_feedforward': 512, 'min_train_loss': 0.9382, 'avg_train_loss': 0.9887, 'median_train_loss': 0.961, 'min_val_loss': 0.0176, 'avg_val_loss': 0.0439, 'median_val_loss': 0.0379}\n",
      "{'nhead': 4, 'num_encoder_layers': 2, 'num_decoder_layers': 3, 'dim_feedforward': 2048, 'min_train_loss': 0.9883, 'avg_train_loss': 1.0205, 'median_train_loss': 0.9964, 'min_val_loss': 0.0181, 'avg_val_loss': 0.0268, 'median_val_loss': 0.0189}\n",
      "{'nhead': 4, 'num_encoder_layers': 2, 'num_decoder_layers': 3, 'dim_feedforward': 4096, 'min_train_loss': 0.989, 'avg_train_loss': 1.0231, 'median_train_loss': 0.9957, 'min_val_loss': 0.0195, 'avg_val_loss': 0.0268, 'median_val_loss': 0.0204}\n",
      "{'nhead': 4, 'num_encoder_layers': 2, 'num_decoder_layers': 3, 'dim_feedforward': 8192, 'min_train_loss': 0.988, 'avg_train_loss': 1.0238, 'median_train_loss': 0.9961, 'min_val_loss': 0.0191, 'avg_val_loss': 0.0256, 'median_val_loss': 0.0204}\n",
      "{'nhead': 4, 'num_encoder_layers': 2, 'num_decoder_layers': 4, 'dim_feedforward': 128, 'min_train_loss': 0.9815, 'avg_train_loss': 1.0207, 'median_train_loss': 0.9924, 'min_val_loss': 0.0182, 'avg_val_loss': 0.0273, 'median_val_loss': 0.0207}\n",
      "{'nhead': 4, 'num_encoder_layers': 2, 'num_decoder_layers': 4, 'dim_feedforward': 256, 'min_train_loss': 0.9443, 'avg_train_loss': 1.0037, 'median_train_loss': 0.9748, 'min_val_loss': 0.0217, 'avg_val_loss': 0.0305, 'median_val_loss': 0.0243}\n",
      "{'nhead': 4, 'num_encoder_layers': 2, 'num_decoder_layers': 4, 'dim_feedforward': 512, 'min_train_loss': 0.9638, 'avg_train_loss': 1.0118, 'median_train_loss': 0.9849, 'min_val_loss': 0.0221, 'avg_val_loss': 0.0359, 'median_val_loss': 0.0347}\n",
      "{'nhead': 4, 'num_encoder_layers': 2, 'num_decoder_layers': 4, 'dim_feedforward': 1024, 'min_train_loss': 0.9581, 'avg_train_loss': 1.0078, 'median_train_loss': 0.9773, 'min_val_loss': 0.0165, 'avg_val_loss': 0.0267, 'median_val_loss': 0.0249}\n",
      "{'nhead': 4, 'num_encoder_layers': 2, 'num_decoder_layers': 4, 'dim_feedforward': 2048, 'min_train_loss': 0.9827, 'avg_train_loss': 1.0173, 'median_train_loss': 0.9901, 'min_val_loss': 0.0238, 'avg_val_loss': 0.0308, 'median_val_loss': 0.0265}\n",
      "{'nhead': 4, 'num_encoder_layers': 2, 'num_decoder_layers': 4, 'dim_feedforward': 4096, 'min_train_loss': 0.9787, 'avg_train_loss': 1.0168, 'median_train_loss': 0.9901, 'min_val_loss': 0.0215, 'avg_val_loss': 0.0273, 'median_val_loss': 0.0229}\n",
      "{'nhead': 4, 'num_encoder_layers': 2, 'num_decoder_layers': 4, 'dim_feedforward': 8192, 'min_train_loss': 0.8862, 'avg_train_loss': 0.9773, 'median_train_loss': 0.93, 'min_val_loss': 0.0236, 'avg_val_loss': 0.0566, 'median_val_loss': 0.064}\n",
      "{'nhead': 4, 'num_encoder_layers': 2, 'num_decoder_layers': 5, 'dim_feedforward': 128, 'min_train_loss': 0.9561, 'avg_train_loss': 1.009, 'median_train_loss': 0.9804, 'min_val_loss': 0.0178, 'avg_val_loss': 0.0296, 'median_val_loss': 0.0252}\n",
      "{'nhead': 4, 'num_encoder_layers': 2, 'num_decoder_layers': 5, 'dim_feedforward': 256, 'min_train_loss': 0.978, 'avg_train_loss': 1.0186, 'median_train_loss': 0.9902, 'min_val_loss': 0.0178, 'avg_val_loss': 0.025, 'median_val_loss': 0.0191}\n",
      "{'nhead': 4, 'num_encoder_layers': 2, 'num_decoder_layers': 5, 'dim_feedforward': 512, 'min_train_loss': 0.9599, 'avg_train_loss': 1.0082, 'median_train_loss': 0.981, 'min_val_loss': 0.0195, 'avg_val_loss': 0.0282, 'median_val_loss': 0.0241}\n",
      "{'nhead': 4, 'num_encoder_layers': 2, 'num_decoder_layers': 5, 'dim_feedforward': 1024, 'min_train_loss': 0.9636, 'avg_train_loss': 1.0128, 'median_train_loss': 0.9853, 'min_val_loss': 0.0186, 'avg_val_loss': 0.0306, 'median_val_loss': 0.0274}\n",
      "{'nhead': 4, 'num_encoder_layers': 2, 'num_decoder_layers': 5, 'dim_feedforward': 2048, 'min_train_loss': 0.9554, 'avg_train_loss': 1.009, 'median_train_loss': 0.981, 'min_val_loss': 0.0188, 'avg_val_loss': 0.0257, 'median_val_loss': 0.0225}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern = r\"Model with parameters: nhead=(\\d+), num_encoder_layers=(\\d+), num_decoder_layers=(\\d+), dim_feedforward=(\\d+)\\s\" \\\n",
    "          r\"Min Train Loss: ([\\d.]+), Avg Train Loss: ([\\d.]+), Median Train Loss: ([\\d.]+)\\s\" \\\n",
    "          r\"Min Validation Loss: ([\\d.]+), Avg Validation Loss: ([\\d.]+), Median Validation Loss: ([\\d.]+)\"\n",
    "\n",
    "# Find all matches\n",
    "matches = re.findall(pattern, output)\n",
    "\n",
    "# Convert matches to list of dictionaries\n",
    "results = []\n",
    "for match in matches:\n",
    "    result = {\n",
    "        \"nhead\": int(match[0]),\n",
    "        \"num_encoder_layers\": int(match[1]),\n",
    "        \"num_decoder_layers\": int(match[2]),\n",
    "        \"dim_feedforward\": int(match[3]),\n",
    "        \"min_train_loss\": float(match[4]),\n",
    "        \"avg_train_loss\": float(match[5]),\n",
    "        \"median_train_loss\": float(match[6]),\n",
    "        \"min_val_loss\": float(match[7]),\n",
    "        \"avg_val_loss\": float(match[8]),\n",
    "        \"median_val_loss\": float(match[9])\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "# Print the extracted information\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(4,\n",
       "  2,\n",
       "  2,\n",
       "  128): ([1.1940816442171733,\n",
       "   1.0939471754762862,\n",
       "   1.0284080935849085,\n",
       "   0.9849201507038541,\n",
       "   0.9284305274486542,\n",
       "   0.8555513785945045,\n",
       "   0.8115296992990706,\n",
       "   0.7847364644209543,\n",
       "   0.7488832771778107,\n",
       "   0.7087344891495175,\n",
       "   0.6742972864045037,\n",
       "   0.661608960893419,\n",
       "   0.6093938681814406,\n",
       "   0.5888364182578193,\n",
       "   0.5808214777045779,\n",
       "   0.5528363254335191,\n",
       "   0.5423910601271523,\n",
       "   0.5216421882311503,\n",
       "   0.5021763245264689,\n",
       "   0.4928634729650285,\n",
       "   0.47983258962631226,\n",
       "   0.47216226988368565,\n",
       "   0.47290630141894024,\n",
       "   0.4421597603294585,\n",
       "   0.43310896224445766,\n",
       "   0.4214458051655028,\n",
       "   0.40905476609865826,\n",
       "   0.4081724137067795,\n",
       "   0.4017705106072956,\n",
       "   0.4068363805611928,\n",
       "   0.3835386269622379,\n",
       "   0.3941490484608544,\n",
       "   0.37253745396931964,\n",
       "   0.37216146455870736,\n",
       "   0.37871281968222725,\n",
       "   0.3550049430794186,\n",
       "   0.3573419435156716,\n",
       "   0.35335756838321686,\n",
       "   0.34944025178750354,\n",
       "   0.3502366029553943,\n",
       "   0.3384972694847319,\n",
       "   0.32914425598250496,\n",
       "   0.3379030641582277,\n",
       "   0.3429259657859802,\n",
       "   0.3207767191860411,\n",
       "   0.310042191710737,\n",
       "   0.30552996860610115,\n",
       "   0.2967173498537805,\n",
       "   0.3481905361016591,\n",
       "   0.317067038681772,\n",
       "   0.30716312759452397,\n",
       "   0.30510759353637695,\n",
       "   0.30101843343840706,\n",
       "   0.3070204332470894,\n",
       "   0.30148111780484516,\n",
       "   0.29501398652791977,\n",
       "   0.3061249670055177,\n",
       "   0.2899842792087131,\n",
       "   0.29900531305207145,\n",
       "   0.2905224975612428,\n",
       "   0.2847221576505237,\n",
       "   0.28501158704360324,\n",
       "   0.29045821809106404,\n",
       "   0.2833151759372817,\n",
       "   0.297960538831022,\n",
       "   0.281486451625824,\n",
       "   0.27892007182041806,\n",
       "   0.27358797523710465,\n",
       "   0.27780351373884415,\n",
       "   0.2693345728847716,\n",
       "   0.27353866812255645,\n",
       "   0.2805527647336324,\n",
       "   0.27488842027054894,\n",
       "   0.27484758612182403,\n",
       "   0.2645553863710827,\n",
       "   0.2674380905098385,\n",
       "   0.2708304648598035,\n",
       "   0.27610502474837834,\n",
       "   0.26982564893033767,\n",
       "   0.27634190602435005,\n",
       "   0.26950445688433117,\n",
       "   0.2661103755235672,\n",
       "   0.27480025920603013,\n",
       "   0.26397350761625504,\n",
       "   0.2704590964648459,\n",
       "   0.27592845178312725,\n",
       "   0.26247626956966186,\n",
       "   0.27554813689655727,\n",
       "   0.26503405306074357,\n",
       "   0.2740631981028451,\n",
       "   0.26830478178130257,\n",
       "   0.2629234906699922,\n",
       "   0.2787777706980705,\n",
       "   0.259647972881794,\n",
       "   0.2657136337624656,\n",
       "   0.25835579219791627,\n",
       "   0.2725286152627733,\n",
       "   0.27363988425996566,\n",
       "   0.26305988017055726,\n",
       "   0.2638584151864052,\n",
       "   0.26904621223608655,\n",
       "   0.2633126477400462,\n",
       "   0.26916175666782594,\n",
       "   0.28182024012009305,\n",
       "   0.2651376136475139,\n",
       "   0.26164444039265317,\n",
       "   0.26880278769466615,\n",
       "   0.27425354801946217,\n",
       "   0.2609828735391299,\n",
       "   0.26018078956339097,\n",
       "   0.2661372406615151,\n",
       "   0.265603218641546,\n",
       "   0.2615472765432464,\n",
       "   0.25989558382166755,\n",
       "   0.25194260478019714,\n",
       "   0.2580443041192161,\n",
       "   0.25845568461550605,\n",
       "   0.2635388928982947,\n",
       "   0.2727302925454246,\n",
       "   0.2597806950410207,\n",
       "   0.2575551321109136,\n",
       "   0.2509658709168434,\n",
       "   0.2656059025062455,\n",
       "   0.2656127040584882,\n",
       "   0.2616362331642045,\n",
       "   0.255418351954884,\n",
       "   0.255765621860822,\n",
       "   0.25327806091970867,\n",
       "   0.25251898169517517,\n",
       "   0.2556539691156811,\n",
       "   0.2644368029303021,\n",
       "   0.2582806837227609,\n",
       "   0.27414288951290977,\n",
       "   0.26377999203072655,\n",
       "   0.24951890773243374,\n",
       "   0.26487764716148376,\n",
       "   0.25953731768661076,\n",
       "   0.261691583527459,\n",
       "   0.25015314999553895,\n",
       "   0.2617918261223369,\n",
       "   0.2600383684039116,\n",
       "   0.26550142880943084,\n",
       "   0.25206269572178525,\n",
       "   0.2641716069645352,\n",
       "   0.2630450808339649,\n",
       "   0.25278567605548435,\n",
       "   0.2634464485777749,\n",
       "   0.26580831160147983,\n",
       "   0.2583412138952149,\n",
       "   0.2548293049136798,\n",
       "   0.2606726719273461,\n",
       "   0.2598913411299388,\n",
       "   0.26205575466156006,\n",
       "   0.25752919912338257,\n",
       "   0.26361702299780315,\n",
       "   0.25191770328415763,\n",
       "   0.26317040539450115,\n",
       "   0.2546130766471227,\n",
       "   0.2644570627146297,\n",
       "   0.25617431352535885,\n",
       "   0.25847021738688153,\n",
       "   0.251815989613533,\n",
       "   0.2579026023546855,\n",
       "   0.2577933544913928,\n",
       "   0.2608587129248513,\n",
       "   0.2536722785896725,\n",
       "   0.2538308807545238,\n",
       "   0.2572857116659482,\n",
       "   0.2691422700881958,\n",
       "   0.2656595872508155,\n",
       "   0.249040140873856,\n",
       "   0.2707323042882813,\n",
       "   0.25326406376229393,\n",
       "   0.26520343869924545,\n",
       "   0.2586668249633577,\n",
       "   0.25612470010916394,\n",
       "   0.25141585287120605,\n",
       "   0.26347775095038944,\n",
       "   0.25290799058145946,\n",
       "   0.2526955513490571,\n",
       "   0.26012086040443844,\n",
       "   0.26364192366600037,\n",
       "   0.2613764281074206,\n",
       "   0.25994519723786247,\n",
       "   0.2602237065633138,\n",
       "   0.2525690363513099,\n",
       "   0.258765840695964,\n",
       "   0.2642446458339691,\n",
       "   0.2647395125693745,\n",
       "   0.2603961726029714], [0.1695374608039856,\n",
       "   0.20586514472961426,\n",
       "   0.15782477855682372,\n",
       "   0.1246507227420807,\n",
       "   0.15583446621894836,\n",
       "   0.1298087477684021,\n",
       "   0.16155113875865937,\n",
       "   0.17711594700813293,\n",
       "   0.15222312808036803,\n",
       "   0.180755090713501,\n",
       "   0.1250921905040741,\n",
       "   0.11556007713079453,\n",
       "   0.17219037115573882,\n",
       "   0.1138246089220047,\n",
       "   0.12984712570905685,\n",
       "   0.1398555725812912,\n",
       "   0.1173863723874092,\n",
       "   0.1154183253645897,\n",
       "   0.10141025930643081,\n",
       "   0.10212266892194748,\n",
       "   0.14237067997455596,\n",
       "   0.15119761526584624,\n",
       "   0.14666942209005357,\n",
       "   0.12070461511611938,\n",
       "   0.17410581409931183,\n",
       "   0.13357782512903213,\n",
       "   0.12252427488565446,\n",
       "   0.14682652056217194,\n",
       "   0.1520289033651352,\n",
       "   0.11304001808166504,\n",
       "   0.12887929230928422,\n",
       "   0.1609887033700943,\n",
       "   0.12455722838640212,\n",
       "   0.13223952502012254,\n",
       "   0.13293039947748184,\n",
       "   0.12408981621265411,\n",
       "   0.12581352293491363,\n",
       "   0.12410790026187897,\n",
       "   0.14165811687707902,\n",
       "   0.11355833411216736,\n",
       "   0.13994381725788116,\n",
       "   0.1331353083252907,\n",
       "   0.13616281747817993,\n",
       "   0.12272917181253433,\n",
       "   0.10917412787675858,\n",
       "   0.14911181181669236,\n",
       "   0.13675402998924255,\n",
       "   0.12776465862989425,\n",
       "   0.11177622377872468,\n",
       "   0.142241407930851,\n",
       "   0.12000736892223358,\n",
       "   0.1158968225121498,\n",
       "   0.10974521860480309,\n",
       "   0.12013310939073563,\n",
       "   0.11883314847946166,\n",
       "   0.12086919844150543,\n",
       "   0.12421463429927826,\n",
       "   0.11522902101278305,\n",
       "   0.12386207431554794,\n",
       "   0.11830707043409347,\n",
       "   0.11332611143589019,\n",
       "   0.10637653470039368,\n",
       "   0.11695541143417358,\n",
       "   0.12713355123996734,\n",
       "   0.11324873119592667,\n",
       "   0.10785021036863326,\n",
       "   0.10754558145999908,\n",
       "   0.11351800858974456,\n",
       "   0.113146111369133,\n",
       "   0.10926057398319244,\n",
       "   0.12047036290168762,\n",
       "   0.12800764590501784,\n",
       "   0.12084401398897171,\n",
       "   0.11324969530105591,\n",
       "   0.12103130966424942,\n",
       "   0.12429093718528747,\n",
       "   0.11874429285526275,\n",
       "   0.11740629225969315,\n",
       "   0.11710695773363114,\n",
       "   0.11452279090881348,\n",
       "   0.12317811995744705,\n",
       "   0.1178387612104416,\n",
       "   0.11534766852855682,\n",
       "   0.11953815668821335,\n",
       "   0.12001432329416276,\n",
       "   0.12219686061143875,\n",
       "   0.12194575667381287,\n",
       "   0.117561174929142,\n",
       "   0.11360722780227661,\n",
       "   0.12065517306327819,\n",
       "   0.11619877815246582,\n",
       "   0.11758022159337997,\n",
       "   0.1181168608367443,\n",
       "   0.1186480313539505,\n",
       "   0.12332243770360947,\n",
       "   0.12153060287237168,\n",
       "   0.11942428797483444,\n",
       "   0.11673855483531952,\n",
       "   0.11848358660936356,\n",
       "   0.11704487055540085,\n",
       "   0.11783095002174378,\n",
       "   0.11177256554365159,\n",
       "   0.11516067087650299,\n",
       "   0.11154569834470748,\n",
       "   0.11523250788450241,\n",
       "   0.11714250445365906,\n",
       "   0.1162828803062439,\n",
       "   0.1194755643606186,\n",
       "   0.11601022034883499,\n",
       "   0.11937668770551682,\n",
       "   0.1183676227927208,\n",
       "   0.11607945412397384,\n",
       "   0.11731123328208923,\n",
       "   0.11362389177083969,\n",
       "   0.11739231944084168,\n",
       "   0.11721034646034241,\n",
       "   0.11821851432323456,\n",
       "   0.11519648879766464,\n",
       "   0.1166950672864914,\n",
       "   0.11462182551622391,\n",
       "   0.11266309171915054,\n",
       "   0.11567576974630356,\n",
       "   0.11301133930683135,\n",
       "   0.11532928049564362,\n",
       "   0.11611643135547638,\n",
       "   0.1146396741271019,\n",
       "   0.115593920648098,\n",
       "   0.11691744327545166,\n",
       "   0.11447583734989167,\n",
       "   0.11355196237564087,\n",
       "   0.11621513664722442,\n",
       "   0.11557809561491013,\n",
       "   0.11425754129886627,\n",
       "   0.11952332854270935,\n",
       "   0.11392022967338562,\n",
       "   0.11939726024866104,\n",
       "   0.11696120053529739,\n",
       "   0.11959934383630752,\n",
       "   0.116819466650486,\n",
       "   0.11449912041425706,\n",
       "   0.11590883582830429,\n",
       "   0.11816283017396927,\n",
       "   0.11551136672496795,\n",
       "   0.11830549538135529,\n",
       "   0.11549542844295502,\n",
       "   0.11931022852659226,\n",
       "   0.11556563228368759,\n",
       "   0.11675090193748475,\n",
       "   0.11657715290784836,\n",
       "   0.11850432008504867,\n",
       "   0.11552485823631287,\n",
       "   0.1171929582953453,\n",
       "   0.1170196995139122,\n",
       "   0.11737254709005356,\n",
       "   0.11722055524587631,\n",
       "   0.11829248517751693,\n",
       "   0.11841773092746735,\n",
       "   0.11893345266580582,\n",
       "   0.1184257909655571,\n",
       "   0.11745083928108216,\n",
       "   0.11647979766130448,\n",
       "   0.11356753259897232,\n",
       "   0.11719509065151215,\n",
       "   0.1143864318728447,\n",
       "   0.11594834178686142,\n",
       "   0.11386420875787735,\n",
       "   0.11657446324825287,\n",
       "   0.1176261231303215,\n",
       "   0.11654648184776306,\n",
       "   0.11423559039831162,\n",
       "   0.1161447286605835,\n",
       "   0.11910871863365173,\n",
       "   0.11901399195194244,\n",
       "   0.11789444386959076,\n",
       "   0.11846826821565629,\n",
       "   0.11659893691539765,\n",
       "   0.11649765521287918,\n",
       "   0.11677445322275162,\n",
       "   0.1159378856420517,\n",
       "   0.11720443218946457,\n",
       "   0.11574124544858932,\n",
       "   0.11529183536767959,\n",
       "   0.119821597635746,\n",
       "   0.11596907824277877,\n",
       "   0.11621969491243363,\n",
       "   0.11824625432491302,\n",
       "   0.11660141050815583,\n",
       "   0.11534010022878646,\n",
       "   0.11639324426651002,\n",
       "   0.11672272086143494], 0.10141025930643081, 0.12231731948099639, 0.11738934591412545),\n",
       " (4,\n",
       "  2,\n",
       "  2,\n",
       "  256): ([1.2045043309529622,\n",
       "   1.124374230702718,\n",
       "   1.0450737410121493,\n",
       "   0.9904008342160119,\n",
       "   0.9492495093080733,\n",
       "   0.9136453105343713,\n",
       "   0.8663513594203525,\n",
       "   0.8034800357288785,\n",
       "   0.7523085640536414,\n",
       "   0.7273747324943542,\n",
       "   0.7189755307303535,\n",
       "   0.6727382838726044,\n",
       "   0.6415601438946195,\n",
       "   0.619005153576533,\n",
       "   0.6016390952799056,\n",
       "   0.5772783309221268,\n",
       "   0.5417288492123286,\n",
       "   0.5521071751912435,\n",
       "   0.5334313147597842,\n",
       "   0.5116327603658041,\n",
       "   0.4798207746611701,\n",
       "   0.4809473670191235,\n",
       "   0.47370169394546086,\n",
       "   0.4636586159467697,\n",
       "   0.4532882455322478,\n",
       "   0.43083678517076707,\n",
       "   0.4196801318062676,\n",
       "   0.42927000257703996,\n",
       "   0.4138532545831468,\n",
       "   0.3966808898581399,\n",
       "   0.4011438753869798,\n",
       "   0.38132279614607495,\n",
       "   0.3952091137568156,\n",
       "   0.38240326113171047,\n",
       "   0.3824259456661012,\n",
       "   0.35666514601972366,\n",
       "   0.3474710484345754,\n",
       "   0.34402250084612107,\n",
       "   0.3542957752943039,\n",
       "   0.33416418896781075,\n",
       "   0.33503594166702694,\n",
       "   0.3340143693817986,\n",
       "   0.319750336309274,\n",
       "   0.3209545910358429,\n",
       "   0.3207630713780721,\n",
       "   0.3209167354636722,\n",
       "   0.30097944372230107,\n",
       "   0.3099004079898198,\n",
       "   0.30025794439845616,\n",
       "   0.30281340413623387,\n",
       "   0.3046477536360423,\n",
       "   0.2961682743496365,\n",
       "   0.3009374489386876,\n",
       "   0.2978997959030999,\n",
       "   0.2919951445526547,\n",
       "   0.2995505498515235,\n",
       "   0.30116327189736897,\n",
       "   0.2900322642591264,\n",
       "   0.27967731571859783,\n",
       "   0.2841362820731269,\n",
       "   0.28385576688581043,\n",
       "   0.2722104771269692,\n",
       "   0.28106579184532166,\n",
       "   0.28525496522585553,\n",
       "   0.28762514640887576,\n",
       "   0.2865385247601403,\n",
       "   0.28375503917535144,\n",
       "   0.27954674926069045,\n",
       "   0.2748735306991471,\n",
       "   0.2687325047122108,\n",
       "   0.28184814006090164,\n",
       "   0.2615770921111107,\n",
       "   0.2609697745905982,\n",
       "   0.2789332957731353,\n",
       "   0.2747719817691379,\n",
       "   0.26342280871338314,\n",
       "   0.2743278154068523,\n",
       "   0.2754361207286517,\n",
       "   0.26420261214176816,\n",
       "   0.2683059730463558,\n",
       "   0.2595101272066434,\n",
       "   0.26290474583705264,\n",
       "   0.25555187463760376,\n",
       "   0.2848699308103985,\n",
       "   0.26199978093306225,\n",
       "   0.2540652934047911,\n",
       "   0.26171306189563537,\n",
       "   0.26595420555935967,\n",
       "   0.26731501519680023,\n",
       "   0.2651146658592754,\n",
       "   0.2600010376837518,\n",
       "   0.2660318894518746,\n",
       "   0.2560349653164546,\n",
       "   0.26992663989464444,\n",
       "   0.2583594247698784,\n",
       "   0.25673859897587037,\n",
       "   0.26516534719202256,\n",
       "   0.2687189413441552,\n",
       "   0.25951282266113496,\n",
       "   0.2593316485484441,\n",
       "   0.25876176522837746,\n",
       "   0.26295017616616356,\n",
       "   0.267802351878749,\n",
       "   0.25590845197439194,\n",
       "   0.25402396503422,\n",
       "   0.24775811864270103,\n",
       "   0.2596781825025876,\n",
       "   0.26123909486664665,\n",
       "   0.2578299840291341,\n",
       "   0.25286731786198086,\n",
       "   0.2559015295571751,\n",
       "   0.25282168719503617,\n",
       "   0.254701081249449,\n",
       "   0.25071758611334694,\n",
       "   0.2557700557841195,\n",
       "   0.2630612676342328,\n",
       "   0.2667272231645054,\n",
       "   0.26844902088244754,\n",
       "   0.2644950540529357,\n",
       "   0.2657744387785594,\n",
       "   0.26170411043696934,\n",
       "   0.25619932760794956,\n",
       "   0.2572399154305458,\n",
       "   0.25848498360978234,\n",
       "   0.25714702241950566,\n",
       "   0.2580989408824179,\n",
       "   0.2576911739177174,\n",
       "   0.24896630313661364,\n",
       "   0.2574653418527709,\n",
       "   0.2590691215462155,\n",
       "   0.2581353924340672,\n",
       "   0.2633429649803374,\n",
       "   0.25839538209968144,\n",
       "   0.2716598990890715,\n",
       "   0.2492439713742998,\n",
       "   0.2522510223918491,\n",
       "   0.2608727365732193,\n",
       "   0.2597711318069034,\n",
       "   0.25246093091037536,\n",
       "   0.2558291190200382,\n",
       "   0.26167310112052494,\n",
       "   0.2571758371260431,\n",
       "   0.2543489659825961,\n",
       "   0.2591033983561728,\n",
       "   0.257346264190144,\n",
       "   0.25456175456444424,\n",
       "   0.2588616344663832,\n",
       "   0.2572729993197653,\n",
       "   0.25635893642902374,\n",
       "   0.2707001707620091,\n",
       "   0.24757438898086548,\n",
       "   0.254558770192994,\n",
       "   0.2722730272346073,\n",
       "   0.2508074500494533,\n",
       "   0.25542594658003914,\n",
       "   0.25616354246934253,\n",
       "   0.2531239067514737,\n",
       "   0.26029044058587814,\n",
       "   0.2533232337898678,\n",
       "   0.26302849915292525,\n",
       "   0.2644829932186339,\n",
       "   0.255029627846347,\n",
       "   0.2555277529690001,\n",
       "   0.25434307836823994,\n",
       "   0.2529763736658626,\n",
       "   0.26491419474283856,\n",
       "   0.24969295412302017,\n",
       "   0.2570933401584625,\n",
       "   0.26058650844626957,\n",
       "   0.25489078130986953,\n",
       "   0.2552671978871028,\n",
       "   0.25943782925605774,\n",
       "   0.24868042684263653,\n",
       "   0.2558262281947666,\n",
       "   0.25376347286833656,\n",
       "   0.24821355856127209,\n",
       "   0.25974707222647136,\n",
       "   0.25951158586475587,\n",
       "   0.25398993078205323,\n",
       "   0.24580940273072985,\n",
       "   0.2583502067459954,\n",
       "   0.2451057525144683,\n",
       "   0.25074203974670833,\n",
       "   0.26769426713387173,\n",
       "   0.2603936460283067,\n",
       "   0.24847547627157635,\n",
       "   0.25538213219907546,\n",
       "   0.25967078159252804,\n",
       "   0.2558964308765199,\n",
       "   0.25883834312359494], [0.17440817058086394,\n",
       "   0.18802996277809142,\n",
       "   0.2654456228017807,\n",
       "   0.22454698085784913,\n",
       "   0.2487279713153839,\n",
       "   0.2580192506313324,\n",
       "   0.21764389276504517,\n",
       "   0.17511188387870788,\n",
       "   0.14795429408550262,\n",
       "   0.18280717730522156,\n",
       "   0.18178818821907045,\n",
       "   0.20714096426963807,\n",
       "   0.1748572826385498,\n",
       "   0.1682073086500168,\n",
       "   0.1684710681438446,\n",
       "   0.18435758650302886,\n",
       "   0.17458859384059905,\n",
       "   0.23212116360664367,\n",
       "   0.2513743132352829,\n",
       "   0.20505146980285643,\n",
       "   0.2795898109674454,\n",
       "   0.2421341359615326,\n",
       "   0.14158772081136703,\n",
       "   0.29870322346687317,\n",
       "   0.17238491177558898,\n",
       "   0.331333726644516,\n",
       "   0.1947038471698761,\n",
       "   0.21759014427661896,\n",
       "   0.2711772322654724,\n",
       "   0.18075959086418153,\n",
       "   0.27991819381713867,\n",
       "   0.22184516191482545,\n",
       "   0.2403504729270935,\n",
       "   0.1792350560426712,\n",
       "   0.22060021460056306,\n",
       "   0.19137804210186005,\n",
       "   0.2099207818508148,\n",
       "   0.23960382640361785,\n",
       "   0.20437286496162416,\n",
       "   0.23967001736164092,\n",
       "   0.2292499750852585,\n",
       "   0.22940561473369597,\n",
       "   0.18144666254520417,\n",
       "   0.22107773125171662,\n",
       "   0.2027920216321945,\n",
       "   0.2186480313539505,\n",
       "   0.1999546229839325,\n",
       "   0.20942140519618987,\n",
       "   0.2045502930879593,\n",
       "   0.2091803640127182,\n",
       "   0.18116167187690735,\n",
       "   0.17854351699352264,\n",
       "   0.21182167530059814,\n",
       "   0.17850473523139954,\n",
       "   0.20730254650115967,\n",
       "   0.21499793529510497,\n",
       "   0.19890295267105101,\n",
       "   0.20366041362285614,\n",
       "   0.20345180630683898,\n",
       "   0.19278044998645782,\n",
       "   0.20295190513134004,\n",
       "   0.20154747664928435,\n",
       "   0.21414740681648253,\n",
       "   0.20473213493824005,\n",
       "   0.22644789814949035,\n",
       "   0.21095994114875793,\n",
       "   0.20998139679431915,\n",
       "   0.20528483390808105,\n",
       "   0.21300520598888398,\n",
       "   0.22094656825065612,\n",
       "   0.22107370793819428,\n",
       "   0.2213983118534088,\n",
       "   0.21246980130672455,\n",
       "   0.21048946678638458,\n",
       "   0.20845856070518493,\n",
       "   0.20702329277992249,\n",
       "   0.2061467081308365,\n",
       "   0.2021130472421646,\n",
       "   0.20627418160438538,\n",
       "   0.2144162178039551,\n",
       "   0.2152254104614258,\n",
       "   0.20659029483795166,\n",
       "   0.20350465476512908,\n",
       "   0.20331737995147706,\n",
       "   0.20810189843177795,\n",
       "   0.21265672743320466,\n",
       "   0.20356447100639344,\n",
       "   0.20637792646884917,\n",
       "   0.20499218106269837,\n",
       "   0.20604596734046937,\n",
       "   0.20076434910297394,\n",
       "   0.20145536661148072,\n",
       "   0.20872552692890167,\n",
       "   0.20259532034397126,\n",
       "   0.205320081114769,\n",
       "   0.2037018597126007,\n",
       "   0.20316498279571532,\n",
       "   0.19713959097862244,\n",
       "   0.19562791883945466,\n",
       "   0.19787228107452393,\n",
       "   0.1990760773420334,\n",
       "   0.19358260035514832,\n",
       "   0.19411886632442474,\n",
       "   0.1930469900369644,\n",
       "   0.1948625087738037,\n",
       "   0.20104798972606658,\n",
       "   0.19818858206272125,\n",
       "   0.2084716886281967,\n",
       "   0.20949167609214783,\n",
       "   0.20461798906326295,\n",
       "   0.20694164037704468,\n",
       "   0.20451854765415192,\n",
       "   0.20102843046188354,\n",
       "   0.20662373304367065,\n",
       "   0.20293923020362853,\n",
       "   0.20006877183914185,\n",
       "   0.19998518824577333,\n",
       "   0.20405233204364776,\n",
       "   0.20363273620605468,\n",
       "   0.20448117554187775,\n",
       "   0.20100131034851074,\n",
       "   0.2052772432565689,\n",
       "   0.20041364133358003,\n",
       "   0.20483456552028656,\n",
       "   0.20283317267894746,\n",
       "   0.2020941197872162,\n",
       "   0.20207076668739318,\n",
       "   0.19999621510505677,\n",
       "   0.2045694410800934,\n",
       "   0.20528181195259093,\n",
       "   0.20446746945381164,\n",
       "   0.20348607897758483,\n",
       "   0.20503169000148774,\n",
       "   0.2049601763486862,\n",
       "   0.20273368954658508,\n",
       "   0.20307418406009675,\n",
       "   0.20065490305423736,\n",
       "   0.20208543241024018,\n",
       "   0.20635065138339997,\n",
       "   0.20099317133426667,\n",
       "   0.19866074919700621,\n",
       "   0.19773707985877992,\n",
       "   0.20043233931064605,\n",
       "   0.1983320951461792,\n",
       "   0.19888052046298982,\n",
       "   0.20100291967391967,\n",
       "   0.19716089069843293,\n",
       "   0.19818440973758697,\n",
       "   0.2016127496957779,\n",
       "   0.19803456366062164,\n",
       "   0.19642337858676912,\n",
       "   0.19951001703739166,\n",
       "   0.20266034305095673,\n",
       "   0.1996706545352936,\n",
       "   0.20066637992858888,\n",
       "   0.19682274758815765,\n",
       "   0.20222682654857635,\n",
       "   0.20071153938770295,\n",
       "   0.2034774601459503,\n",
       "   0.20270080268383026,\n",
       "   0.19894535541534425,\n",
       "   0.19739996194839476,\n",
       "   0.1998192310333252,\n",
       "   0.1981738716363907,\n",
       "   0.20012645721435546,\n",
       "   0.20049206018447877,\n",
       "   0.2025319367647171,\n",
       "   0.20008258521556854,\n",
       "   0.20137932300567626,\n",
       "   0.19931527972221375,\n",
       "   0.20284193456172944,\n",
       "   0.19739803075790405,\n",
       "   0.20052935183048248,\n",
       "   0.19714538156986236,\n",
       "   0.19770975708961486,\n",
       "   0.1995991051197052,\n",
       "   0.20084739327430726,\n",
       "   0.19648402333259582,\n",
       "   0.1996701955795288,\n",
       "   0.20163442492485045,\n",
       "   0.20433229207992554,\n",
       "   0.1977519929409027,\n",
       "   0.1996370553970337,\n",
       "   0.2016102284193039,\n",
       "   0.19623594880104064,\n",
       "   0.19981993436813356,\n",
       "   0.20222186148166657,\n",
       "   0.20153867602348327,\n",
       "   0.1974079877138138,\n",
       "   0.2043093979358673], 0.14158772081136703, 0.20555718311353732, 0.2026805728673935),\n",
       " (4,\n",
       "  2,\n",
       "  2,\n",
       "  512): ([1.1660193867153592,\n",
       "   1.1076376305686102,\n",
       "   1.025038954284456,\n",
       "   0.940264062749015,\n",
       "   0.8999652564525604,\n",
       "   0.8587728838125864,\n",
       "   0.8202737205558353,\n",
       "   0.7858271499474844,\n",
       "   0.7490927543905046,\n",
       "   0.6968332131703695,\n",
       "   0.6761040190855662,\n",
       "   0.6382757773001989,\n",
       "   0.6076410379674699,\n",
       "   0.5928285817305247,\n",
       "   0.5862292978498671,\n",
       "   0.5595365828937955,\n",
       "   0.5322797877920998,\n",
       "   0.5130478607283698,\n",
       "   0.5103069477611117,\n",
       "   0.48654431766933864,\n",
       "   0.4650709645615684,\n",
       "   0.4625231706433826,\n",
       "   0.4571341342396206,\n",
       "   0.4355983982483546,\n",
       "   0.43523215419716305,\n",
       "   0.4175199038452572,\n",
       "   0.4058717257446713,\n",
       "   0.41249458657370675,\n",
       "   0.3924318270550834,\n",
       "   0.3771302368905809,\n",
       "   0.36862241642342675,\n",
       "   0.37683113581604427,\n",
       "   0.37851957976818085,\n",
       "   0.3578563779592514,\n",
       "   0.3515179968542523,\n",
       "   0.34762606687015957,\n",
       "   0.33885052965746987,\n",
       "   0.3424205713801914,\n",
       "   0.33193789919217426,\n",
       "   0.3407675077517827,\n",
       "   0.33130849566724563,\n",
       "   0.3242645379569795,\n",
       "   0.32230886485841537,\n",
       "   0.31803686254554325,\n",
       "   0.30693502393033767,\n",
       "   0.30713796201679444,\n",
       "   0.31074027634329265,\n",
       "   0.31092210362354916,\n",
       "   0.3008115126027001,\n",
       "   0.2974655090106858,\n",
       "   0.29631921152273816,\n",
       "   0.29340924074252445,\n",
       "   0.2893632534477446,\n",
       "   0.2916157924466663,\n",
       "   0.28311995996369255,\n",
       "   0.29889387057887185,\n",
       "   0.28768934061129886,\n",
       "   0.2811102643609047,\n",
       "   0.279593153960175,\n",
       "   0.28767190045780605,\n",
       "   0.27802782009045285,\n",
       "   0.2799970507621765,\n",
       "   0.2839059895939297,\n",
       "   0.2942424698008431,\n",
       "   0.2714111391041014,\n",
       "   0.2815605559282833,\n",
       "   0.27141237921184963,\n",
       "   0.2684808886713452,\n",
       "   0.2756756924920612,\n",
       "   0.27067769318819046,\n",
       "   0.26542339391178554,\n",
       "   0.2682196721434593,\n",
       "   0.26972391870286727,\n",
       "   0.2656182588802444,\n",
       "   0.268889216085275,\n",
       "   0.2666606778899829,\n",
       "   0.2739669390850597,\n",
       "   0.2689947320355309,\n",
       "   0.2641633657945527,\n",
       "   0.27108839485380387,\n",
       "   0.25942768073744243,\n",
       "   0.2654063312543763,\n",
       "   0.26490187562174267,\n",
       "   0.2619845900270674,\n",
       "   0.26104212635093266,\n",
       "   0.2573836992184321,\n",
       "   0.26344525400135255,\n",
       "   0.2590021656619178,\n",
       "   0.256855142613252,\n",
       "   0.2614415991637442,\n",
       "   0.25901777462826836,\n",
       "   0.2629050711790721,\n",
       "   0.2551087232099639,\n",
       "   0.2656007674005296,\n",
       "   0.25467008931769264,\n",
       "   0.2527027403314908,\n",
       "   0.26399562507867813,\n",
       "   0.24859248184495503,\n",
       "   0.26014724870522815,\n",
       "   0.261168897151947,\n",
       "   0.2611927307314343,\n",
       "   0.2560579495297538,\n",
       "   0.25415819303856957,\n",
       "   0.2633403581049707,\n",
       "   0.2545556351542473,\n",
       "   0.2622476981745826,\n",
       "   0.25740805516640347,\n",
       "   0.25963781856828266,\n",
       "   0.2600036503540145,\n",
       "   0.25097489688131547,\n",
       "   0.25881851464509964,\n",
       "   0.2592041591803233,\n",
       "   0.25378522608015275,\n",
       "   0.26484810726510155,\n",
       "   0.2472847956750128,\n",
       "   0.2530875437789493,\n",
       "   0.2544834622078472,\n",
       "   0.26161382844050723,\n",
       "   0.2584565621283319,\n",
       "   0.2626117459601826,\n",
       "   0.2549095352490743,\n",
       "   0.2613936836520831,\n",
       "   0.256965739859475,\n",
       "   0.25072933981815976,\n",
       "   0.2590821178423034,\n",
       "   0.2558077582054668,\n",
       "   0.24815796067317328,\n",
       "   0.25136438343260026,\n",
       "   0.24807010342677435,\n",
       "   0.24884400599532658,\n",
       "   0.2529732825027572,\n",
       "   0.2631429160634677,\n",
       "   0.26369787835412556,\n",
       "   0.25630807131528854,\n",
       "   0.24814177635643217,\n",
       "   0.2504851669073105,\n",
       "   0.24784991145133972,\n",
       "   0.25802872826655704,\n",
       "   0.25196803361177444,\n",
       "   0.25435608873764676,\n",
       "   0.2521877379881011,\n",
       "   0.2515065165029632,\n",
       "   0.24991753531826866,\n",
       "   0.2551868077781465,\n",
       "   0.24741360545158386,\n",
       "   0.25137785491016174,\n",
       "   0.2548206051190694,\n",
       "   0.25119686954551274,\n",
       "   0.25577254593372345,\n",
       "   0.2542779404256079,\n",
       "   0.2519120905134413,\n",
       "   0.2602977156639099,\n",
       "   0.25711294925875133,\n",
       "   0.2613119234641393,\n",
       "   0.256045653588242,\n",
       "   0.2601117003295157,\n",
       "   0.2603997869624032,\n",
       "   0.25651321974065566,\n",
       "   0.25796761363744736,\n",
       "   0.25002434021896786,\n",
       "   0.25216259476211333,\n",
       "   0.2468398834268252,\n",
       "   0.2523622330692079,\n",
       "   0.2588038179609511,\n",
       "   0.2478820855418841,\n",
       "   0.24833561562829548,\n",
       "   0.2603014475769467,\n",
       "   0.25394044982062447,\n",
       "   0.25418513102663887,\n",
       "   0.25458796736266875,\n",
       "   0.2530255276295874,\n",
       "   0.2611646147237884,\n",
       "   0.247291828195254,\n",
       "   0.25069676919115913,\n",
       "   0.2537251106566853,\n",
       "   0.26220234235127765,\n",
       "   0.266796017686526,\n",
       "   0.2544999106062783,\n",
       "   0.2608449583252271,\n",
       "   0.2628544337219662,\n",
       "   0.25654561403724885,\n",
       "   0.25430483867724735,\n",
       "   0.25713588462935555,\n",
       "   0.2536778110596869,\n",
       "   0.25331252151065403,\n",
       "   0.2582491768731011,\n",
       "   0.252957310113642,\n",
       "   0.2546341270208359,\n",
       "   0.2536468861831559,\n",
       "   0.24961441258589426], [0.2076658010482788,\n",
       "   0.17550771832466125,\n",
       "   0.14801352620124816,\n",
       "   0.1952626734972,\n",
       "   0.2170741528272629,\n",
       "   0.19660688936710358,\n",
       "   0.15090625882148742,\n",
       "   0.17018381059169768,\n",
       "   0.151921808719635,\n",
       "   0.20677026808261872,\n",
       "   0.15303085297346114,\n",
       "   0.17944494783878326,\n",
       "   0.1528340384364128,\n",
       "   0.22900060713291168,\n",
       "   0.23259131312370301,\n",
       "   0.16284868270158767,\n",
       "   0.1755104348063469,\n",
       "   0.17332492470741273,\n",
       "   0.16974023580551148,\n",
       "   0.15835852921009064,\n",
       "   0.175125852227211,\n",
       "   0.15808526873588563,\n",
       "   0.1940308153629303,\n",
       "   0.15950207263231278,\n",
       "   0.1366383969783783,\n",
       "   0.18903118669986724,\n",
       "   0.13947006464004516,\n",
       "   0.1490971952676773,\n",
       "   0.1341573640704155,\n",
       "   0.1423102617263794,\n",
       "   0.14651417583227158,\n",
       "   0.12802500426769256,\n",
       "   0.1362825185060501,\n",
       "   0.16692005097866058,\n",
       "   0.13647792339324952,\n",
       "   0.13669717758893968,\n",
       "   0.14139043241739274,\n",
       "   0.14875104874372483,\n",
       "   0.16728654205799104,\n",
       "   0.13859803080558777,\n",
       "   0.15453965812921525,\n",
       "   0.12813740223646164,\n",
       "   0.14963200688362122,\n",
       "   0.13078575879335402,\n",
       "   0.13691471964120866,\n",
       "   0.12625601142644882,\n",
       "   0.13652984648942948,\n",
       "   0.1279631808400154,\n",
       "   0.1301773115992546,\n",
       "   0.13353628516197205,\n",
       "   0.12871068716049194,\n",
       "   0.14908938407897948,\n",
       "   0.1386222094297409,\n",
       "   0.12930369079113008,\n",
       "   0.1322941452264786,\n",
       "   0.12588863223791122,\n",
       "   0.1369483768939972,\n",
       "   0.11766943633556366,\n",
       "   0.11806710213422775,\n",
       "   0.12074937969446183,\n",
       "   0.1203070878982544,\n",
       "   0.1256742924451828,\n",
       "   0.14478016197681426,\n",
       "   0.1500878006219864,\n",
       "   0.14284290820360185,\n",
       "   0.13337676525115966,\n",
       "   0.1308924749493599,\n",
       "   0.12736880034208298,\n",
       "   0.13471185564994811,\n",
       "   0.12782494276762008,\n",
       "   0.12776994705200195,\n",
       "   0.12261158078908921,\n",
       "   0.12959882467985154,\n",
       "   0.12787458151578904,\n",
       "   0.1364518865942955,\n",
       "   0.12527745664119722,\n",
       "   0.12129798680543899,\n",
       "   0.12501232624053954,\n",
       "   0.12353029698133469,\n",
       "   0.12625745236873626,\n",
       "   0.13137613534927367,\n",
       "   0.13577281832695007,\n",
       "   0.13089134842157363,\n",
       "   0.13175919502973557,\n",
       "   0.13506369590759276,\n",
       "   0.13297455310821532,\n",
       "   0.1246499702334404,\n",
       "   0.1279596582055092,\n",
       "   0.13006251603364943,\n",
       "   0.12964306771755219,\n",
       "   0.13149856328964232,\n",
       "   0.12614600956439972,\n",
       "   0.12985755354166031,\n",
       "   0.12693264931440354,\n",
       "   0.1288858473300934,\n",
       "   0.1286248117685318,\n",
       "   0.13073704838752748,\n",
       "   0.1315454751253128,\n",
       "   0.13247538805007936,\n",
       "   0.1264525830745697,\n",
       "   0.12867461144924164,\n",
       "   0.1314932405948639,\n",
       "   0.1311189904808998,\n",
       "   0.13298261016607285,\n",
       "   0.12634045779705047,\n",
       "   0.1299345925450325,\n",
       "   0.1326090082526207,\n",
       "   0.1351803496479988,\n",
       "   0.13672029972076416,\n",
       "   0.13566385805606843,\n",
       "   0.13528741896152496,\n",
       "   0.13255397230386734,\n",
       "   0.13122113943099975,\n",
       "   0.1329529255628586,\n",
       "   0.13437388092279434,\n",
       "   0.13083288967609405,\n",
       "   0.13248980790376663,\n",
       "   0.13305058926343918,\n",
       "   0.13064514249563217,\n",
       "   0.13187891542911528,\n",
       "   0.12799364179372788,\n",
       "   0.12861630767583848,\n",
       "   0.12964062690734862,\n",
       "   0.13131911307573318,\n",
       "   0.12605260014533998,\n",
       "   0.1311796188354492,\n",
       "   0.12569496184587478,\n",
       "   0.1285068526864052,\n",
       "   0.12817256301641464,\n",
       "   0.12914958894252776,\n",
       "   0.12715169936418533,\n",
       "   0.1271955743432045,\n",
       "   0.130156309902668,\n",
       "   0.12746818959712983,\n",
       "   0.13014306128025055,\n",
       "   0.13254696130752563,\n",
       "   0.12960229963064193,\n",
       "   0.13008972257375717,\n",
       "   0.13012340813875198,\n",
       "   0.12765928357839584,\n",
       "   0.1286861777305603,\n",
       "   0.1288815125823021,\n",
       "   0.1270693376660347,\n",
       "   0.13032643646001815,\n",
       "   0.12759843319654465,\n",
       "   0.12840537279844283,\n",
       "   0.12828518450260162,\n",
       "   0.13043503016233443,\n",
       "   0.12842555791139604,\n",
       "   0.13198622167110444,\n",
       "   0.12872692346572875,\n",
       "   0.1290365517139435,\n",
       "   0.12955455332994462,\n",
       "   0.12601377069950104,\n",
       "   0.1286826863884926,\n",
       "   0.12663236409425735,\n",
       "   0.12952368408441545,\n",
       "   0.12776510119438172,\n",
       "   0.13007033318281175,\n",
       "   0.1289848804473877,\n",
       "   0.12731223702430725,\n",
       "   0.1286931335926056,\n",
       "   0.13140705674886705,\n",
       "   0.12999382764101028,\n",
       "   0.1309581533074379,\n",
       "   0.1301145985722542,\n",
       "   0.12943680882453917,\n",
       "   0.1286250278353691,\n",
       "   0.13054341673851014,\n",
       "   0.12891093343496324,\n",
       "   0.1275055542588234,\n",
       "   0.13194966018199922,\n",
       "   0.13173232227563858,\n",
       "   0.13200916200876237,\n",
       "   0.12867583185434342,\n",
       "   0.13105646669864654,\n",
       "   0.12737713158130645,\n",
       "   0.13033901453018187,\n",
       "   0.12682250738143921,\n",
       "   0.1300793170928955,\n",
       "   0.1280507490038872,\n",
       "   0.12788713872432708,\n",
       "   0.12804760932922363,\n",
       "   0.1311345413327217,\n",
       "   0.12621721029281616,\n",
       "   0.12880939245224,\n",
       "   0.12774948477745057,\n",
       "   0.13050245344638825,\n",
       "   0.12999865412712097,\n",
       "   0.12787437736988067], 0.11766943633556366, 0.13773175458374776, 0.1305229350924492),\n",
       " (4,\n",
       "  2,\n",
       "  2,\n",
       "  1024): ([1.1234787868128882,\n",
       "   1.0799722108576033,\n",
       "   1.0132455329100292,\n",
       "   0.9631990889708201,\n",
       "   0.9038063122166528,\n",
       "   0.8315649827321371,\n",
       "   0.7910472187731001,\n",
       "   0.7501804365052117,\n",
       "   0.7225050926208496,\n",
       "   0.691648006439209,\n",
       "   0.6543706009785334,\n",
       "   0.6221760246488783,\n",
       "   0.6150167849328783,\n",
       "   0.5966999746031232,\n",
       "   0.5522051403919855,\n",
       "   0.5505870878696442,\n",
       "   0.5306078924073113,\n",
       "   0.4999462415774663,\n",
       "   0.48487871719731224,\n",
       "   0.48092348045772976,\n",
       "   0.46873362362384796,\n",
       "   0.456883955332968,\n",
       "   0.4318234920501709,\n",
       "   0.42502983411153156,\n",
       "   0.41429828935199314,\n",
       "   0.41557369298405117,\n",
       "   0.39831890828079647,\n",
       "   0.37607645988464355,\n",
       "   0.3700900557968352,\n",
       "   0.37094133761194015,\n",
       "   0.35363034904003143,\n",
       "   0.3655373040172789,\n",
       "   0.35382626454035443,\n",
       "   0.34306703839037156,\n",
       "   0.33568978475199807,\n",
       "   0.3373957756492827,\n",
       "   0.3318190342850155,\n",
       "   0.33488675786389244,\n",
       "   0.3236268063386281,\n",
       "   0.31824219392405617,\n",
       "   0.31479019588894314,\n",
       "   0.3190656138790978,\n",
       "   0.30893325474527145,\n",
       "   0.29977639267841977,\n",
       "   0.30647771971093285,\n",
       "   0.30076947228776085,\n",
       "   0.29953397230969536,\n",
       "   0.2923707365989685,\n",
       "   0.291868935028712,\n",
       "   0.29037901428010726,\n",
       "   0.2934042356080479,\n",
       "   0.28492526296112275,\n",
       "   0.28348617255687714,\n",
       "   0.2825535486141841,\n",
       "   0.2772620974315537,\n",
       "   0.27291907866795856,\n",
       "   0.2705007592837016,\n",
       "   0.27135403868224883,\n",
       "   0.2750700323118104,\n",
       "   0.2729321676823828,\n",
       "   0.2660307163993518,\n",
       "   0.26532083253065747,\n",
       "   0.26432857910792035,\n",
       "   0.2655290241042773,\n",
       "   0.2547483584947056,\n",
       "   0.27135838237073684,\n",
       "   0.26447754684421754,\n",
       "   0.264852171142896,\n",
       "   0.26281310038434136,\n",
       "   0.26138346642255783,\n",
       "   0.2596636522147391,\n",
       "   0.2662322728170289,\n",
       "   0.2657325996292962,\n",
       "   0.2566143365369903,\n",
       "   0.2635601825184292,\n",
       "   0.26209650271468693,\n",
       "   0.26296570069260067,\n",
       "   0.2525656181905005,\n",
       "   0.25215273681614137,\n",
       "   0.25736304372549057,\n",
       "   0.24942654950751197,\n",
       "   0.25439031836059356,\n",
       "   0.2574370743499862,\n",
       "   0.2511593484216266,\n",
       "   0.25320152358876336,\n",
       "   0.25613397277063793,\n",
       "   0.2529484008749326,\n",
       "   0.24685402545664045,\n",
       "   0.247768920328882,\n",
       "   0.25168582631482017,\n",
       "   0.24788778440819847,\n",
       "   0.24852231641610464,\n",
       "   0.25161227997806335,\n",
       "   0.24912729197078282,\n",
       "   0.2507159461577733,\n",
       "   0.24870961325036156,\n",
       "   0.2490286777416865,\n",
       "   0.24343130654758877,\n",
       "   0.2469733324315813,\n",
       "   0.2502773561411434,\n",
       "   0.24834666732284758,\n",
       "   0.24803339938322702,\n",
       "   0.24306925965680015,\n",
       "   0.2444887790415022,\n",
       "   0.24827193137672213,\n",
       "   0.24875189695093367,\n",
       "   0.2433661553594801,\n",
       "   0.23619947582483292,\n",
       "   0.24961567007833058,\n",
       "   0.25565557016266716,\n",
       "   0.24354872107505798,\n",
       "   0.2453872056470977,\n",
       "   0.24682469169298807,\n",
       "   0.24219941513405907,\n",
       "   0.24501216494374806,\n",
       "   0.24775667902496126,\n",
       "   0.2438458651304245,\n",
       "   0.2411297799812423,\n",
       "   0.24378410312864515,\n",
       "   0.24364295519060558,\n",
       "   0.25750129255983567,\n",
       "   0.24686536192893982,\n",
       "   0.2479433740178744,\n",
       "   0.2437838390469551,\n",
       "   0.2516738689608044,\n",
       "   0.2508084923028946,\n",
       "   0.24227468338277605,\n",
       "   0.2490120621191131,\n",
       "   0.24721240335040623,\n",
       "   0.2517979078822666,\n",
       "   0.2449752481447326,\n",
       "   0.24813015510638556,\n",
       "   0.2448925342824724,\n",
       "   0.24479316671689352,\n",
       "   0.24799533933401108,\n",
       "   0.24752382189035416,\n",
       "   0.24002267668644586,\n",
       "   0.24183501054843268,\n",
       "   0.24193439384301504,\n",
       "   0.25745805849631626,\n",
       "   0.24488287998570335,\n",
       "   0.24200305922163856,\n",
       "   0.2392269422610601,\n",
       "   0.23883701860904694,\n",
       "   0.24276913702487946,\n",
       "   0.24531654765208563,\n",
       "   0.24290963179535335,\n",
       "   0.24907435476779938,\n",
       "   0.24168980535533693,\n",
       "   0.2428061705496576,\n",
       "   0.2485038157966402,\n",
       "   0.2431479791800181,\n",
       "   0.24177414923906326,\n",
       "   0.24117517719666162,\n",
       "   0.24156108581357533,\n",
       "   0.248093134827084,\n",
       "   0.24553353753354815,\n",
       "   0.24524852302339342,\n",
       "   0.24281974054045147,\n",
       "   0.23689857042498058,\n",
       "   0.25071441464953953,\n",
       "   0.24017871833509868,\n",
       "   0.24577994561857647,\n",
       "   0.24341881689098147,\n",
       "   0.24538720730278227,\n",
       "   0.24117815163400438,\n",
       "   0.2414836651749081,\n",
       "   0.24798538121912214,\n",
       "   0.24141273399194083,\n",
       "   0.24830383476283815,\n",
       "   0.2445119015044636,\n",
       "   0.2517850266562568,\n",
       "   0.2475347336795595,\n",
       "   0.2445709216925833,\n",
       "   0.24177789688110352,\n",
       "   0.2468360902534591,\n",
       "   0.24480308012829888,\n",
       "   0.24347012903955248,\n",
       "   0.24964756435818142,\n",
       "   0.24861005776458317,\n",
       "   0.24053928752740225,\n",
       "   0.24186458355850643,\n",
       "   0.23946143935124078,\n",
       "   0.2492371689942148,\n",
       "   0.2459827173087332,\n",
       "   0.24901405142413247,\n",
       "   0.24545298433966106,\n",
       "   0.24278358783986834,\n",
       "   0.248544595307774,\n",
       "   0.24263466811842388], [0.2424274742603302,\n",
       "   0.2983677089214325,\n",
       "   0.360425865650177,\n",
       "   0.3575376272201538,\n",
       "   0.4132474005222321,\n",
       "   0.31631290912628174,\n",
       "   0.27733375430107116,\n",
       "   0.28396871089935305,\n",
       "   0.30615618228912356,\n",
       "   0.36498565673828126,\n",
       "   0.2784997045993805,\n",
       "   0.3575094103813171,\n",
       "   0.33994872868061066,\n",
       "   0.174345925450325,\n",
       "   0.19516215324401856,\n",
       "   0.27437532842159273,\n",
       "   0.23284821808338166,\n",
       "   0.1701674774289131,\n",
       "   0.2805922538042068,\n",
       "   0.20918897092342376,\n",
       "   0.19502746760845185,\n",
       "   0.2357717603445053,\n",
       "   0.2588764250278473,\n",
       "   0.22763662338256835,\n",
       "   0.27445978224277495,\n",
       "   0.19704825282096863,\n",
       "   0.21292638182640075,\n",
       "   0.2054722487926483,\n",
       "   0.2626005679368973,\n",
       "   0.15608635544776917,\n",
       "   0.2313281625509262,\n",
       "   0.2260184496641159,\n",
       "   0.20658386945724488,\n",
       "   0.17453522533178328,\n",
       "   0.21974994540214537,\n",
       "   0.16018396317958833,\n",
       "   0.20325720310211182,\n",
       "   0.20930591523647307,\n",
       "   0.14441563189029694,\n",
       "   0.20446769893169403,\n",
       "   0.19961610436439514,\n",
       "   0.18475157469511033,\n",
       "   0.19943850636482238,\n",
       "   0.1663365125656128,\n",
       "   0.16115211993455886,\n",
       "   0.1703726828098297,\n",
       "   0.17752777636051179,\n",
       "   0.1782562866806984,\n",
       "   0.17091913670301437,\n",
       "   0.21384404897689818,\n",
       "   0.16206265836954117,\n",
       "   0.17436132729053497,\n",
       "   0.1874754548072815,\n",
       "   0.17143296003341674,\n",
       "   0.17667259126901627,\n",
       "   0.16693567633628845,\n",
       "   0.17005819082260132,\n",
       "   0.1693760111927986,\n",
       "   0.17805426716804504,\n",
       "   0.17382413148880005,\n",
       "   0.16190792322158815,\n",
       "   0.16441434919834136,\n",
       "   0.16817861795425415,\n",
       "   0.16659520119428634,\n",
       "   0.1782185673713684,\n",
       "   0.17987485826015473,\n",
       "   0.18183534145355223,\n",
       "   0.17393872141838074,\n",
       "   0.16646519005298616,\n",
       "   0.17095387279987334,\n",
       "   0.16995952427387237,\n",
       "   0.17719672620296478,\n",
       "   0.17092137932777404,\n",
       "   0.1762011706829071,\n",
       "   0.16436765789985658,\n",
       "   0.15990519523620605,\n",
       "   0.16602745056152343,\n",
       "   0.168542742729187,\n",
       "   0.16267555803060532,\n",
       "   0.16317740380764006,\n",
       "   0.15578562021255493,\n",
       "   0.1594031572341919,\n",
       "   0.16035997867584229,\n",
       "   0.16461123079061507,\n",
       "   0.16951555013656616,\n",
       "   0.1646343544125557,\n",
       "   0.16643769145011902,\n",
       "   0.17167021781206132,\n",
       "   0.17170680612325667,\n",
       "   0.1727108284831047,\n",
       "   0.1716492936015129,\n",
       "   0.17631454020738602,\n",
       "   0.17680226266384125,\n",
       "   0.1708271622657776,\n",
       "   0.1682970017194748,\n",
       "   0.1662661224603653,\n",
       "   0.16738722026348113,\n",
       "   0.16714612245559693,\n",
       "   0.16278420984745026,\n",
       "   0.16226385682821273,\n",
       "   0.16445151567459107,\n",
       "   0.16854030787944793,\n",
       "   0.1618262931704521,\n",
       "   0.16110349148511888,\n",
       "   0.1646457701921463,\n",
       "   0.16206621676683425,\n",
       "   0.16136271357536316,\n",
       "   0.16971122324466706,\n",
       "   0.1616693913936615,\n",
       "   0.16092697083950042,\n",
       "   0.15942222028970718,\n",
       "   0.16295377016067505,\n",
       "   0.16020215600728988,\n",
       "   0.16069558262825012,\n",
       "   0.16360443234443664,\n",
       "   0.16666654348373414,\n",
       "   0.163160839676857,\n",
       "   0.1603465273976326,\n",
       "   0.16281700432300567,\n",
       "   0.15896978825330735,\n",
       "   0.16541602909564973,\n",
       "   0.16103965491056443,\n",
       "   0.16294746696949006,\n",
       "   0.16940869987010956,\n",
       "   0.16351931989192964,\n",
       "   0.16703831702470778,\n",
       "   0.15981490015983582,\n",
       "   0.16404913663864135,\n",
       "   0.16099219918251037,\n",
       "   0.16064419448375702,\n",
       "   0.1628222793340683,\n",
       "   0.16268527805805205,\n",
       "   0.1632764905691147,\n",
       "   0.1621778905391693,\n",
       "   0.16237728744745256,\n",
       "   0.16055619567632676,\n",
       "   0.16298433095216752,\n",
       "   0.16241250336170196,\n",
       "   0.1614199012517929,\n",
       "   0.16222831904888152,\n",
       "   0.16284539550542831,\n",
       "   0.16860719323158263,\n",
       "   0.15988885760307311,\n",
       "   0.16180974543094634,\n",
       "   0.15872515141963958,\n",
       "   0.16398252099752425,\n",
       "   0.16174194514751433,\n",
       "   0.1647237867116928,\n",
       "   0.16915861368179322,\n",
       "   0.16201506853103637,\n",
       "   0.160678468644619,\n",
       "   0.16903581619262695,\n",
       "   0.1601326882839203,\n",
       "   0.1611146256327629,\n",
       "   0.16610964089632035,\n",
       "   0.1625061422586441,\n",
       "   0.16838354468345643,\n",
       "   0.16355710327625275,\n",
       "   0.16159497648477555,\n",
       "   0.16708755493164062,\n",
       "   0.16493727564811705,\n",
       "   0.16765524446964264,\n",
       "   0.16558263301849366,\n",
       "   0.16070185899734496,\n",
       "   0.16287022978067398,\n",
       "   0.16099002659320832,\n",
       "   0.16337151527404786,\n",
       "   0.1630183070898056,\n",
       "   0.16430114805698395,\n",
       "   0.16440079361200333,\n",
       "   0.16338431984186172,\n",
       "   0.16169354915618897,\n",
       "   0.16405586302280425,\n",
       "   0.16101159751415253,\n",
       "   0.15926404446363449,\n",
       "   0.16010191440582275,\n",
       "   0.16382207870483398,\n",
       "   0.16308854073286055,\n",
       "   0.16460607647895814,\n",
       "   0.16835162788629532,\n",
       "   0.16015526056289672,\n",
       "   0.16533301174640655,\n",
       "   0.16213661432266235,\n",
       "   0.16286214888095857,\n",
       "   0.16735314428806305,\n",
       "   0.161688169836998,\n",
       "   0.16612045615911483,\n",
       "   0.16259792149066926,\n",
       "   0.16326180398464202,\n",
       "   0.16159609109163284], 0.14441563189029694, 0.18377114393993424, 0.16611504852771758),\n",
       " (4,\n",
       "  2,\n",
       "  2,\n",
       "  2048): ([1.027201731999715,\n",
       "   0.9656292564339108,\n",
       "   0.8740776909722222,\n",
       "   0.8373595343695747,\n",
       "   0.7850570115778182,\n",
       "   0.7702769272857242,\n",
       "   0.7176496750778623,\n",
       "   0.6893882817692227,\n",
       "   0.6611554523309072,\n",
       "   0.6208361585934957,\n",
       "   0.6236876083744897,\n",
       "   0.5917997691366408,\n",
       "   0.5717860758304596,\n",
       "   0.5480621390872531,\n",
       "   0.5264641775025262,\n",
       "   0.504707814918624,\n",
       "   0.4684636178943846,\n",
       "   0.46028243833118015,\n",
       "   0.4414829694562488,\n",
       "   0.4355291508966022,\n",
       "   0.45547305213080513,\n",
       "   0.41591939826806384,\n",
       "   0.42218316263622707,\n",
       "   0.39819830987188554,\n",
       "   0.3941546363963021,\n",
       "   0.37195058001412284,\n",
       "   0.3644753512408998,\n",
       "   0.37085964779059094,\n",
       "   0.35625028444661033,\n",
       "   0.3636395451095369,\n",
       "   0.3359370678663254,\n",
       "   0.35163509017891353,\n",
       "   0.3379599129160245,\n",
       "   0.32694971561431885,\n",
       "   0.33498673803276485,\n",
       "   0.323372015522586,\n",
       "   0.3122411295771599,\n",
       "   0.31663500931527877,\n",
       "   0.29933825466367936,\n",
       "   0.304939693874783,\n",
       "   0.30131440858046216,\n",
       "   0.304559600022104,\n",
       "   0.2962096482515335,\n",
       "   0.29557444983058506,\n",
       "   0.2930321759647793,\n",
       "   0.2873911162217458,\n",
       "   0.28315735856692,\n",
       "   0.28746340175469715,\n",
       "   0.28637918995486367,\n",
       "   0.278254428671466,\n",
       "   0.2814408242702484,\n",
       "   0.2762356673677762,\n",
       "   0.2750761037071546,\n",
       "   0.27880244619316524,\n",
       "   0.2739277473754353,\n",
       "   0.27405230121480095,\n",
       "   0.26432472632990944,\n",
       "   0.26646054287751514,\n",
       "   0.27237958792183137,\n",
       "   0.26243090877930325,\n",
       "   0.26410138689809376,\n",
       "   0.2597780070371098,\n",
       "   0.26022741860813564,\n",
       "   0.2545914302269618,\n",
       "   0.26061150679985684,\n",
       "   0.25526318036847645,\n",
       "   0.2602020154396693,\n",
       "   0.2580193198389477,\n",
       "   0.24945165630843905,\n",
       "   0.24944516519705454,\n",
       "   0.25691936413447064,\n",
       "   0.25901662061611813,\n",
       "   0.25256264126963085,\n",
       "   0.2476283609867096,\n",
       "   0.2537311961253484,\n",
       "   0.25655831148227054,\n",
       "   0.25508682678143185,\n",
       "   0.2571486325727569,\n",
       "   0.2484965937005149,\n",
       "   0.24766832341750464,\n",
       "   0.24907977879047394,\n",
       "   0.25612790137529373,\n",
       "   0.2603630738125907,\n",
       "   0.24504378106858996,\n",
       "   0.24921440415912205,\n",
       "   0.24835013349850973,\n",
       "   0.2549228138393826,\n",
       "   0.24699589196178648,\n",
       "   0.2478416305449274,\n",
       "   0.24723885622289446,\n",
       "   0.24725578063064152,\n",
       "   0.24238859199815327,\n",
       "   0.24703080786599052,\n",
       "   0.24037962820794848,\n",
       "   0.24571307748556137,\n",
       "   0.25049438493119347,\n",
       "   0.24487456182638803,\n",
       "   0.2509699339667956,\n",
       "   0.24592413090997273,\n",
       "   0.2516506123873923,\n",
       "   0.24258805645836723,\n",
       "   0.24610019226868948,\n",
       "   0.2423725633157624,\n",
       "   0.2473590084248119,\n",
       "   0.2512648039393955,\n",
       "   0.25553136236137813,\n",
       "   0.2423375364806917,\n",
       "   0.2380762274066607,\n",
       "   0.2464970408214463,\n",
       "   0.23766811026467216,\n",
       "   0.24177505158715779,\n",
       "   0.2398083657026291,\n",
       "   0.24008228381474814,\n",
       "   0.23815985769033432,\n",
       "   0.2474827708469497,\n",
       "   0.23993739320172203,\n",
       "   0.23692035261127684,\n",
       "   0.24475092358059353,\n",
       "   0.2457597835196389,\n",
       "   0.23686370087994468,\n",
       "   0.23992430087592867,\n",
       "   0.2443438023328781,\n",
       "   0.2340070746011204,\n",
       "   0.241245841814412,\n",
       "   0.24426182938946617,\n",
       "   0.24017036623424953,\n",
       "   0.24553013924095365,\n",
       "   0.24169601500034332,\n",
       "   0.24321039186583626,\n",
       "   0.24690732691023085,\n",
       "   0.23326786524719662,\n",
       "   0.2402548392613729,\n",
       "   0.24594697770145205,\n",
       "   0.2389511474304729,\n",
       "   0.2484560286005338,\n",
       "   0.24417654093768862,\n",
       "   0.24936337769031525,\n",
       "   0.23325304935375848,\n",
       "   0.23686150875356463,\n",
       "   0.24200253933668137,\n",
       "   0.245458179877864,\n",
       "   0.24200572570165,\n",
       "   0.24732624077134663,\n",
       "   0.2367753783861796,\n",
       "   0.23998626073201498,\n",
       "   0.24062601228555044,\n",
       "   0.23486009571287367,\n",
       "   0.24256938364770678,\n",
       "   0.2411543842819002,\n",
       "   0.24454299360513687,\n",
       "   0.2390668425295088,\n",
       "   0.24403548489014307,\n",
       "   0.24014638529883492,\n",
       "   0.24086691521935993,\n",
       "   0.23541436509953606,\n",
       "   0.24230875819921494,\n",
       "   0.2370680562323994,\n",
       "   0.23745068245463902,\n",
       "   0.2402973464793629,\n",
       "   0.2337389894657665,\n",
       "   0.2371359086698956,\n",
       "   0.23925848139656913,\n",
       "   0.24125488185220295,\n",
       "   0.24151970942815146,\n",
       "   0.23217815160751343,\n",
       "   0.23812936080826652,\n",
       "   0.24668990903430515,\n",
       "   0.2382489542166392,\n",
       "   0.24405615776777267,\n",
       "   0.2451052318016688,\n",
       "   0.23670163502295813,\n",
       "   0.23320344007677501,\n",
       "   0.23605636341704261,\n",
       "   0.2504420032103856,\n",
       "   0.2387006183465322,\n",
       "   0.24030864901012844,\n",
       "   0.24001238329543007,\n",
       "   0.24400768015119764,\n",
       "   0.24187427510817847,\n",
       "   0.23620137737856972,\n",
       "   0.23929674923419952,\n",
       "   0.2413586899638176,\n",
       "   0.2373217600915167,\n",
       "   0.23763497173786163,\n",
       "   0.23799440264701843,\n",
       "   0.24471943825483322,\n",
       "   0.2358776099152035,\n",
       "   0.24542200565338135,\n",
       "   0.24094029267628989,\n",
       "   0.23971368372440338], [0.32623994946479795,\n",
       "   0.3117467999458313,\n",
       "   0.35676167607307435,\n",
       "   0.31951757073402404,\n",
       "   0.42006638646125793,\n",
       "   0.3956339478492737,\n",
       "   0.3251643359661102,\n",
       "   0.4492786467075348,\n",
       "   0.3214061319828033,\n",
       "   0.16322896182537078,\n",
       "   0.16743956059217452,\n",
       "   0.32699918150901797,\n",
       "   0.32070050239562986,\n",
       "   0.24460493326187133,\n",
       "   0.24535500407218933,\n",
       "   0.3029236435890198,\n",
       "   0.24380494952201842,\n",
       "   0.38235774636268616,\n",
       "   0.24289392232894896,\n",
       "   0.4921180963516235,\n",
       "   0.3168070733547211,\n",
       "   0.20712074935436248,\n",
       "   0.2306305706501007,\n",
       "   0.24043967425823212,\n",
       "   0.23066258430480957,\n",
       "   0.18610792756080627,\n",
       "   0.2203830748796463,\n",
       "   0.26964603960514066,\n",
       "   0.1295030638575554,\n",
       "   0.2834719896316528,\n",
       "   0.16072797924280166,\n",
       "   0.28290734589099886,\n",
       "   0.19889520555734636,\n",
       "   0.2074237734079361,\n",
       "   0.20136078000068663,\n",
       "   0.21027298420667648,\n",
       "   0.2511051639914513,\n",
       "   0.20549817085266114,\n",
       "   0.19721837043762208,\n",
       "   0.2035144031047821,\n",
       "   0.2181759864091873,\n",
       "   0.2216463029384613,\n",
       "   0.24762551486492157,\n",
       "   0.19345630407333375,\n",
       "   0.18767114877700805,\n",
       "   0.23938124179840087,\n",
       "   0.17834088504314421,\n",
       "   0.20572540462017058,\n",
       "   0.20541445314884185,\n",
       "   0.18971065580844879,\n",
       "   0.20739170908927917,\n",
       "   0.21039270162582396,\n",
       "   0.19468220472335815,\n",
       "   0.1947529584169388,\n",
       "   0.19416687041521072,\n",
       "   0.2047067254781723,\n",
       "   0.2003970354795456,\n",
       "   0.19400751888751983,\n",
       "   0.20313703417778015,\n",
       "   0.21556802988052368,\n",
       "   0.200090891122818,\n",
       "   0.18169833421707154,\n",
       "   0.18443379700183868,\n",
       "   0.19102272093296052,\n",
       "   0.2036482572555542,\n",
       "   0.20906769335269929,\n",
       "   0.18765187561511992,\n",
       "   0.18149834573268891,\n",
       "   0.18267764747142792,\n",
       "   0.17821713984012605,\n",
       "   0.18730815798044204,\n",
       "   0.1898426443338394,\n",
       "   0.19159293472766875,\n",
       "   0.19267921447753905,\n",
       "   0.1806218683719635,\n",
       "   0.18816840201616286,\n",
       "   0.19816593825817108,\n",
       "   0.19354033023118972,\n",
       "   0.20061027109622956,\n",
       "   0.18910300731658936,\n",
       "   0.1854827180504799,\n",
       "   0.1803238347172737,\n",
       "   0.19161017686128617,\n",
       "   0.18842992484569548,\n",
       "   0.18737176358699797,\n",
       "   0.1866280972957611,\n",
       "   0.18720867335796357,\n",
       "   0.18637617379426957,\n",
       "   0.19683690667152404,\n",
       "   0.19051946997642516,\n",
       "   0.19310852289199829,\n",
       "   0.18344757407903672,\n",
       "   0.18380942642688752,\n",
       "   0.17885205447673796,\n",
       "   0.1859710246324539,\n",
       "   0.1880943611264229,\n",
       "   0.18652423322200776,\n",
       "   0.1883890897035599,\n",
       "   0.18519631326198577,\n",
       "   0.1907922938466072,\n",
       "   0.19064653068780898,\n",
       "   0.18870611488819122,\n",
       "   0.18576107621192933,\n",
       "   0.1868382453918457,\n",
       "   0.18803258538246154,\n",
       "   0.1814754158258438,\n",
       "   0.1860387146472931,\n",
       "   0.1830814242362976,\n",
       "   0.18166896402835847,\n",
       "   0.18087383061647416,\n",
       "   0.18458518087863923,\n",
       "   0.18369469344615935,\n",
       "   0.18420916497707368,\n",
       "   0.17884428203105926,\n",
       "   0.18031133711338043,\n",
       "   0.1770797476172447,\n",
       "   0.17875130474567413,\n",
       "   0.17875193655490876,\n",
       "   0.1918872833251953,\n",
       "   0.1833176612854004,\n",
       "   0.1867155060172081,\n",
       "   0.17909567356109618,\n",
       "   0.17747516185045242,\n",
       "   0.17732728719711305,\n",
       "   0.1794743075966835,\n",
       "   0.18290772736072541,\n",
       "   0.18429940938949585,\n",
       "   0.1828949749469757,\n",
       "   0.18138845562934874,\n",
       "   0.18258908689022063,\n",
       "   0.1783405512571335,\n",
       "   0.17900814414024352,\n",
       "   0.18045465648174286,\n",
       "   0.18220631778240204,\n",
       "   0.1747720330953598,\n",
       "   0.18567847311496735,\n",
       "   0.1805262565612793,\n",
       "   0.17952038943767548,\n",
       "   0.1754734754562378,\n",
       "   0.17719947993755342,\n",
       "   0.17954773604869842,\n",
       "   0.1825392946600914,\n",
       "   0.17754647135734558,\n",
       "   0.176273974776268,\n",
       "   0.17816369831562043,\n",
       "   0.1782111793756485,\n",
       "   0.17402181923389434,\n",
       "   0.17812047600746156,\n",
       "   0.1782839462161064,\n",
       "   0.17537053525447846,\n",
       "   0.1796875923871994,\n",
       "   0.1798749953508377,\n",
       "   0.1775766968727112,\n",
       "   0.1786438837647438,\n",
       "   0.17410817444324495,\n",
       "   0.18574877679347992,\n",
       "   0.1770620197057724,\n",
       "   0.1758289083838463,\n",
       "   0.17489326000213623,\n",
       "   0.17707078158855438,\n",
       "   0.17697880864143373,\n",
       "   0.17730149030685424,\n",
       "   0.17685711085796357,\n",
       "   0.17657896280288696,\n",
       "   0.18047684729099273,\n",
       "   0.1739886671304703,\n",
       "   0.17976153492927552,\n",
       "   0.17697803676128387,\n",
       "   0.17671341001987456,\n",
       "   0.17569175958633423,\n",
       "   0.17471854984760285,\n",
       "   0.17748060077428818,\n",
       "   0.17852227836847306,\n",
       "   0.18345940709114075,\n",
       "   0.18209478855133057,\n",
       "   0.1738433375954628,\n",
       "   0.17648708820343018,\n",
       "   0.17339564859867096,\n",
       "   0.17575739324092865,\n",
       "   0.17654992043972015,\n",
       "   0.1778463304042816,\n",
       "   0.17865090370178222,\n",
       "   0.17477497160434724,\n",
       "   0.17613341957330703,\n",
       "   0.18602894991636276,\n",
       "   0.17801962792873383,\n",
       "   0.1821005716919899,\n",
       "   0.17794251441955566,\n",
       "   0.1777442663908005,\n",
       "   0.1779739513993263], 0.1295030638575554, 0.20329594679568938, 0.18533951565623283),\n",
       " (4,\n",
       "  2,\n",
       "  2,\n",
       "  4096): ([0.9951142536269294,\n",
       "   0.9449179669221243,\n",
       "   0.8868971566359202,\n",
       "   0.8276092674997118,\n",
       "   0.7988399598333571,\n",
       "   0.7345704403188493,\n",
       "   0.7007420758406321,\n",
       "   0.6574202213022444,\n",
       "   0.6283855222993426,\n",
       "   0.5933485875527064,\n",
       "   0.5669104291333092,\n",
       "   0.5387708213594224,\n",
       "   0.5479811711443795,\n",
       "   0.5145456741253535,\n",
       "   0.49025364220142365,\n",
       "   0.47211186587810516,\n",
       "   0.45951246387428707,\n",
       "   0.45908180210325455,\n",
       "   0.43402664528952706,\n",
       "   0.43335893915759194,\n",
       "   0.40259162253803676,\n",
       "   0.3985990186532338,\n",
       "   0.3860490504238341,\n",
       "   0.3918674737215042,\n",
       "   0.3711937467257182,\n",
       "   0.3772595292992062,\n",
       "   0.3553420702616374,\n",
       "   0.3501872784561581,\n",
       "   0.3449546645085017,\n",
       "   0.34192025661468506,\n",
       "   0.3332594599988725,\n",
       "   0.3277510768837399,\n",
       "   0.3255518658293618,\n",
       "   0.3212299115127987,\n",
       "   0.32156578534179264,\n",
       "   0.31213191979461247,\n",
       "   0.32240668104754555,\n",
       "   0.3011801739533742,\n",
       "   0.30116599301497143,\n",
       "   0.2927187780539195,\n",
       "   0.2886732154422336,\n",
       "   0.28235360980033875,\n",
       "   0.2903599739074707,\n",
       "   0.2763480717937152,\n",
       "   0.28586994608243305,\n",
       "   0.27953102936347324,\n",
       "   0.27795911331971485,\n",
       "   0.2860376404391395,\n",
       "   0.2722134341796239,\n",
       "   0.27442896614472073,\n",
       "   0.2673916725648774,\n",
       "   0.26293451173437965,\n",
       "   0.2645936674541897,\n",
       "   0.26850096384684247,\n",
       "   0.2620851571361224,\n",
       "   0.26403609414895374,\n",
       "   0.26217874470684266,\n",
       "   0.26523913111951614,\n",
       "   0.2588840582304531,\n",
       "   0.2606714458929168,\n",
       "   0.24996057649453482,\n",
       "   0.25328759766287273,\n",
       "   0.2551971582902802,\n",
       "   0.24749384737677044,\n",
       "   0.24716909643676546,\n",
       "   0.2469258200791147,\n",
       "   0.24419598778088888,\n",
       "   0.2565087717440393,\n",
       "   0.24342837019099128,\n",
       "   0.24281715601682663,\n",
       "   0.2461360933052169,\n",
       "   0.25271471093098324,\n",
       "   0.2396787620253033,\n",
       "   0.24434581647316614,\n",
       "   0.24367164489295748,\n",
       "   0.24816637403435177,\n",
       "   0.24020571344428593,\n",
       "   0.24556824896070692,\n",
       "   0.24722067018349966,\n",
       "   0.2474478930234909,\n",
       "   0.23937792579332987,\n",
       "   0.23616356237067115,\n",
       "   0.2393264298637708,\n",
       "   0.24279181990358564,\n",
       "   0.2444762173626158,\n",
       "   0.24429151084687975,\n",
       "   0.24068469968107012,\n",
       "   0.2316490022672547,\n",
       "   0.2323561054137018,\n",
       "   0.22642351935307184,\n",
       "   0.23684578968418968,\n",
       "   0.2353295957048734,\n",
       "   0.23108445604642233,\n",
       "   0.23742451187637117,\n",
       "   0.23754961291948953,\n",
       "   0.23823978337976667,\n",
       "   0.22742212067047754,\n",
       "   0.2277512343393432,\n",
       "   0.2400379768676228,\n",
       "   0.23635533286465538,\n",
       "   0.23722590257724127,\n",
       "   0.23352960331572425,\n",
       "   0.23784381730688942,\n",
       "   0.23865879161490333,\n",
       "   0.23556394543912676,\n",
       "   0.2363547757267952,\n",
       "   0.23677130457427767,\n",
       "   0.23064705812268788,\n",
       "   0.23575714230537415,\n",
       "   0.23719963100221422,\n",
       "   0.23318419605493546,\n",
       "   0.2401663346423043,\n",
       "   0.23587341606616974,\n",
       "   0.23077593661016887,\n",
       "   0.23671668685144848,\n",
       "   0.23292238430844414,\n",
       "   0.23635102642907035,\n",
       "   0.24080057111051348,\n",
       "   0.22447836564646828,\n",
       "   0.2372888285252783,\n",
       "   0.2304493569665485,\n",
       "   0.23024452063772413,\n",
       "   0.23286276972956127,\n",
       "   0.23737095213598675,\n",
       "   0.24001619882053798,\n",
       "   0.23315402617057165,\n",
       "   0.23103601733843485,\n",
       "   0.23869047231144375,\n",
       "   0.23070779111650255,\n",
       "   0.23472293300761116,\n",
       "   0.23027045362525517,\n",
       "   0.23261490795347425,\n",
       "   0.23847587323851055,\n",
       "   0.22876719054248598,\n",
       "   0.23676912320984733,\n",
       "   0.23478097551398808,\n",
       "   0.23433807988961539,\n",
       "   0.2353183420168029,\n",
       "   0.24182282305426067,\n",
       "   0.23237715827094185,\n",
       "   0.23179336223337385,\n",
       "   0.23272623866796494,\n",
       "   0.23196706838077968,\n",
       "   0.23581219298972023,\n",
       "   0.2298509817984369,\n",
       "   0.2375798142618603,\n",
       "   0.2336466188232104,\n",
       "   0.228713629146417,\n",
       "   0.23500425865252814,\n",
       "   0.2334449150496059,\n",
       "   0.23153236591153675,\n",
       "   0.2349697152773539,\n",
       "   0.23724045273330477,\n",
       "   0.23448807249466577,\n",
       "   0.23291743215587404,\n",
       "   0.23515725053018993,\n",
       "   0.22926413847340477,\n",
       "   0.233531278040674,\n",
       "   0.23567076606882942,\n",
       "   0.2427657296260198,\n",
       "   0.22782766239510643,\n",
       "   0.23482993659045961,\n",
       "   0.23028595993916193,\n",
       "   0.23142105754878786,\n",
       "   0.2302847926815351,\n",
       "   0.23148577163616815,\n",
       "   0.23625819964541328,\n",
       "   0.22950313571426603,\n",
       "   0.23334132962756687,\n",
       "   0.23073364380333158,\n",
       "   0.2300529604156812,\n",
       "   0.23477038823895985,\n",
       "   0.23989912536409166,\n",
       "   0.2377048929532369,\n",
       "   0.23857630789279938,\n",
       "   0.23723231918281978,\n",
       "   0.23716657443179023,\n",
       "   0.23431438869900173,\n",
       "   0.2380558815267351,\n",
       "   0.22838962409231398,\n",
       "   0.23647940158843994,\n",
       "   0.230129889316029,\n",
       "   0.23252756810850567,\n",
       "   0.2328889717658361,\n",
       "   0.23786138080888325,\n",
       "   0.23227023250526851,\n",
       "   0.22897506919172075,\n",
       "   0.23752292742331824,\n",
       "   0.23725009130107033,\n",
       "   0.23932976524035135], [0.3208488583564758,\n",
       "   0.34777119755744934,\n",
       "   0.3109926521778107,\n",
       "   0.29075213670730593,\n",
       "   0.2737134426832199,\n",
       "   0.2093674510717392,\n",
       "   0.18092958629131317,\n",
       "   0.21665777564048766,\n",
       "   0.2527394026517868,\n",
       "   0.2682438552379608,\n",
       "   0.34599729180336,\n",
       "   0.13657119274139404,\n",
       "   0.20650258362293245,\n",
       "   0.313730862736702,\n",
       "   0.22225162386894226,\n",
       "   0.3421750068664551,\n",
       "   0.16979722380638124,\n",
       "   0.28724485337734224,\n",
       "   0.33841065466403963,\n",
       "   0.3260443300008774,\n",
       "   0.30479141771793367,\n",
       "   0.34687682390213015,\n",
       "   0.24171143174171447,\n",
       "   0.3053749531507492,\n",
       "   0.45544481873512266,\n",
       "   0.2987187594175339,\n",
       "   0.3216730445623398,\n",
       "   0.3302528202533722,\n",
       "   0.24477624595165254,\n",
       "   0.3201544970273972,\n",
       "   0.25357591807842256,\n",
       "   0.40871991515159606,\n",
       "   0.2479429677128792,\n",
       "   0.30461613833904266,\n",
       "   0.23353905379772186,\n",
       "   0.2709930926561356,\n",
       "   0.2496883451938629,\n",
       "   0.21592026203870773,\n",
       "   0.25391332507133485,\n",
       "   0.23308227062225342,\n",
       "   0.23802008330821992,\n",
       "   0.30178664326667787,\n",
       "   0.240338534116745,\n",
       "   0.286446687579155,\n",
       "   0.27785693407058715,\n",
       "   0.2351079612970352,\n",
       "   0.23933872878551482,\n",
       "   0.23868948221206665,\n",
       "   0.24192020744085313,\n",
       "   0.1926054686307907,\n",
       "   0.20695632696151733,\n",
       "   0.22452975064516068,\n",
       "   0.22452524602413176,\n",
       "   0.20103730261325836,\n",
       "   0.2131420463323593,\n",
       "   0.2544396460056305,\n",
       "   0.2028172254562378,\n",
       "   0.21333540976047516,\n",
       "   0.2048223227262497,\n",
       "   0.17386323511600493,\n",
       "   0.20491330325603485,\n",
       "   0.21198078691959382,\n",
       "   0.19741799533367158,\n",
       "   0.20830849707126617,\n",
       "   0.22305093705654144,\n",
       "   0.21326780021190644,\n",
       "   0.18809932172298433,\n",
       "   0.20749151557683945,\n",
       "   0.20417742133140565,\n",
       "   0.19569965004920958,\n",
       "   0.1909983843564987,\n",
       "   0.1981944888830185,\n",
       "   0.19089415073394775,\n",
       "   0.20700494945049286,\n",
       "   0.20992719531059265,\n",
       "   0.20578281283378602,\n",
       "   0.19110195338726044,\n",
       "   0.19378714263439178,\n",
       "   0.19829676002264024,\n",
       "   0.1994619846343994,\n",
       "   0.21412387788295745,\n",
       "   0.2089245855808258,\n",
       "   0.21293650418519974,\n",
       "   0.218426413834095,\n",
       "   0.20712114572525026,\n",
       "   0.20878258645534514,\n",
       "   0.20795686542987823,\n",
       "   0.20317223072052001,\n",
       "   0.20579136312007903,\n",
       "   0.19860534071922303,\n",
       "   0.202019801735878,\n",
       "   0.19565365016460418,\n",
       "   0.19074636995792388,\n",
       "   0.19318631291389465,\n",
       "   0.18509210497140885,\n",
       "   0.1885794699192047,\n",
       "   0.1968183249235153,\n",
       "   0.19768352508544923,\n",
       "   0.20687618851661682,\n",
       "   0.21150635778903962,\n",
       "   0.20987484008073806,\n",
       "   0.20154283344745635,\n",
       "   0.19485819339752197,\n",
       "   0.19039121270179749,\n",
       "   0.19614247977733612,\n",
       "   0.21180531084537507,\n",
       "   0.20459022372961044,\n",
       "   0.20579946488142015,\n",
       "   0.203007248044014,\n",
       "   0.21290523409843445,\n",
       "   0.21066240072250367,\n",
       "   0.20560475885868074,\n",
       "   0.206708125770092,\n",
       "   0.20668232142925264,\n",
       "   0.20814831256866456,\n",
       "   0.204367595911026,\n",
       "   0.20978703498840331,\n",
       "   0.20971059799194336,\n",
       "   0.21625580489635468,\n",
       "   0.2084932655096054,\n",
       "   0.21413810849189757,\n",
       "   0.20860191881656648,\n",
       "   0.20283843576908112,\n",
       "   0.20885105133056642,\n",
       "   0.20737823247909545,\n",
       "   0.2064767748117447,\n",
       "   0.21032966375350953,\n",
       "   0.2068694144487381,\n",
       "   0.19645735025405883,\n",
       "   0.20070588290691377,\n",
       "   0.20572005957365036,\n",
       "   0.2007442057132721,\n",
       "   0.20580917596817017,\n",
       "   0.19679502844810487,\n",
       "   0.2057766854763031,\n",
       "   0.20407153964042662,\n",
       "   0.20400800704956054,\n",
       "   0.20922377109527587,\n",
       "   0.1991206079721451,\n",
       "   0.20828550159931183,\n",
       "   0.21165796965360642,\n",
       "   0.20184095799922944,\n",
       "   0.20678507387638093,\n",
       "   0.20813277065753938,\n",
       "   0.19981821179389953,\n",
       "   0.1987660348415375,\n",
       "   0.2056572824716568,\n",
       "   0.20108133256435395,\n",
       "   0.19800777435302735,\n",
       "   0.20791864693164824,\n",
       "   0.20300957262516023,\n",
       "   0.19950415194034576,\n",
       "   0.20689626038074493,\n",
       "   0.20484942197799683,\n",
       "   0.2002844512462616,\n",
       "   0.20324059277772905,\n",
       "   0.20580433160066605,\n",
       "   0.20179450809955596,\n",
       "   0.19697066843509675,\n",
       "   0.20862769782543183,\n",
       "   0.19985203444957733,\n",
       "   0.19793172776699067,\n",
       "   0.19972844421863556,\n",
       "   0.20482761561870574,\n",
       "   0.20193989276885987,\n",
       "   0.20446582585573198,\n",
       "   0.1936650112271309,\n",
       "   0.19401436001062394,\n",
       "   0.20015499144792556,\n",
       "   0.19411519467830657,\n",
       "   0.2007289707660675,\n",
       "   0.20440073311328888,\n",
       "   0.201621612906456,\n",
       "   0.198700475692749,\n",
       "   0.2028727948665619,\n",
       "   0.20441872105002404,\n",
       "   0.19333175718784332,\n",
       "   0.20337338447570802,\n",
       "   0.20162282288074493,\n",
       "   0.2009367376565933,\n",
       "   0.20076309740543366,\n",
       "   0.20088386237621308,\n",
       "   0.20642689019441604,\n",
       "   0.1970261186361313,\n",
       "   0.20128773152828217,\n",
       "   0.20493173152208327,\n",
       "   0.20597655177116395,\n",
       "   0.19668891429901122,\n",
       "   0.19721896052360535,\n",
       "   0.19385606348514556], 0.13657119274139404, 0.22213629901801285, 0.20620172098279)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "class HRIRPredictor:\n",
    "    def __init__(self, n_estimators=100, max_depth=3, learning_rate=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the HRIR predictor with XGBoost parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        - n_estimators: Number of gradient boosted trees. Equivalent to number of boosting rounds.\n",
    "        - max_depth: Maximum tree depth for base learners.\n",
    "        - learning_rate: Boosting learning rate (xgb's \"eta\")\n",
    "        \"\"\"\n",
    "        self.model = xgb.XGBRegressor(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate)\n",
    "    \n",
    "    def prepare_data(self, hrir_input, angles):\n",
    "        \"\"\"\n",
    "        Prepares the data for training or prediction. This function assumes the HRIR data\n",
    "        is passed as a numpy array where each row is an HRIR sample, and angles are provided\n",
    "        as a numpy array of corresponding angles.\n",
    "        \n",
    "        Parameters:\n",
    "        - hrir_input: A 2D numpy array of shape (n_samples, hrir_length).\n",
    "        - angles: A 1D numpy array of angles corresponding to each HRIR sample.\n",
    "        \n",
    "        Returns:\n",
    "        - A 2D numpy array where each sample is concatenated with its corresponding angle.\n",
    "        \"\"\"\n",
    "        # Assuming angles is a 1D numpy array and needs to be reshaped to concatenate with hrir_input\n",
    "        print(hrir_input.shape, angles.shape)\n",
    "        angles_expanded = angles[:, np.newaxis]  # Reshape angles to be a 2D array of shape (n_samples, 1)\n",
    "        data = np.concatenate([hrir_input, angles_expanded], axis=1)  # Concatenate along the columns\n",
    "        return data\n",
    "    \n",
    "    def fit(self, X_train, y_train, angles_train):\n",
    "        \"\"\"\n",
    "        Train the XGBoost model on the provided HRIR data and angles.\n",
    "        \n",
    "        Parameters:\n",
    "        - X_train: A 2D numpy array of input HRIR samples.\n",
    "        - y_train: A 2D numpy array of target HRIR samples to predict.\n",
    "        - angles_train: A 1D numpy array of angles corresponding to each HRIR sample in X_train.\n",
    "        \"\"\"\n",
    "        # Prepare the training data\n",
    "        train_data = self.prepare_data(X_train, angles_train)\n",
    "        self.model.fit(train_data, y_train)\n",
    "    \n",
    "    def predict(self, X_test, angles_test):\n",
    "        \"\"\"\n",
    "        Predict the HRIR given new HRIR data and angles using the trained model.\n",
    "        \n",
    "        Parameters:\n",
    "        - X_test: A 2D numpy array of input HRIR samples for prediction.\n",
    "        - angles_test: A 1D numpy array of angles corresponding to each HRIR sample in X_test.\n",
    "        \n",
    "        Returns:\n",
    "        - A 2D numpy array of predicted HRIR samples.\n",
    "        \"\"\"\n",
    "        # Prepare the test data\n",
    "        test_data = self.prepare_data(X_test, angles_test)\n",
    "        return self.model.predict(test_data)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HRIRPredictor(n_estimators=100, max_depth=3, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/workspace/fourth_year_project/HRTF Models/')\n",
    "\n",
    "from BasicDataset import BasicDataset\n",
    "# from BasicTransformer import BasicTransformer\n",
    "\n",
    "sofa_file = '/workspace/fourth_year_project/HRTF Models/sofa_hrtfs/RIEC_hrir_subject_001.sofa'\n",
    "# Basic Dataset only loads the HRIRs at 0 degrees and 90 degrees for baseline and 45 degree for testing\n",
    "hrir_dataset = BasicDataset()\n",
    "for i in range(1,100):\n",
    "    hrir_dataset.load(sofa_file.replace('001', str(i).zfill(3)))\n",
    "\n",
    "hrir0, angle0, hrir90, angle90, hrir45, angle45 = hrir_dataset.get_all()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indexes of 0, 45, 90\n",
    "import numpy as np\n",
    "\n",
    "hrir_w_angle0 = np.concatenate((hrir0, np.array(angle0).reshape(99, 1, 1) * np.ones((99, 1, 512))), axis=1)\n",
    "hrir_w_angle90 = np.concatenate((hrir90, np.array(angle90).reshape(99, 1, 1) * np.ones((99, 1, 512))), axis=1)\n",
    "\n",
    "# Use 0 and 90 as train, 45 is what we want to predict\n",
    "# hrirs = np.concatenate([hrir0, hrir90], axis=1)\n",
    "# angles = np.concatenate([angle0, angle90], axis=1)\n",
    "\n",
    "hrir_w_angle = np.concatenate([hrir_w_angle0, hrir_w_angle90], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 6, 512)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hrir_w_angle.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_angle45 = np.array(angle45).reshape(99, 1, 1) * np.ones((99, 1, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 1, 512)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_angle45.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 7, 512)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hrir_w_angle = np.concatenate([hrir_w_angle, np_angle45], axis=1)\n",
    "hrir_w_angle.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 1, 512)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = np.array(hrir45)[:,0,:].reshape(99, 1, 512)\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "inputs = hrir_w_angle.reshape(99, -1)\n",
    "target = target.reshape(99, -1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: [0.00190521 0.00224655 0.00167643 0.00198121 0.00194867 0.00279147\n",
      " 0.00312522 0.00317197 0.0027345  0.00222221 0.00180301 0.00194808\n",
      " 0.00208339 0.00245969 0.00258889 0.00271421 0.00237771 0.0024036\n",
      " 0.00223826 0.00198274 0.00204243 0.00182977 0.00194016 0.00197549\n",
      " 0.00198102 0.00179046 0.00176416 0.00231825 0.00234067 0.00262038\n",
      " 0.00252182 0.00246584 0.00230699 0.00201891 0.00225596 0.00189604\n",
      " 0.00244115 0.00238684 0.00289508 0.00294856 0.00336184 0.00335368\n",
      " 0.00355083 0.0033579  0.00322736 0.00272617 0.00272911 0.00308264\n",
      " 0.00297486 0.00381001 0.0038145  0.0045657  0.00501122 0.0054861\n",
      " 0.00590034 0.00596047 0.00574592 0.00480472 0.00400278 0.00339641\n",
      " 0.0036172  0.00460183 0.00615199 0.00736145 0.00895565 0.00963975\n",
      " 0.01025196 0.01020529 0.00939792 0.00843247 0.00693984 0.00593959\n",
      " 0.00611336 0.00948255 0.01546496 0.02649212 0.23458084 0.15419103\n",
      " 0.09074854 0.1282612  0.20275101 0.12219176 0.05590379 0.14153917\n",
      " 0.01963474 0.08879793 0.03931944 0.25392267 0.13555738 0.08604398\n",
      " 0.12183141 0.1354812  0.2147644  0.16126285 0.23940067 0.10923187\n",
      " 0.1645972  0.18916799 0.27909273 0.2977796  0.17328575 0.3911446\n",
      " 0.1389205  0.24905063 0.40177456 0.17347363 0.19961563 0.16321152\n",
      " 0.25544375 0.22517501 0.14304045 0.3085083  0.30186725 0.26229826\n",
      " 0.34230557 0.26689532 0.34863365 0.38436523 0.38200983 0.2664007\n",
      " 0.43255693 0.5577353  0.5393476  0.4635071  0.39769572 0.3894511\n",
      " 0.41291878 0.34063226 0.31018606 0.38065645 0.30767587 0.34124684\n",
      " 0.3682664  0.19036074 0.21078171 0.12882937 0.12831241 0.08812748\n",
      " 0.10394886 0.11329995 0.0692162  0.08696834 0.07117609 0.04619637\n",
      " 0.06652531 0.05262288 0.05606789 0.04786634 0.04807353 0.06154856\n",
      " 0.06840047 0.04117042 0.04614195 0.041877   0.03671258 0.02596447\n",
      " 0.02991049 0.03296467 0.01950323 0.02243262 0.01773198 0.02855581\n",
      " 0.01519906 0.01744376 0.01940136 0.02164275 0.02821058 0.02364103\n",
      " 0.01566209 0.0203697  0.01698681 0.01778841 0.01224474 0.02610059\n",
      " 0.01959936 0.01854414 0.03149946 0.01846972 0.01183505 0.01013896\n",
      " 0.0127752  0.01659286 0.0128501  0.01094032 0.00868103 0.00703641\n",
      " 0.01095557 0.0187476  0.01653841 0.00929352 0.01553083 0.00620766\n",
      " 0.01443946 0.00852816 0.00900613 0.00962428 0.00918582 0.01562864\n",
      " 0.00830098 0.01253908 0.0089652  0.0044355  0.00556323 0.00860925\n",
      " 0.00809767 0.00872594 0.00778772 0.00849308 0.00445329 0.00597697\n",
      " 0.00796233 0.00558856 0.00617307 0.00442569 0.00651548 0.00561858\n",
      " 0.00449712 0.00583045 0.00552117 0.00486471 0.00492397 0.00525617\n",
      " 0.0064828  0.00478655 0.00480109 0.0037222  0.00478341 0.00290457\n",
      " 0.00249014 0.00293219 0.00375161 0.00385378 0.00487117 0.0027113\n",
      " 0.00373739 0.00333403 0.00432938 0.00303955 0.00461833 0.00277741\n",
      " 0.00340379 0.00249274 0.00285322 0.00323111 0.00263712 0.00304271\n",
      " 0.00283779 0.00327291 0.00388938 0.00265314 0.00200817 0.00272103\n",
      " 0.00231817 0.00264841 0.00263984 0.00211925 0.00198455 0.00273963\n",
      " 0.00223004 0.002027   0.00173829 0.00173169 0.00282315 0.00198529\n",
      " 0.00207121 0.00279954 0.00207768 0.00200364 0.0021721  0.0022702\n",
      " 0.00232307 0.00203246 0.00232829 0.00236867 0.00215162 0.0018891\n",
      " 0.00221379 0.001681   0.00207975 0.00221651 0.00186115 0.00209493\n",
      " 0.00222914 0.00179731 0.00213999 0.00229777 0.00195453 0.00219054\n",
      " 0.00198978 0.00180781 0.00214906 0.00195415 0.00219308 0.00202236\n",
      " 0.00182128 0.00228232 0.00197158 0.001928   0.0019346  0.00180109\n",
      " 0.00204827 0.00216881 0.00188692 0.00190219 0.00196046 0.00195732\n",
      " 0.00207502 0.00194596 0.00181362 0.00201173 0.00194363 0.00189795\n",
      " 0.00204679 0.00186181 0.00189282 0.00201671 0.00201956 0.0019078\n",
      " 0.00184655 0.00193108 0.00203349 0.00185903 0.00186192 0.00198096\n",
      " 0.00184043 0.00190809 0.00193063 0.00181301 0.00188642 0.00187626\n",
      " 0.00189476 0.00184214 0.00190509 0.00188276 0.00184879 0.00188571\n",
      " 0.00185774 0.00188691 0.00182567 0.00182038 0.00195784 0.00181045\n",
      " 0.00187549 0.00181539 0.00179427 0.00193056 0.00181186 0.00189029\n",
      " 0.0018083  0.00177258 0.00178296 0.00178296 0.00178296 0.00178296\n",
      " 0.00178296 0.00178296 0.00178296 0.00178296 0.00178296 0.00178296\n",
      " 0.00178296 0.00178296 0.00178296 0.00178296 0.00178296 0.00178296\n",
      " 0.00178296 0.00178296 0.00178296 0.00178296 0.00178296 0.00178296\n",
      " 0.00178296 0.00178296 0.00178296 0.00178296 0.00178296 0.00178296\n",
      " 0.00178296 0.00178296 0.00178296 0.00178296 0.00178296 0.00178296\n",
      " 0.00178296 0.00178296 0.00178296 0.00178296 0.00178296 0.00178296\n",
      " 0.00178296 0.00178296 0.00178296 0.00178296 0.00178296 0.00178296\n",
      " 0.00178296 0.00178296 0.00178296 0.00178296 0.00178296 0.00178296\n",
      " 0.00178296 0.00178296 0.00178296 0.00178296 0.00178296 0.00178296\n",
      " 0.00178296 0.00178296 0.00178296 0.00178296 0.00178296 0.00178296\n",
      " 0.00178296 0.00178296 0.00178296 0.00178296 0.00178296 0.00178296\n",
      " 0.00178296 0.00178296 0.00178296 0.00178296 0.00178296 0.00178296\n",
      " 0.00178296 0.00178296 0.00178296 0.00178296 0.00178296 0.00178296\n",
      " 0.00178296 0.00178296 0.00178296 0.00178296 0.00178296 0.00178296\n",
      " 0.00178296 0.00178296 0.00178296 0.00178296 0.00178296 0.00178296\n",
      " 0.00178296 0.00178296 0.00178296 0.00178296 0.00178296 0.00178296\n",
      " 0.00178296 0.00178296 0.00178296 0.00178296 0.00178296 0.00178296\n",
      " 0.00178296 0.00178296 0.00178296 0.00178296 0.00178296 0.00178296\n",
      " 0.00178296 0.00178296 0.00178296 0.00178296 0.00178296 0.00178296\n",
      " 0.00178296 0.00178296 0.00178296 0.00178296 0.00178296 0.00178296\n",
      " 0.00178296 0.00178296 0.00178296 0.00178296 0.00178296 0.00178296\n",
      " 0.00178296 0.00178296 0.00178296 0.00178296 0.00178296 0.00178296\n",
      " 0.00178296 0.00178296 0.00178296 0.00178296 0.00178296 0.00178296\n",
      " 0.00178296 0.00178296 0.00178296 0.00178296 0.00178296 0.00178296\n",
      " 0.00178296 0.00178296 0.00178296 0.00178296 0.00178296 0.00178296\n",
      " 0.00178296 0.00178296 0.00178296 0.00178296 0.00178296 0.00178296\n",
      " 0.00178296 0.00178296]\n"
     ]
    }
   ],
   "source": [
    "model = xgb.XGBRegressor(objective='reg:squarederror', colsample_bytree=0.3, learning_rate=0.1,\n",
    "                         max_depth=10, alpha=10, n_estimators=100)\n",
    "\n",
    "# XGBoost expects 2D input for y if predicting multiple outputs, which we have prepared\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluating the model - this example uses RMSE, consider adjusting for multi-output\n",
    "rmse = np.sqrt(mean_squared_error(y_test, predictions, multioutput='raw_values'))\n",
    "print(f\"RMSE: {rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 512), (1, 512))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth = y_test[10].reshape(1, 512)\n",
    "prediction = predictions[10].reshape(1, 512)\n",
    "ground_truth.shape, prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(ground_truth[0], label='Source')\n",
    "plt.title('Source')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(prediction[0], label='Target')\n",
    "plt.title('Target')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.show()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
