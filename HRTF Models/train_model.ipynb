{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/workspace/fourth_year_project/HRTF Models/')\n",
    "\n",
    "from HRIRDataset import HRIRDataset\n",
    "from MainModel import MainModel\n",
    "from AutoregressiveModel import AutoregressiveModel\n",
    "import matplotlib.pyplot as plt\n",
    "from MaskModel import MaskModel\n",
    "from SeqModel import SeqModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sofa_file = '/workspace/fourth_year_project/HRTF Models/sofa_hrtfs/RIEC_hrir_subject_001.sofa'\n",
    "hrir_dataset = HRIRDataset()\n",
    "for i in range(1,100):\n",
    "    hrir_dataset.load(sofa_file.replace('001', str(i).zfill(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7128"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hrir_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/venv_work/lib/python3.8/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "# model = MainModel()\n",
    "model = MaskModel()\n",
    "# Set the model to training mode\n",
    "model.train()\n",
    "num_epochs = 100\n",
    "\n",
    "# Create the DataLoader\n",
    "#dataloader = DataLoader(hrir_dataset, batch_size=32, shuffle=True)\n",
    "device = torch.device('cuda')\n",
    "model = model.to(device)\n",
    "\n",
    "# Split the dataset into a training, validation and test set\n",
    "# 0.8, 0.1, 0.1 respectively\n",
    "train_size = int(0.7 * len(hrir_dataset))\n",
    "val_size = int(0.2 * len(hrir_dataset))\n",
    "test_size = len(hrir_dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(hrir_dataset, [train_size, val_size, test_size])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('/workspace/fourth_year_project/HRTF Models/mask_models/model_4.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=6)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=6)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_folder = '/workspace/fourth_year_project/HRTF Models/mask_models/'\n",
    "# Create it if it doesn't exist\n",
    "import os\n",
    "if not os.path.exists(target_folder):\n",
    "    os.makedirs(target_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_values(tensor, mask_value, mask_prob):\n",
    "    \"\"\"\n",
    "    Masks values in a tensor with `mask_value` with probability `mask_prob`.\n",
    "\n",
    "    Args:\n",
    "    tensor (torch.Tensor): The input tensor.\n",
    "    mask_value (float): The value to use for masking.\n",
    "    mask_prob (float): The probability of masking each value in the tensor.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: The masked tensor.\n",
    "    torch.Tensor: The original tensor before masking.\n",
    "    \"\"\"\n",
    "    # Create a mask tensor with the same size as the input tensor\n",
    "    # The mask tensor has values of 1 where the input tensor is to be masked\n",
    "    mask = torch.bernoulli(torch.full_like(tensor, mask_prob))\n",
    "\n",
    "    # Create a masked tensor by replacing values where the mask is 1 with `mask_value`\n",
    "    masked_tensor = tensor * (1 - mask) + mask * mask_value\n",
    "\n",
    "    return masked_tensor, tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.,  2.,  3.,  4.,  5., -2.,  7.,  8.,  9., 10.])\n"
     ]
    }
   ],
   "source": [
    "# Testing the mask function\n",
    "tgt = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).float()\n",
    "tgt, true_values = mask_values(tgt, -2, 0.2)\n",
    "print(tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim, nn\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# learning_rate = 0.01\n",
    "# Define the optimizer and loss function\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_function = nn.MSELoss(reduction='none')\n",
    "scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "# Set the model to training mode\n",
    "model.train()\n",
    "percent_masked = 0.1\n",
    "factor = 0.003\n",
    "# myshape = (32, 2, 512)\n",
    "# Create a weight tensor that has higher values for the early part of the impulse response\n",
    "# weights = torch.ones_like(myshape)\n",
    "# weights[:, :200] *= 5  # Increase the weight for the first 200 samples\n",
    "# # # move to cuda\n",
    "# weights = weights.to(device)\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "# Loop over each epoch\n",
    "\n",
    "for epoch in range(1, num_epochs):\n",
    "    # Initialize the epoch loss\n",
    "    epoch_loss = 0.0\n",
    "    model.train()\n",
    "    # Loop over each batch\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        # Get the src and tgt sequences from the batch\n",
    "        src, _, angle = batch\n",
    "        \n",
    "        src, true_values = mask_values(src, -2, percent_masked + factor * epoch)\n",
    "        mask = (src == -2).float()\n",
    "        # Masked values are weighted 10 times more\n",
    "        weights = mask * 9 + torch.ones_like(src)\n",
    "        weights = weights.to(device)\n",
    "        \n",
    "\n",
    "        # Move data to the same device as the model\n",
    "        src = src.to(device)\n",
    "        angle = angle.to(device)\n",
    "        true_values = true_values.to(device)\n",
    "        # print(src.shape, tgt.shape, angle.shape)\n",
    "        # convert to floats\n",
    "        angle = angle.float()\n",
    "        src = src.float()\n",
    "        true_values = true_values.float()\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass through the model\n",
    "        output = model(src, angle, true_values)\n",
    "        \n",
    "        # remove the last feature dimension from output\n",
    "        # [batch_size, d_model, seq_length] --> [batch_size, d_model-1, seq_length]\n",
    "        output = output[:, :-1, :]\n",
    "        loss = loss_function(output, true_values)\n",
    "        loss = loss * weights\n",
    "        loss = loss.mean()\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the batch loss\n",
    "        epoch_loss += loss.item()\n",
    "    val_loss = 0\n",
    "    scheduler.step()\n",
    "    # Validate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(val_loader):\n",
    "            src, _, angle = batch\n",
    "            src = src.to(device)\n",
    "            src, true_values = mask_values(src, -2, percent_masked + factor * epoch)\n",
    "            mask = (src == -2).float()\n",
    "            # Masked values are weighted 10 times more\n",
    "            weights = mask * 9 + torch.ones_like(src)\n",
    "            weights = weights.to(device)\n",
    "            angle = angle.to(device)\n",
    "            angle = angle.float()\n",
    "            src = src.float()\n",
    "            true_values = true_values.to(device)\n",
    "            true_values = true_values.float()\n",
    "            output = model(src, angle, true_values)\n",
    "            # remove the last feature dimension from output\n",
    "            output = output[:, :-1, :]\n",
    "            #print(\"Before loss val: \",output.shape, tgt.shape)\n",
    "            loss = loss_function(output, true_values)\n",
    "            loss = loss * weights\n",
    "            loss = loss.mean()\n",
    "            val_loss += loss.item()\n",
    "    # Print the average loss for this epoch\n",
    "    print(f'Epoch {epoch} | Training Loss: {epoch_loss / len(train_dataset)} | Validation Loss: {val_loss / len(val_dataset)} | Learning Rate: {scheduler.get_last_lr()} | Percentage Masked: {percent_masked + factor * epoch} | Elements Masked: {int((percent_masked + factor * epoch) * 512)}')\n",
    "    if epoch % 2 == 0:\n",
    "        torch.save(model.state_dict(), f'{target_folder}model_{epoch}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tune the model on sequence generation\n",
    "seq_model = SeqModel()\n",
    "seq_model.load_state_dict(torch.load('/workspace/fourth_year_project/HRTF Models/mask_models/model_4.pth'))\n",
    "seq_model = seq_model.to(device)\n",
    "seq_model.train()\n",
    "# Define the optimizer and loss function\n",
    "optimizer = optim.Adam(seq_model.parameters())\n",
    "loss_function = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_folder = '/workspace/fourth_year_project/HRTF Models/seq_models/'\n",
    "# Create it if it doesn't exist\n",
    "import os\n",
    "if not os.path.exists(target_folder):\n",
    "    os.makedirs(target_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, num_epochs):\n",
    "    # Initialize the epoch loss\n",
    "    epoch_loss = 0.0\n",
    "    model.train()\n",
    "    # Loop over each batch\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        # Get the src and tgt sequences from the batch\n",
    "        src, tgt, angle = batch\n",
    "        # Move data to the same device as the model\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "        angle = angle.to(device)\n",
    "        # convert to floats\n",
    "        angle = angle.float()\n",
    "        src = src.float()\n",
    "        tgt = tgt.float()\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass through the model\n",
    "        output = model(src, angle, tgt)\n",
    "        \n",
    "        # remove the last feature dimension from output\n",
    "        # [batch_size, d_model, seq_length] --> [batch_size, d_model-1, seq_length]\n",
    "        output = output[:, :-1, :]\n",
    "        loss = loss_function(output, tgt)\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the batch loss\n",
    "        epoch_loss += loss.item()\n",
    "    val_loss = 0\n",
    "    scheduler.step()\n",
    "    # Validate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(val_loader):\n",
    "            src, tgt, angle = batch\n",
    "            src = src.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "            angle = angle.to(device)\n",
    "            angle = angle.float()\n",
    "            src = src.float()\n",
    "            tgt = tgt.float()\n",
    "            output = model(src, angle, tgt)\n",
    "            # remove the last feature dimension from output\n",
    "            output = output[:, :-1, :]\n",
    "            #print(\"Before loss val: \",output.shape, tgt.shape)\n",
    "            loss = loss_function(output, tgt)\n",
    "            val_loss += loss.item()\n",
    "    # Print the average loss for this epoch\n",
    "    print(f'Epoch {epoch} | Training Loss: {epoch_loss / len(train_dataset)} | Validation Loss: {val_loss / len(val_dataset)} | Learning Rate: {scheduler.get_last_lr()} | Percentage Masked: {percent_masked + factor * epoch} | Elements Masked: {int((percent_masked + factor * epoch) * 512)}')\n",
    "    if epoch % 2 == 0:\n",
    "        torch.save(model.state_dict(), f'{target_folder}model_{epoch}.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
